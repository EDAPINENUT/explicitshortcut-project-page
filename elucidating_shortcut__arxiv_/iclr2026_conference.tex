
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage{url}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{array}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{setspace}
\definecolor{lightblue}{RGB}{100,149,237} % 这里用的是 cornflowerblue，你可以调整RGB值
\definecolor{darkblue}{RGB}{60,100,140}
\usepackage{amsthm}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{minitoc}
\usepackage[toc,page,header]{appendix}
\usepackage{dsfont}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[table]{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathrsfs}
\renewcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{tikz}
\usepackage{url}            % simple URL typesetting
\usepackage{microtype}      % microtypography        
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{overpic}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{nicefrac}  
\usepackage{listings}
\renewcommand{\ttdefault}{pcr}   % Courier
\usepackage{inconsolata}        % Inconsolata (更现代)
\usepackage{tikz} % draw timeline
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{microtype}      % microtypography        
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{overpic}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{nicefrac}
\usetikzlibrary{arrows.meta, positioning, calc}



\definecolor{myverylightgray}{gray}{0.97}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\definecolor{rebuttalred}{RGB}{139, 0, 0}
\newenvironment{rebuttal}
  {\begingroup\color{rebuttalred}}
  {\endgroup}

\newenvironment{theoremgray}[1][]{%
  \begin{tcolorbox}[
    colback=myverylightgray,  % 背景色
    boxrule=0pt,              % 边框宽度=0
    frame empty,              % 完全去掉边框
    left=0pt,right=0pt,top=2pt,bottom=2pt % 内边距可调
  ]
  \begin{theorem}[#1]}%
  {\end{theorem}\end{tcolorbox}}
\newenvironment{lemmagray}[1][]{%
  \begin{tcolorbox}[
    colback=myverylightgray,  % 背景色
    boxrule=0pt,              % 边框宽度=0
    frame empty,              % 完全去掉边框
    left=0pt,right=0pt,top=2pt,bottom=2pt % 内边距可调
  ]
  \begin{lemma}[#1]}%
  {\end{lemma}\end{tcolorbox}}
% \newenvironment{remarkgray}[1][]{%
%   \begin{tcolorbox}[colback=gray!10, colframe=black!50, title=Theorem]%
%   \begin{remark}[#1]}%
%   {\end{remark}\end{tcolorbox}}
\newenvironment{propositiongray}[1][]{%
  \begin{tcolorbox}[
    colback=myverylightgray,  % 背景色
    boxrule=0pt,              % 边框宽度=0
    frame empty,              % 完全去掉边框
    left=0pt,right=0pt,top=2pt,bottom=2pt % 内边距可调
  ]
  \begin{proposition}[#1]}%
  {\end{proposition}\end{tcolorbox}}

\newenvironment{remarkgray}[1][]{%
\begin{tcolorbox}[
    colback=myverylightgray,  % 背景色
    boxrule=0pt,              % 边框宽度=0
    frame empty,              % 完全去掉边框
    left=0pt,right=0pt,top=2pt,bottom=2pt % 内边距可调
  ]
  \begin{remark}[#1]}%
  {\end{remark}\end{tcolorbox}}
\usepackage[colorlinks=true, linkcolor=darkblue, citecolor=darkblue, urlcolor=lightblue]{hyperref}
\title{On the Design of One-step Diffusion via \\ Shortcutting Flow Paths }

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Haitao Lin\thanks{Equal contribution.}%
\footnotemark[1]~~\textsuperscript{1},\;
Peiyan Hu\footnotemark[1]~~\textsuperscript{1,2} \& Minsi Ren\textsuperscript{1}  \\
\texttt{\{linhaitao, hupeiyan, renminsi\}@westlake.edu.cn} \\
\FixAnd 
Zhifeng Gao\textsuperscript{3},  \\ 
\texttt{gaozf@dp.tech} \\
\AND
Zhi-Ming Ma\textsuperscript{2}, \\
\texttt{mazm@amt.ac.cn} \\
\And
Guolin Ke\thanks{Corresponding authors.}~~\textsuperscript{3},  \\
\texttt{kegl@dp.tech}\\
\And Tailin Wu\footnotemark[2]~~\textsuperscript{1} \& Stan Z. Li\footnotemark[2]~~\textsuperscript{1} \\
\texttt{\{wutailin, stan.zq.li\}@westlake.edu.cn}
\AND
\textit{\normalfont{\textsuperscript{1}Department of Artificial Intelligence, School of Engineering, Westlake University;}}\\
{\textsuperscript{2}Academy of Mathematics and Systems Science, Chinese Academy of Sciences;}\\
{\textsuperscript{3}DP Technology, Beijing.}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\doparttoc
\faketableofcontents
\maketitle\vspace{-2em}
\begin{abstract}
Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (\emph{a.k.a.} shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space.
To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256×256 under the classifier-free guidance setting with one step generation, and further reaches FID50k of 2.53 with 2× training steps. Remarkably, the model requires no pre-training, distillation, or curriculum learning.
We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.
\end{abstract}
\vspace{-0.5em}
\section{Introduction}
\vspace{-0.5em}
Diffusion-based models have become the dominant paradigm in deep generative modeling~\citep{sohldickstein2015deepunsupervisedlearningusing, hodiffusion, sde}, progressively transforming samples from a prior distribution toward the data distribution. However, dozens or even hundreds of neural function evaluations~(NFEs) are typically required, resulting in slow inference and limited real-time use~\citep{song2020generativemodelingestimatinggradients,salimans2022progressivedistillationfastsampling,dpmplus,dpmv3}. Consistency models~\citep{cm,icm} are pioneering works that attempt to achieve one-step generation~\citep{Luo2023LatentCM,wang2023diffusiongantraininggansdiffusion,yin2024improved,yin2024onestepdiffusiondistributionmatching,salimans2024multistepdistillationdiffusionmodels,geng2023onestep,geng2025consistency}, but a costly two-stage training process is required, \emph{i.e.}, first training a reliable diffusion model and then distilling velocity or score from it. Despite the costly two-stage training, they offer fast generation, which motivates further research into improving training efficiency.

Recently, one-step diffusion models trained from scratch have emerged, such as Consistency Training~(CT)~\citep{cm} as the training-from-scratch variant of consistency models, Inductive Moment Matching~(IMM)~\citep{imm}, and Shortcut Diffusion~(SCD)~\citep{scd}. These models aim to learn direct shortcut mappings between intermediate states along the probability flow  trajectories of the probability flow, thus enabling one-step generation; we refer to such models as \emph{shortcut models}. Building on this principle, continuous-time shortcut models such as sCT~\citep{scm} and MeanFlow~\citep{meanflow}  have been introduced, achieving state-of-the-art performance in one-step generation for image synthesis. Their efficiency and effectiveness in both training and generation have stimulated further exploration in improving their sampling fidelity.

Although these models share the same objective, the barrier to understanding the working mechanisms remains non-trivial. Specifically, the literature on them is dense on theory, derivations of method formulations and the corresponding learning objectives, as well as technical details like time samplers and curriculum, and training tricks, \emph{etc.}, leading to a less intuitive design paradigm. As a result, it may inadvertently obscure the underlying design space, making each carefully crafted module appear indispensable, so that altering a single component seems to threaten the integrity of the entire system. 

Therefore, we first contribute to \emph{proposing a common design framework for these shortcut models from a practical standpoint}.
We summarize that both discrete- and continuous-time variants share the principle of approximating two-step flow map targets with one-step parameterized predictions. We also provide a general theoretical justification for the validity of this design paradigm. This framework allows us to disentangle the concrete modules within these models, offering clearer insights into how the components interact and what flexibility remains in shaping the overall method design.


Secondly, our contribution lies in \emph{elucidating the design space of shortcut models}. We decompose each model into distinct modules aligned with their learning objectives, and then conduct an in-depth empirical investigation and theoretical analysis of different module combinations. In summary, we demonstrate the advantages of linear paths in settings of shortcut model trained from scratch, discuss the scenarios where continuous-time variants exhibit superior sampling fidelity over discrete-time ones, and figure out the impacts of time samplers on training convergence.

Further, the third set of contributions centers on \emph{improvements to the training of continuous-time shortcut models}. Building on the previous analysis, we introduce three technical refinements for enhancing training stability: (i)~the use of plug-in velocity and its correction under classifier-free-guidance training, (ii)~a gradual time sampler, and (iii)~several established training techniques such as variational adaptive loss weighting. Our experiments demonstrate that these techniques consistently improve performance. Finally, we conduct a scaling-up evaluation on ImageNet-256$\times$256. By incorporating the proposed improvements into our modeling framework, we achieve an FID50k of 2.85 under one-step generation, setting a new state of the art among shortcut models trained from scratch. We believe that our work facilitates component-level innovation and thereby enables more systematic and targeted exploration of the design space of shortcut models.


\section{Expressing One-step Diffusion through Shortcut Models}
\label{sec:flowmap}

\subsection{Shortcutting flows with flow map solvers}
\paragraph{Diffusion models.}Let $p_{\text{data}}(\bm{x})$ be the data distribution, and 
$p_\text{prior} = \mathcal{N}(\bm{0}, \sigma^2\bm{\mathrm{I}})$ be a Gaussian distribution 
with zero mean and variance $\sigma^2$. In the following, we write $\sigma =1$ by default for notational simplicity.
According to stochastic interpolants~\citep{stochasticinterp}, diffusion models establish a probabilistic path between 
$p_0 = p_{\text{data}}$ and $p_1 = p_{\text{prior}}$ such that 
$
\bm{x}_t = \alpha_t \bm{x}_0 + \sigma_t \bm{\varepsilon},
$
where $\bm{x}_0 \sim p_0$, $\bm{\varepsilon} \sim p_1$, and $\alpha_t, \sigma_t \geq 0$; with boundary conditions $\alpha_0 = \sigma_1 = 1$ and $\alpha_1 = \sigma_0 = 0$. Both the forward noising and inverse denoising processes are governed by the probability flow ODE~(PF-ODE) as $\dot{\bm{ x}}_t = \bm{v}_t(\bm{x}_t)$, where $\bm{v}_t(\bm{x})$ is the marginal \emph{velocity}
% which solves the corresponding transport equation 
 $\bm{v}_t(\bm{x}) = \dot{\alpha}_t \mathbb{E}(\bm{ x}_0 | \bm{x}_t = \bm{x}) + \dot{\sigma}_t \mathbb{E}(\bm{\varepsilon} | \bm{x}_t = \bm{x})$.  


\paragraph{Flow paths.} Probabilistic paths satisfying the above are defined as {flow paths}.  For example, with the reformulation by \citet{scm}, the EDM preconditioner path~\citep{edm} can be transformed to a cosine path~\citep{sit} with $\sigma = \sigma_{\text{data}}$,
$\alpha_t = \cos(\frac{\pi}{2}t)$, and $\sigma_t = \sin(\frac{\pi}{2}t)$; in Rectified Flow~\citep{recflow}, $\sigma=1$, $\alpha_t=1-t$ and $\sigma_t=t$, leading to the linear path~\citep{fm,ifm}. Since $\bm{v}_t(\bm{x})$ is inaccessible, the conditional path is established for tractable training, where the corresponding conditional velocity is $\bm{v}_{t|0} = \bm{v}_t(\bm{x}_t|\bm{x}_0) = \dot\alpha_t\bm{x}_0 + \dot\sigma_t\bm{\varepsilon}$ that neural networks $F^{\theta}(\bm{x}_t,t)$ are trained to approximate. In sampling, one can first sample $\bm{x}_1 = \bm{\varepsilon} \sim p_{\text{prior}}$, and then simulate a trajectory of the flow through the PF-ODE as $\dot {\bm{x}}_t = F^{\theta}(\bm{x}_t,t)$. 


\paragraph{Flow maps.} In order to shortcut established flow paths from time $t$ to $r$ ($0\leq r\leq t\leq 1$), we introduce the flow map notation~\citep{flowmap, tutorial, boffi2025how} to express the design frame for simplicity. A flow map $X_{t,r}$  is defined as the unique map such that
    $X_{t,r}(\bm{x}_t) = \bm{x}_r$,  \text{for all } $(t,r) \in [0,1]^2$,
    where $\bm{x}_r$ is the solution of PF-ODE, which corresponds to \emph{position} in physics. According to the PF-ODE, one can easily derive the flow map solution through
\begin{equation}
\bm{x}_r = X_{t,r}(\bm{x}_t) = \bm{x}_t + \int_t^r\bm{v}_\tau (\bm{x}_\tau) d\tau,     \label{eq:velocityint}
\end{equation}
where $\int_t^r\bm{v}_\tau (\bm{x}_\tau) d\tau$ corresponds to \emph{displacement} in physics. 


\paragraph{Flow map solvers.} With the definition of \emph{average velocity} over time~\citep{meanflow} as $\bm{u}_{t,r}$,  we can rewrite Eq.~\ref{eq:velocityint} to express the flow map solution  $\bm{x}_r = {X}_{t,r}(\bm{x}_t)$ through
\begin{equation}
\begin{aligned}
X_{t,r}(\bm{x}_t) &= \bm{x}_t + (r-t)\cdot\bm{u}_{t,r}(\bm{x}_t) \\
   & \text{where }\quad\bm{u}_{t,r}(\bm{x}_t) = \frac{1}{r - t} \int_t^r \bm{v}_\tau(\bm{x}_\tau) \, d\tau , \label{eq:meanvelo}
\end{aligned}
\end{equation} 
 or to infer ${X}_{t,r}(\bm{x}_t)$ with the \emph{instantaneous velocity} $\bm{v}_t$, through DDIM-solver~\citep{ddim} as first-order approximation of DPM-solver~\citep{dpm}, which reads
\begin{equation}
\begin{aligned}
 {X}_{t,r}(\bm{x}_t) \approx \mathrm{DDIM}(\bm{x}_t,&\bm{v}_t,t,r)=\bar\alpha_{t,r} \bm{x}_t + \bar \beta_{t,r} \bm{v}_t, \label{eq:ddimv} 
\end{aligned}
\end{equation}
where $\bar{\alpha}_{t,r} = \cos(\frac{\pi}{2}(r-t))$ and $\bar{\beta}_{t,r} =\frac{2}{\pi} \sin(\frac{\pi}{2}(r-t))$ in cosine paths; and $\bar{\alpha}_{t,r} = 1$ and $\bar{\beta}_{t,r} = r-t$ in linear paths. The general formulation and detailed derivation are given in Appendix~\ref{app:ddim}. 

With the solvers, if a model learns the solution to the flow maps from any $t$ to $r$, it can bypass the costly iterative procedure and achieve one-step generation by predicting $X^\theta_{1,0}(\bm{x}_1)$.


\subsection{Learning to shortcut flow paths}
\label{sec:designframe}
\begin{figure}
    \centering
    \includegraphics[width=0.97\textwidth, trim=142 220 150 40, clip]{shortcutmodel_demon.pdf}
    \caption{The physical picture of ideal and practical learning of discrete- and continuous-time shortcut models~(DTSC\&CTSC) 
     where $\bm{u}_{t,r}^{\mathrm{tgt}}$ denotes the target obtained by the two-step flow maps, and $\bm{u}_{t,r}^{\theta}$ is the models' prediction for one-step flow maps.  (a) shows the marginal velocity field from $\mathcal{N}(0, 1)$ to a Gaussian Mixture. (b) and (c) illustrate the ideal learning of DTSC and CTSC, where $\bm{x}_r$ is sampled from the same trajectory of PF-ODE, and thus $\bm{u}_{t,r}^{\mathrm{tgt}}$ serves as the correct supervisory signal for training. (d) and (e) depict the practical learning of DTSC and CTSC, where the targets deviate from the trajectory, thus leading to models' prediction drifts away correspondingly.}~\label{fig:phyintuition} \vspace{-1em}
\end{figure}

\paragraph{Overall design frame.} We claim that the previous methods shortcut the flow paths of a diffusion model by regularizing a one-step flow map prediction against the two-step flow map target.  In practice, they first sample time points $r,s,t \sim p(\tau) $ with $r \leq s \leq t$, and then  use the consistency property~\citep{tutorial,flowmap} as detailed in Appendix~\ref{app:preliminary} to design a shortcut model trained from scratch, which reads 
\begin{equation}
    X_{s,r}(X_{t,s}(\bm{x}_t)) = X_{t,r}(\bm{x}_t). \label{eq:consistprop}
\end{equation}
 Specifically, these methods aim to construct a two-step flow map target from $t$ to $s$, then to $r$, \emph{i.e.},
$
X_{s,r} \circ X_{t,s}(\bm{x}_t),
$
and then make the parameterized flow map $X^\theta_{t,r}(\bm{x}_t)$ to approximate this target in a single step. It allows the model to achieve one-step generation by predicting $\bm{x}_0^\theta =  X^\theta_{1,0}(\bm{x}_1)$ with $\bm{x}_1 = \bm{\varepsilon} \sim p_1$. As a result, their learning objectives $\mathcal{L}$ can be expressed as
\begin{equation}
    \arg\min_\theta 
\mathbb{E}_{r,s,t\sim p(\tau), \,\bm{x}_t\sim p_t}
\big[\underbrace{
    w(r,s,t) \cdot d(    \overbrace{X^\theta_{t,r}(\bm{x}_t)}^{\text{one-step prediction}},\quad \overbrace{\mathrm{sg}(\hat X_{s,r}\circ \hat X_{t,s}(\bm{x}_t))}^{\text{two-step target}}\quad 
)}_{l(\bm{x}_t, r,s,t;\theta)}
\big],
\label{eq:flowmapconsist}
\end{equation}  
where $w$ is the weight term, $\hat X$ and  $X^\theta$ are flow maps obtained with the conditional velocity or the neural network $F^{\theta}$, $d(\cdot,\cdot)$ is a loss metric function, such as the squared $l_2$-distance, and $\mathrm{sg}(\cdot)$ is the stop gradient operator in backpropagation. We call $\hat X_{s,r}\circ \hat X_{t,s}(\bm{x}_t)$ two-step flow map targets and $X^\theta_{t,r}(\bm{x}_t)$ one-step flow map predictions, and write the inner loss term of expectation as $l(\bm{x}_t, r,s,t; \theta)$.

\paragraph{Time sampler.} To construct the training objective,  time points $\{r,s,t\}$ are sampled with $r\leq s\leq t$.  
We refer to this as the discrete-time shortcut model (DTSC) when $\{r,s,t\}$ are discrete time points. For example, in CTs, $r$ is fixed at $0$, and $t$ and $s$ are sampled from a non-uniform discretization curriculum that gradually changes from sparse to dense such that $t$ is always chosen to be one time step ahead of $s$; SCD divides the time interval
into equal segments based on different powers of $2$, 
and samples uniformly between adjacent grid points with spacing $h$, as denoted by $(t,h) \sim \mathrm{Uniform}\log_{2}(t,h)$ ; IMM samples time with $r$ and $t$ uniformly from $[0,1]$, and $\{s,t\}$ separated by a fixed gap.  
As the gap between two time points becomes infinitesimal, the discrete-time shortcut model converges to a continuous-time form~(CTSC). For example, sCTs and MeanFlows recover this by setting $s \rightarrow t$.  

\paragraph{Network parameterization and flow map solution.} We denote by $F^{\theta}$ the neural network with parameters $\theta$, whose architecture is instantiated as  U-Net~\citep{sde} in the pixel space, or as DiT~\citep{dit} / SiT~\citep{sit} in the latent space.  To obtain the flow map, Eq.~\ref{eq:velocityint}, Eq.~\ref{eq:meanvelo}, and Eq.~\ref{eq:ddimv} can all serve as solutions.  
Since the integral term $\int_{t}^{r}\bm{v}_\tau (\bm{x}_\tau)d\tau$ in Eq.~\ref{eq:velocityint} is intractable in general, the DDIM solver with instantaneous velocity is adopted practically when estimating the flow map with $\bm{v}_t^{\theta}$ parameterized by $F^{\theta}$ or the conditional velocity $\bm{v}_{t|0}$ through Eq.~\ref{eq:ddimv}. 
Alternatively, if $F^{\theta}$ parameterizes average velocity $\bm{u}^{\theta}_{t,r}$, the flow map can be obtained directly through Eq.~\ref{eq:meanvelo}. 


\subsection{Examples: discrete- and continuous-time shortcut models}
\label{sec:examples}
\begin{table}[]\vspace{-1em}
\caption{Specific design choices employed by different shortcut models. `sg EMA decay' means that the parameters $\theta$ in the stop-gradient targets are updated in a delayed manner with EMA.   \vspace{-0.5em}}~\label{tab:methodconclude}
\resizebox{\linewidth}{!}{
\begin{tabular}{lllllll}
\toprule
                                                                              &          & CT                                                                                                                                                                                                                                                                                                                                                                                                 & SCD                                                                                                          & IMM                      & sCT\scriptsize (note: $\triangle$)                                                                                                                                       & MeanFlow                                                                                                                                                                                                 \\\midrule 
\multicolumn{2}{l}{\textbf{Diffusion basis}$^\dag$}                    &                                                                                                                                                                                                                                                                                                                                                                                                                                      &                                                                                                                &                            &                                              &                                                                                                                                                                                                           \\ [-0.1ex]

\multicolumn{2}{l}{Flow path}                  & Cosine                                                                                                                                                                                                                                                                                                                                                                                             & Linear                                                                                                         & Linear                     & Cosine                                                                                                                                   & Linear                                                                                                                                                                                                    \\[3.0ex]
\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Network\\ $F^\theta$\end{tabular}}} & Architecture    & U-Net                                                                                                                                                                                                                                                                                                                                                                                         & DiT                                                                                                            & DiT                        & U-Net                                                                                                                              & DiT                                                                                                                                                                                                      \\ [1.2ex]
                                                                              & Output   & $\bm{v}^{\theta}$                                                                                                                                                                                                                                                                                                                                            & $\bm{u}^{\theta}$                                            & $\bm{v}^{\theta}$        & $\bm{v}^{\theta}$                                                                                                                      & $\bm{u}^{\theta}$                                                                                                                                                                                         \\
\midrule
\multicolumn{2}{l}{\textbf{Flow map construction}} & 
&                                                                                                                &                            &                                                                                                                                        &                                                                                                                                                                                                        \\ [1.2ex]
\multicolumn{2}{l}{Time sampler}               & \small \begin{tabular}[c]{@{}l@{}} \scriptsize (note: $*$)  \\ $t = \frac{\pi}{2}\arctan([\sigma_{\text{max} }^{1/\rho}+ $\\\quad \quad$ \frac{\tau}{K}(\sigma_{\text{min} }^{1/\rho} - \sigma_{\text{max} }^{1/\rho})]^{\rho})$\\ $s =  \frac{\pi}{2}\arctan([\sigma_{\text{max} }^{1/\rho}+$ \\ \quad \quad $ \frac{\tau+1}{K}(\sigma_{\text{min} }^{1/\rho} - \sigma_{\text{max} }^{1/\rho})]^{\rho})$\\ $r=0 $, where \\$\tau \sim \mathcal{U}\{0,\ldots, K-1\}$ \\ \\ \\ \\ \end{tabular} & \small \begin{tabular}[c]{@{}l@{}}   \\ $t=\tau$ \\ $s = \tau - h$\\ $r = \tau -2h$ \\ and with $p_{\text{teq}}$, \\ $r=s=t$,\\  where  $\tau,h \sim$\\$ \text{Uniform}\log_{2}(\tau,h)$ \\ \\ \\ \\ \end{tabular} & \small  \begin{tabular}[c]{@{}l@{}}\scriptsize (note: $\star$) \\ $t \sim$ $\mathcal{U}[0,1]$ \\ $n_s = \frac{1}{1-t} - \frac{1}{2^\gamma} $\\$s =\frac{n_s}{n_s + 1} $ \\ $r \sim \mathcal{U}[0,t]$\\ \\ \\ \\ \\ \\ \\ \end{tabular}& \small  \begin{tabular}[c]{@{}l@{}} $t = \frac{2}{\pi}\mathrm{arctan}(\exp(\tau))$ \\ $s=t - dt$ \\ $r=0$,  where \\ $ \tau \sim \mathcal{N}(P_\text{mean}, P^2_{\text{std}})$ \\ \\  \\ \\ \\ \\ \scriptsize (note: \ddag) \end{tabular}  & \small  \begin{tabular}[c]{@{}l@{}} $r,t =\{\text{sigmoid}(\tau_1),$\\$ \quad \quad \quad \text{sigmoid}(\tau_2)\}$ \\ \emph{s.t.}   $r\leq t$, \\  $s=t - dt$,  and with\\ $p_{\text{teq}}, r=s=t$, where \\ $\tau_1, \tau_2 \sim \mathcal{N}(P_\text{mean}, P^2_{\text{std}})$ \\ \\  \\\\\scriptsize (note: \ddag)    \end{tabular}  \\ [6.0ex]

\multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Two-step\\ target\end{tabular}}}    & 1st-step~($\bm{\hat{x}}_s$)  &\small  $ \mathrm{DDIM}(\bm{x}_t,\bm{v}_{t|0},t,s)$                                                                                                                                                                                                                                                                                                                                                                                                   & \small   $ \bm{x}_t - h \bm{u}_{t, s}^{\theta}(\bm{x}_t)$                                                                                                             &\small   $ \mathrm{DDIM}(\bm{x}_t,\bm{v}_{t|0},t,s)$                               &\small      $ \mathrm{DDIM}(\bm{x}_t,\bm{v}_{t|0},t,s)$                                                                                                                                     &\small                                             $ \mathrm{DDIM}(\bm{x}_t,\bm{v}_{t|0},t,s) $                                                                                                                                                              \\[1.1ex]
                                                                              & 2nd-step~($\bm{\hat{x}}_r$) &\small    $ \mathrm{DDIM}(\bm{\hat x}_s,\bm{v}^\theta_s,s,r)$                                                                                                                                                                         &\small    $  \bm{\hat x}_s - h \bm{u}_{s, r}^{\theta}(\bm{\hat x}_s)$                                                                                                               &\small $ \mathrm{DDIM}(\bm{\hat x}_s,\bm{v}^\theta_s,s,r)$                               &\small    $ \mathrm{DDIM}(\bm{\hat x}_s,\bm{v}^\theta_s,s,r)$                                                                                                                                       &\small  $\bm{\hat x}_s + (r-s)\bm{u}^{\theta}_{s, r}(\bm{x}_s)$                                                                                                                                                                                                           \\[1.2ex]
\multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}One-step prediction ($\bm{x}_r^{\theta}$)\end{tabular}}       &\small       $\mathrm{DDIM}(\bm{x}_t, \bm{v}_t^{\theta}, t,r)$                                                                                                                                                                                                                                                                                                                                                                                                &\small  $\bm{x}_t - 2h\bm{u}_{t, r}^\theta(\bm{x}_t)$                                                                                                               &\small       $\mathrm{DDIM}(\bm{x}_t, \bm{v}_t^{\theta}, t,r)$                         &\small      $\mathrm{DDIM}(\bm{x}_t, \bm{v}^\theta_t, t,r)$                                                                                                                                    &\small      $\bm{x}_t + (r-t)\bm{u}_{t, r}^\theta(\bm{x}_t)$                                                                                                                                                                                                       \\[1.2ex]
\midrule
\multicolumn{3}{l}{\textbf{Training}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &                                                                                                                &                            &                                                                                                                                          &                                                                                                                                                                                                           \\[1.2ex]
\multicolumn{2}{l}{Loss metric $d$}                                                          &       LPIPS                                                                                                                                                                                                                                                                                                                                                                                            &                                                Squared $l_2$-distance                                                                &    Grouped kernel                        &     Squared $l_2$-distance                                                                                                                                     &    Squared $l_2$-distance                                                                                                                                                                                                       \\[1.2ex]
\multicolumn{2}{l}{sg EMA decay}                                                         &   \ding{51}                                                                                                                                                                                                                                                                                                                                                                                                  &                                        \ding{55}                                                                    &                        \ding{55}          &     \ding{55}                                                                                                                                          &      \ding{55}   \\
\bottomrule
\end{tabular}
} \vspace{0.3em}
{\scriptsize
\begin{spacing}{1.2}
\dag Demonstration of the configuration on ImageNet. *In CT, $\rho, \sigma_\text{max}, \sigma_{\text{min}}$ are adopted from EDM, usually set as $7, 0.001$ and $80$. $K$ gradually increases from $K_{\text{min}}$ (usually set as 2) to $K_{\text{max}}$ (usually about 200); In CT's original paper, network output's are the score function and the reformulation is given in Appendix~\ref{app:flowpath}. $\star$$\gamma$ is usually set as 12.
  \ddag  In sCT and MeanFlow, since $s = t - dt$, which involves differentiation \emph{w.r.t.} $t$, terms in loss metrics are normalized by $d t$. The expression is an intuitive analogy, while the derivation is given in Appendix~\ref{app:flowmapconstruct}.  $\triangle$Although sCT is originally initialized from a teacher diffusion model, we suppose that it can attain comparable performance when trained from scratch, similar to the behavior observed in CT and MeanFlow. 
  \end{spacing}
  } \vspace{-1em}
\end{table}

% \paragraph{Discrete-time shortcut models.} CTs, SCDs and IMMs are representative DTSCs.   Under a reformulation from score to velocity shown in Appendix~\ref{app:flowpath},  the output of the neural network in CT reads
% ${F}^{\theta}(\bm{x}_t, t) = \bm{v}^{\theta}_t(\bm{x}_t)$. 
% One-step flow map prediction is obtained by the DDIM solver using the network output velocity $\bm{v}^{\theta}_t(\bm{x}_t)$ from $t$ to $r$, whereas the two-step prediction is derived by alternating between the conditional velocity $\bm{v}_{t|0}$ and the network output $\bm{v}^{\theta}_s(\bm{x}_s)$ as shown in Appendix~\ref{app:ct}. LPIPS~\citep{lpips} as metric function is used directly in the pixel space, leading to
% % Additionally, DDIM solver enables one-step flow map prediction as $X^\theta_{t,r}(\bm{x}_t) = \mathrm{DDIM}(\bm{x}_t, \bm{v}^{\theta}_t(\bm{x}_t), t, s)$, 
% % % mapping any $t \in [0,1]$ to $r = 0$, 
% % % \emph{\emph{\emph{i.e.}}} $X^\theta_{t,r}(\bm{x}_t) = \mathrm{DDIM}(\bm{{x}}_t, \bm{v}_t^{\theta}(\bm{{x}}_t), t, r)$ as the prediction of $\bm{{x}}^{\theta}_0$; 
% % and obtains the two-step target by first calculating
% % $\bm{\hat{x}}_s$ 
% % % =\hat X_{t,s}(\bm{x}_t)=\mathrm{DDIM}(\bm{x}_t, \bm{v}_{t|0}, t, s)
% % with $\bm{v}_{t|0}$, and then applies
% % $X^\theta_{s,r}(\hat{\bm{x}}_s) =\mathrm{DDIM}(\hat{\bm{x}}_s, \bm{v}_s^{\theta}(\hat{\bm{x}}_s),s,r)$ as shown in Appendix~\ref{app:ct}. By setting $w=1$ and the metric function as LPIPS~\citep{lpips}, it reads
% \begin{equation}
%     l_{\mathrm{ct}}(\bm{x}_t, r,s,t; \theta) = \mathrm{LPIPS}\left(\mathrm{DDIM}(\bm{{x}}_t, \bm{v}_t^{\theta}(\bm{{x}}_t), t, r), \mathrm{sg}\left(\mathrm{DDIM}(\bm{\hat{x}}_s, \bm{v}_s^{\theta}(\hat{\bm{x}}_s),s,r)\right)\right). \label{eq:ctloss}
% \end{equation}


% % $t=\tau$,
% % $s = \tau - h$ and $r = \tau - 2h$ with $(\tau,h) \sim \mathrm{Uniform}\log_{2}(\tau,h)$. We regard it as using $F^\theta(\bm{x}_t, t,r)$ to parameterize the average velocity $ \bm{u}_{t,r}(\bm{x}_t)$ defined in Eq.~\ref{eq:meanvelo}. 
% SCD parameterizes the average velocity with $\bm{u}^\theta_{t,r}(\bm{x}_t) =F^\theta(\bm{x}_t, t,r). $ The one-step prediction and both two steps for the target are all estimated with neural networks by Eq.~\ref{eq:meanvelo}.
% % , \emph{\emph{i.e.}} $ \bm{\hat x}_s = \hat X_{t,s}(\bm{x}_t) = \bm{x}_t + h  \bm{u}^{\theta}_{t,s} (\bm{x}_t)$ and $ \hat X_{s,r}(\bm{\hat x}_s) = \bm{\hat x}_s + h  \bm{u}^{\theta}_{s,r} (\bm{\hat x}_s)$. 
% Using the squared $l_2$-distance and simplifying with $w = \frac{1}{4h^2}$ as shown in Appendix~\ref{app:scd}, $l$ in Eq.~\ref{eq:flowmapconsist} reads
% \begin{equation}
% l_{\mathrm{scd}}(\bm{x}_t, r,s,t; \theta) = \left\| 
% \bm{u}^{\theta}_{t,r}(\bm{x}_t)
% - \frac{1}{2}\mathrm{sg}\!\left( 
% \bm{u}^{\theta}_{t,s}(\bm{x}_t) 
% + \bm{u}^{\theta}_{s,r}(\bm{\hat x}_s) 
% \right)
% \right\|_2 ^2.
% \label{eq:scdloss}
% \end{equation}
% Moreover, IMM firstly partitions samples $\{\bm{x}^{(i)}_{0}, \bm{\varepsilon}^{(i)}\}_{i=1}^{B}$ within a mini-batch of size $B$ into different groups, and samples $r,s,t$ over time for different groups.  With similar flow map construction to CTs, a grouped kernel function implemented with MMD~\citep{mmd} is applied to evaluate both inter- and intra-sample similarities between $\{\bm{\hat x}_r, \bm{x}^{\theta}_r\}$ in a group, as further detailed in Appendix~\ref{app:imm}.


\paragraph{Discrete-time shortcut models.} CTs, SCDs and IMMs are representative DTSCs. 
If parameterizing velocity with neural networks as  
$F^{\theta}(\bm{x}_t, t) = \bm{v}^{\theta}_t(\bm{x}_t)$,
we can then adopt the DDIM as flow map solvers. Specifically, we first use the parameterized velocity $\bm{v}^{\theta}_t(\bm{x}_t)$ to solve the flow map $\bm{x}_r^\theta = X^\theta_{t,r}(\bm{x}_t)$, which serves as the one-step prediction.
For the two-step target, we alternate between the conditional velocity $\bm{v}_{t|0}$ to obtain $\hat{\bm{x}}_s = \hat{X}_{t,s}(\bm{x}_t)$, and the parameterized velocity $\bm{v}^{\theta}_s(\hat{\bm{x}}_s)$ to obtain $\hat{\bm{x}}_r = \hat{X}_{s,r}(\hat{\bm{x}}_s)$.
In this way, we can derive $l(\bm{x}_t, r,s,t; \theta)$ in Eq.~\ref{eq:flowmapconsist} as 
\begin{equation}
    l_{\mathrm{ct}}(\bm{x}_t, r,s,t; \theta) = \mathrm{LPIPS}\Big(\mathrm{DDIM}(\bm{x}_t, \bm{v}_t^{\theta}(\bm{x}_t), t, r), \;\mathrm{sg}\big(\mathrm{DDIM}(\hat{\bm{x}}_s, \bm{v}_s^{\theta}(\hat{\bm{x}}_s), s, r)\big)\Big), 
    \label{eq:ctloss}
\end{equation}
where the loss metric is LPIPS~\citep{lpips} applied directly in pixel space with $w=1$, and $l_{\mathrm{ct}}(\bm{x}_t, r,s,t; \theta)$ coincides with its formulation in CTs with details shown in Appendix~\ref{app:ct}.
Alternatively, if we parameterize the average velocity as $\bm{u}^\theta_{t,r}(\bm{x}_t) =F^\theta(\bm{x}_t, t,r) $, and estimate both the one-step prediction and the two steps in the target with neural networks $F^\theta$, we thus obtain the  $l(\bm{x}_t, r,s,t; \theta)$ in SCD through Eq.~\ref{eq:meanvelo}. Due to equi-spacing time points as $t-s=s-r=h$, it reads
\begin{equation}
    l_{\mathrm{scd}}(\bm{x}_t, r,s,t; \theta) = \left\| 
\bm{u}^{\theta}_{t,r}(\bm{x}_t)
- \frac{1}{2}\mathrm{sg}\!\left( 
\bm{u}^{\theta}_{t,s}(\bm{x}_t) 
+ \bm{u}^{\theta}_{s,r}(\hat{\bm x}_s) 
\right)
\right\|_2 ^2,
\label{eq:scdloss}
\end{equation}
where the loss metric is set as squared $l_2$-distance, $w = \frac{1}{4h^2}$, and $\hat{\bm{x}}_s = \bm{x}_t + (s-t)\bm{u}^\theta_{s,t}(\bm{x}_t)$, with details shown in Appendix~\ref{app:scd}.
Moreover, for IMMs, conditional samples $\{\bm{x}^{(i)}_{0}, \bm{\varepsilon}^{(i)}\}_{i=1}^{B}$ within a mini-batch of size $B$ are first partitioned into different groups, and $\{r,s,t\}$ are drawn for each group.  With similar flow map construction to CTs,  the loss metric is implemented with a grouped kernel function as to minimize MMD~\citep{mmd}, applied to measure both inter- and intra-sample similarities between $\{\bm{\hat x}_r, \bm{x}^{\theta}_r\}$ within the group, as further detailed in Appendix~\ref{app:imm}.

\paragraph{Continuous-time shortcut models.} When the difference between two time points is infinitesimal, the resulting shortcut models are referred to CTSCs, by setting $s=t-dt$ and normalizing $l(\bm{x}_t,r,s,t;\theta)$ by $dt$. For instance, MeanFlows are continuous-time shortcut models in which $s = t - dt$.  They leverage linear paths with squared $l_2$-distance as the loss metric and parameterizes the average velocity $\bm{u}_{t,r}(\bm{x}_t)$ with neural networks $F^{\theta}$. 
By writing $l(\bm{x}_t,r,t-dt,t;\theta) = w\cdot \left\| \frac{d}{dt}\left(X^\theta_{t,r}(\bm{x}_t) - \hat X_{t-dt,r}\circ \hat X_{t,t-dt}(\bm{x}_t)\right)\right\|^2$ and $\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t) = \partial_t \bm u^\theta_{t,r}(\bm x_t) + (\nabla_{\bm{x}} \bm u^\theta_{t,r})(\bm x_t) \cdot \bm{v}_{t}$, 
and applying Eq.~\ref{eq:velocityint} and~\ref{eq:meanvelo} with approximation shown in Appendix~\ref{app:MeanFlow}, 
we correspondingly obtain
\begin{equation}
    l_{\mathrm{mf}}(\bm{x}_t, r, t-dt, t;\theta) = w\cdot \left\|\bm{u}^\theta_{t,r}(\bm{x}_t)
-\mathrm{sg}\left(\bm{v}_{t|0} + (r-t) \frac{d\bm{u}^{\theta}_{t,r}(\bm{x}_t)}{dt}\right) \right\|_2^2, \label{eq:MeanFlowloss}
\end{equation} under squared $l_2$-distance with adaptive weighting $w$, as detailed in Appendix~\ref{app:MeanFlow}. Note that there is a predefined probability $p_{\text{teq}}$ such that $r = t$, which results in $l_{\text{mf}} = w \|\bm{u}^{\theta}_{t,t} - \bm{v}_t \|^{2}$ during training. This training technique of instantaneous conditional velocity supervision is also employed in SCDs. 

sCTs, as the continuous-time variants of CTs, use squared $l_2$-distance instead of LPIPS. Under $s=t-dt$, Appendix~\ref{app:sct} shows that the gradient of $l_{\mathrm{ct}}(\bm{x}_t, r, s, t; \theta)$ \emph{w.r.t.} $\theta$ can be approximated as
$ \nabla_\theta l(\bm{x}_t, r, s, t; \theta) 
    \approx \nabla_\theta \| \bm{v}^\theta_{t}(\bm{x}_t) - \mathrm{sg}( \bm{v}^\theta_{t}(\bm{x}_t) + w(t) \frac{d}{dt}X^\theta_{t,r}(\bm{x}_t) )\|_2^2$.
By setting $w(t)= \cos(\frac{\pi}{2}t)$,  
\begin{equation}
    l_{\mathrm{sct}}(\bm{x}_t,r,t-dt,t;\theta) = \left\| \bm{v}^\theta_{t}(\bm{x}_t) - \mathrm{sg}\left( \bm{v}^\theta_{t}(\bm{x}_t) + w(t) \frac{d \mathrm{DDIM}(\bm{x}_t, \bm{v}^{\theta}_t(\bm{x}_t), t, r)}{dt} \right)\right\|_2^2, \label{eq:sctloss}
\end{equation}
 
% In addition, sCTs transform the path of preconditioner from EDM via change of variables, and apply $\arctan(\cdot)$ to time samples from the log-normal distribution proposed by EDM to achieve full equivalence to the cosine path as shown in Appendix~\ref{app:flowpath}. 
% We denote this time sampling process as 
% $\tau \sim \mathcal{N}(P_{\mathrm{mean}}, P_{\mathrm{std}}^{2})$,  $t = \arctan(e^\tau)$, and $r = 0$.

\begin{remarkgray}~\label{thm:remark1}
    sCT with linear paths is of the same form as MeanFlow, as proved in Appendix~\ref{app:remark1}.  
\end{remarkgray} 


\paragraph{Putting it together.} Table~\ref{tab:methodconclude} summarizes the deterministic variants reproduced from the discussed representative methods, including DTSCs and CTSCs, within our framework.  The goal of this reframing is to disentangle the independent components that are often intertwined in prior work. Within our framework, these components can be explicitly separated, such that any reasonable combination of components will yield a functioning model.  In practice, the relative effectiveness of different choices and combinations is the focus of our investigation in Sec.~\ref{sec:elucidating}.

\subsection{Discussion: shortcutting flow paths under marginal velocity fields}
\textbf{\emph{- Q.1: Why share a common design frame?}} \vspace{-0.25em} 

 We inherently aim to simulate the PF-ODE with the \emph{marginal velocity field}, written as $\bm{v}_t(\bm{x})$.
% $
% \bm{v}_t(\bm{x}) = \int_{\Omega} \bm{v}_t(\bm{x}|\bm{x}_0)\, \frac{p_t(\bm{x}| \bm{x}_0)\, p_{0}(\bm{x}_0)}{p_t(\bm{x})}\, d\bm{x}_0 
% $.
Consequently, shortcut models essentially operate along the sampling trajectories of the flow governed by $\bm{v}_t(\bm{x})$, as shown in Fig.~\ref{fig:phyintuition}(a).
Intuitively, the ideal construction of the learning target is to sample two distinct states $\bm{x}_t$ and $\bm{x}_r$ along the same curved trajectory from the flow paths,  
so that the neural network can directly map $\bm{x}_t$ to $\bm{x}_r$ such that $
F^{\theta}(\bm{x}_t, t, r) \approx \bm{x}_r ,
$ as illustrated in Fig.~\ref{fig:phyintuition}(b) and~(c).

However, such pairs $\{\bm{x}_t, \bm{x}_r\}$ cannot be obtained via simulation-based sampling:  
once $\bm{x}_t$ is sampled, $\bm{x}_r$ remains inaccessible because both $\bm{v}_t(\bm{x})$ and its integral from $t$ to $r$ are intractable.  
To overcome this, a common design paradigm is employed, which is to let the network’s outputs,  
or the conditional velocity alternatively, estimate $\bm{x}_r$ in two steps: first producing  
an intermediate $\hat{\bm{x}}_s$, and then constructing an estimated target $\hat{\bm{x}}_r$, as shown in Eq.~\ref{eq:flowmapconsist} in Sec.~\ref{sec:designframe}.  
This makes training feasible. Although one may also construct multi-step (\emph{i.e.}, more than two) flow map targets for simulating the $\{\bm{x}_t, \bm{x}_r\}$ pairs~\citep{kim2023consistency}, the paradigms of two-step target construction approximated by one-step prediction are sufficiently general according to the following theoretical justification with detailed proof in Appendix~\ref{app:thmbound}. Note that we classify the aforementioned methods' training objective into DTSC and CTSC. For example, $l_{\text{mf}}$ and $l_{\text{sct}}$ are  instances of $l_{\text{ctsc}}$.
\begin{theoremgray}
    [Error bound of DTSC\&CTSC (brief)]~\label{thm:errorbound} Under the mild assumptions with details given in Theorem~\ref{thm:apperrorbound} of (i) one-sided Lipschitz condition of marginal velocity and (ii) twice continuous differentiability with bounded second derivatives of $X_{\tau_1,\tau_2}^\theta$ for any $\tau_1,\tau_2 \in [0,1]$. Let $p_0$ the density of $\bm{x}_0$, and ${p}^{\theta}_0$ the density of $\bm{x}^\theta_0 = X^\theta_{1,0}(\bm{x}_1)$,  under the squared $l_2$-distance:
  \begin{align*}
   &W_2^2(p_0, p^\theta_0) \leq C_1 \mathcal{L}_\text{dtsc}(\theta) + C_2(t-s); 
   &W_2^2(p_0, p^\theta_0) \leq C_3 \mathcal{L}_\text{ctsc}(\theta), 
  \end{align*}
    where we write the training objective in Eq.~\ref{eq:flowmapconsist} as $\mathcal{L}_\bullet(\theta) = \mathbb{E}_{r,s,t\sim p(\tau), \,\bm{x}_t\sim p_t}[
    l_\bullet(\bm{x}_t,r,s,t;\theta)]
$ with $\bullet \in \{\text{dtsc}, \text{ctsc}\}$, $W_2(\cdot,\cdot)$ is the Wasserstein-2 distance, $\{C_1, C_2\}$ are given in Theorem~\ref{thm:dtscbound}, and $C_3$ is given in Theorem~\ref{thm:ctscbound1} and~\ref{thm:ctscbound2} in Appendix~\ref{app:thmbound}.
\end{theoremgray}
\textbf{\emph{- Q.2: What challenges in constructing flow map targets?}} \vspace{-0.25em} 

From this perspective, ideal learning for DTSC and CTSC shares a similar physical picture as shown from Fig.~\ref{fig:phyintuition}(b) and~(c).  
However, the practical construction of the two-step flow map target inevitably causes the obtained 
$\hat{\bm{x}}_s$ and $\hat{\bm{x}}_r$ to deviate from $\bm{x}_s$ and $\bm{x}_r$ on the sampling trajectory governed by marginal velocity fields as shown in Fig.~\ref{fig:phyintuition}(d) and~(e), leading to bias and variance in estimating $\bm{x}_r$ with $\hat{\bm{x}}_r$.  
Introducing this deviation into the supervision of model training greatly affects the performance differences across various shortcut model designs as justified in Prop.~\ref{thm:infererror}.  

\textbf{\emph{- Q.3: Why distillation from pretrained velocity fields performs better?}} \vspace{-0.25em} 

From another perspective, this explains why distilling from a pretrained diffusion model is often more effective than training from scratch~\citep{cm,scm}. Unlike (s)CT, which are trained from scratch, (s)CM benefits from distillation by learning from a pretrained velocity field $\bm{v}_t^{\phi}(\bm{x})$. In practical training, the conditional velocity $\bm{v}_{t|0}$ and network output $\bm{v}_t^{\theta}$ in $\mathrm{sg}(\cdot)$ in Eq.~\ref{eq:ctloss} and~\ref{eq:sctloss} are replaced with $\bm{v}_t^{\phi}$, which closely approximates $\bm{v}_t(\bm{x}_t)$. This substantially reduces errors in estimating the two-step flow targets, providing more accurate supervision for network training.

% From another perspective, this also explains why distilling from a pretrained diffusion model  
% is often more effective than training from scratch, as demonstrated by~\cite{cm} and~\cite{scm}.
% Compared with (s)CT, which trains from scratch,  
% (s)CM achieves better performance.  
% The reason is that in practice, (s)CM as the distillation variant of (s)CT,  learns from an already pretrained velocity field $\bm{v}_t^{\phi}(\bm{x})$, where the conditional velocity $\bm{v}_{t|0}$ and network output $\bm{v}_t^{\theta}$ in $\mathrm{sg}(\cdot)$ operator in Eq.~\ref{eq:ctloss} and~\ref{eq:sctloss}
% are replaced with $\bm{v}_t^{\phi}$ when training. Because the employed velocity field satisfies 
% $\bm{v}_t^{\phi}(\bm{x}) \approx \bm{v}_t(\bm{x})$, the deviation in estimating the two-step flow map targets $\hat{\bm{x}}_r$ is substantially reduced, thereby providing more accurate supervision for the training of neural networks.  

\section{Elucidating the design space of shortcut models}\label{sec:elucidating} \vspace{-0.25em}
According to our design framework, we analyze existing shortcut models from several key perspectives, including the \emph{\textbf{choice of flow path}} and \emph{\textbf{design of time sampler}}, which primarily determine how the flow map is constructed. In the following, we aim to address several corresponding questions to empirically and theoretically elucidate the design space of one-step shortcut models.

Empirically, we evaluate the proposed formulation using a unified codebase implementation with the same training iterations and batch sizes. 
For unconditional generation on CIFAR-10, we employ U-Nets~\citep{sde}~($\sim$\texttt{55M param.}) as the network architecture operating directly in the pixel space. 
For conditional generation in ImageNet-256$\times$256, with and without classifier-free guidance, we use a SiT-B/2~\citep{sit} architecture~($\sim$\texttt{131M param.}), operating in the latent space via a pretrained VQVAE~\citep{stablediff}. 
    While sCT is originally initialized from the teacher diffusion model as stated in \cite{scm}, we train all the discussed models from scratch, for a fair comparison.
 Fig.~\ref{fig:elucidsc} summarizes the results of one-step generation on the two datasets, with additional setting of classifier-guidance-free learning, as discussed in \cite{meanflow}.
Further details on settings are provided in Appendix~\ref{app:elucidexpsetting}.

 
\textbf{ \emph{- Q.1: Following linear or cosine paths?}}\vspace{-0.25em} 

Linear paths are generally regarded as more analytically tractable and easier to employ for training and sampling tricks (\emph{e.g.}, classifier-free guidance), owing to their simple formulation. By contrast, in pixel-space generative modeling, cosine paths are often considered more stable for training convergence, because they induce a stochastic process with fixed variance. Exploration of these two flow paths in the context of shortcut models remains underexplored. Here we extend cosine-path-based models (CT and sCT) to their linear-path counterparts. Fig.~\ref{fig:elucidsc} shows that shortcut models with linear paths are more competitive. We attribute this to the fact that the marginal velocity fields generated by linear paths as conditional paths are at lower convex transport cost~\citep{recflow}, implying lower curvature of the velocity-field-governed trajectories. Consequently, the simulated two-step flow map targets are less likely to deviate from the ideal. Furthermore, while cosine paths are optimal in the setting of diffusion and flow matching~\citep{santos2023usingornsteinuhlenbeckprocessunderstand} under Fisher information metrics, we theoretically justify that \emph{linear paths in the setting of shortcut models are optimal conditioned on data samples} in Appendix~\ref{app:thmlinearopt}. Based on this, our subsequent analysis will focus on the linear path. 
% \begin{propositiongray}\label{prop:linearopt}
% For $\forall \bm x_0$, the schedule $(\alpha_t, \sigma_t)$ under linear path is the optimal schedule considering $p(\bm u_{0,t} (\bm x_t) \mid \bm x_0, t)$  under the Fisher information metric.
% \end{propositiongray}
% \begin{propositiongray}
%     lower convex transport cost~\citep{recflow}, implying lower curvature of the velocity-field-governed trajectories. Consequently, the simulated two-step flow map targets are more likely to deviate less from the ideal. Based on this, our subsequent analysis will mainly focus on the linear path.
% \end{propositiongray}
    
\begin{figure*}
\begin{minipage}{1.00\linewidth}
    \centering\vspace{-1em}\hspace{-1em}
        \subfigure[Uncond. CIFAR ]{
            \includegraphics[width=0.33\linewidth, trim=20 00 30 30, clip]{cifar_uncond.pdf}
            \label{fig:elucidsccifar}
            }\hspace{-0.5em}
        \subfigure[Cond. ImageNet]{
            \includegraphics[width=0.33\linewidth, trim=20 00 30 30, clip]{imgnet_cond.pdf}
            \label{fig:elucidscimgcnd}
            }\hspace{-0.5em}
        \subfigure[CFG. ImageNet]{
            \includegraphics[width=0.33\linewidth, trim=20 00 30 30, clip]{imgnet_cfg.pdf}
            \label{fig:elucidscimgcfg}
            }\hspace{-1em}\vspace{-1em}
        \caption{Comparison of FID50k during training among different shortcut models described in Table~\ref{tab:methodconclude}. (a) is the unconditional~(Uncond.) generation on CIFAR-10; (b) is class-conditional~(Cond.) generation; and (c) is classifier-free-guidance~(CFG.) training on ImageNet-256$\times$256.  }\label{fig:elucidsc}
        \label{fig:deltabind}
    \hfill
\end{minipage}
\vspace{-2.2em}
\end{figure*}

\textbf{ \emph{- Q.2: Shortcutting flow paths discretely or continuously?}} \vspace{-0.25em} 

Under the same training setup and within a unified codebase, continuous-time shortcut models clearly outperform their discrete-time counterparts. As shown in Fig.~\ref{fig:elucidsccifar}, both sCT and MeanFlow achieve lower FID50k scores on CIFAR-10 compared to CT and SCD. A similar conclusion can be drawn on ImageNet-256$\times$256 from Fig.~\ref{fig:elucidscimgcnd}\&\ref{fig:elucidscimgcfg}. Below, we analyze the inference error of the discussed methods with linear paths in Prop.~\ref{thm:infererror}, and characterize the regimes in which each objective is preferable. We denote sCT and MeanFlow with linear paths by subscripts ${\mathrm{ctsc}}$, thanks to their same formulations according to Remark~\ref{thm:remark1}, and discrete-time models by $\mathrm{dtsc}$ as well. In addition, we write the parameterized $\bm{v}^\theta_t$ in sCT as $\bm{u}^\theta_{t,0}$ under the linear path according to Appendix~\ref{app:remark1}. 
\begin{propositiongray}
    [Inference error analysis] 
\label{thm:infererror}
Under mild regularity conditions shown in Appendix~\ref{app:assumptions}, 
the Wasserstein-2 distance of shortcut models with one-step generation is bounded as:
\begin{align}
&W_2^2(p_0, p^\theta_0)
\leq  2\left(\mathrm{BV_{\text{ctsc}}} +8 \mathrm{Var}\big[\frac{d}{dt} \bm u_{t,r}^\theta(\bm x_t)\big] 
+ 8\sigma_{\bm{v}_{t|0}}^2 \right) \Big|_{r=0,t=1}, \label{eq:bverror}\\
&W_2^2(p_0, p^\theta_0) \leq 2\Big( \mathrm{BV_{\text{dtsc}}} 
+ 8\delta_2^2\,\mathrm{Var}\big[\bm u_{s,r}^\theta(\bm x_t)\big]
+8(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\text{dtsc}}^2\Big)\Big|_{r=0,t=1}, ~\label{eq:dtscbverror}
% &W_2^2(p_0, p^\theta_0) \leq 2\Big( \mathrm{BV_{\text{scd}}}
% + 8\delta_2^2\,\mathrm{Var}\big[\bm u_{s,r}^\theta(\bm x_t)\big]
% +8(1+\ell^2\delta_2^2)\delta_1^2\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,s)\big]\Big)\Big|_{r=0,t=1},
\end{align}
where $\mathrm{BV_{\bullet}} = \mathrm{Bias}^2_{\bullet\text{-tgt}} + \mathrm{Bias}^2_{\bullet\text{-loss}} +2\mathrm{Var}\!\big[\bm u_{t,r}^\theta(\bm x_1)\big]$  with $\bullet\in\{\mathrm{ctsc},\mathrm{dtsc}\}$, and $\mathrm{Bias}^2_{\bullet\text{-tgt}}$ and $\mathrm{Bias}^2_{\bullet\text{-loss}}$ are defined in Prop.\ref{app:thmomfererror} ; $\delta_1 = t-s$, $\delta_2 = s-r$;  $\ell$ is the local Lipschitz constant of $\bm u^\theta$; $\sigma_{\bm{v}_{t|0}}^2$ is the variance of the conditional velocity, defined by $
\sigma_{\bm{v}_{t|0}}^2 \coloneqq \mathrm{Var}(\bm v_{t}(\bm x_t| \bm x_0))$; $\sigma_{\text{dtsc}}^2 = \sigma_{\bm{v}_{t|0}}^2$ for CT's two-step flow map targets, or $\sigma_{\text{dtsc}}^2 = \mathrm{Var}\!\big[\bm u_{t,s}^\theta(\bm x_t)\big]$ when using SCD's flow map targets.
\end{propositiongray}
From Theorem~\ref{thm:errorbound}, for CT and CTSC, we conclude that if 
\(\delta_2^2\mathrm{Var}\!\big[ \bm u_{t,r}^\theta(\bm x_t)\big]\) 
and \(\mathrm{Var}\!\big[\tfrac{d}{dt} \bm u_{t,r}^\theta(\bm x_t)\big]\) 
are of the same order, the right-hand side of Eq.~\ref{eq:dtscbverror} contains an additional term 
\(\ell^2\delta_2^2\delta_1^2\sigma_{\text{dtsc}}^2\) compared with Eq.~\ref{eq:bverror}, which is likely to result in higher inference error and instability in training, as the proof in Appendix~\ref{app:thmomfererror} shows the inference error already subsumes the training error bound. Further, 
when \(s \to t\), and \(\sigma_{\bm{v}_{t|0}}^2\) dominates both 
\(\delta_2^2\mathrm{Var}\!\big[ \bm u_{t,r}^\theta(\bm x_t)\big]\) 
and \(\mathrm{Var}\!\big[\tfrac{d}{dt} \bm u_{t,r}^\theta(\bm x_t)\big]\), the training convergence and sampling fidelity of CTSC and CT are both closely tied to the variance of the conditional velocity used for supervision. 
Therefore, being able to provide a low-variance velocity supervision during training, such as one obtained from a pretrained neural network, helps to improve shortcut models.

\textbf{ \emph{- Q.3: Fixing the terminal time or not?}}\vspace{-0.25em}

Since sCT-linear is a special case of MeanFlows where the terminal time $r$ is fixed at $0$, the empirical results on CIFAR-10 in Fig.~\ref{fig:elucidsccifar} and on ImageNet-256$\times$256 in Fig.~\ref{fig:elucidscimgcnd}\&\ref{fig:elucidscimgcfg} demonstrate that, in general, random sampling of $r$ is beneficial in capturing the overall shortcut patterns. However, in the early stage of training (approximately before 20–40k epochs), sCT-linear exhibits faster convergence in terms of FID50k for one-step generation. We conjecture that in the early stages, continually adding supervision of $\bm{x}_0$, akin to a denoising task, provides a simpler learning task that accelerates convergence toward favorable local optima. Yet, without intermediate flow path targets $\bm{x}_r$ where $r>0$, the model may remain stuck in these sub-optima during the later training stage.

% Second, during sampling, since DDIM can be viewed as a first-order approximation of DPM-Solver where the velocity in PF-ODE is approximated by a constant direction, its sampling error is relatively smaller under linear paths, while under cosine paths,  errors  from DDIM are typically larger and the sampling requires higher-order solvers for compensation.

\section{Improvements to training}\label{sec:improvement}
Building on the above analysis, all subsequent techniques and developments will be carried out under the \textit{continuous-time} shortcut model with  \textit{linear paths}, so we choose MeanFlow with SiT-B/2 architecture as our baseline implementation with its default hyperparameters shown in Appendix~\ref{app:imgnetablation}. Table~\ref{tab:imgnetablation} presents an ablation study that shows the effectiveness of our improvement techniques, where ESC as \underline{e}xplicit\&\underline{e}asier \underline{s}hort\underline{c}ut model is the CTSC with all the proposed techniques as follows. 

\paragraph{Plug-in velocity instead of conditional one.} Since the marginal velocity is intractable, training relies on the conditional velocity, obtained by sampling $\bm{x}_0$ from the finite training set $\{\bm{y}^{(i)}\}_{i=1}^{N}$. Based on it, we derive $\bm{v}^*_t(\bm{x}_t|\{\bm{y}^{(i)}\}_{i=1}^{N})$ as the marginal velocity under the empirical data distribution, which we refer to as the ideal velocity in the following:
\begin{propositiongray}[Marginal velocity of empirical distribution and bias-variance comparison]~\label{thm:idealvel} 
     Assume the data distribution is the empirical distribution, as $p_{{0}}(\bm{y}) = \frac{1}{N}\sum_{i=1}^{N} \mathds{1}_{\bm{y}_i}(\bm{y}) $, the marginal velocity reads
    \begin{align}
        \bm{v}^*_t(\bm{x}_t| \{\bm{y}^{(i)}\}_{i=1}^{N}) = \sum_{i}^{N}\frac{\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(i)},\sigma^2_t\bm{\mathrm{I}})}{\sum_{j}^{N}\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(j)},\sigma^2_t\bm{\mathrm{I}})}(\dot\alpha_t\bm{y}^{(i)} + \frac{\dot\sigma_t}{\sigma_t}(\bm{x}_t - \alpha_t \bm{y}^{(i)}) ), \label{eq:idealvel}
    \end{align}
    where $\bm{x}_t = \alpha_t\bm{x}_0 + \sigma_t\bm{\varepsilon}$. Specifically, under mild assumptions in Prop.~\ref{app:thmvelbv} in Appendix~\ref{app:bvanalysisplugin}, substituting $\bm v_t$ in $\mathcal{L}_\text{ctsc}$ with $\bm v^*_t$ significantly decreases Eq. \ref{eq:bverror}'s last term $\sigma_{\bm v_{t\mid0}} = \mathbb{E}\|\bm v_{t\mid0}-\bm v_t\|^2$, which reduces the variance by $\mathcal{O}(1-1/N)$ while increasing the bias by $\mathcal{O}(1/N)$. 
\end{propositiongray}
\begin{figure}
\begin{minipage}{0.48\linewidth}
  \centering
  \captionof{table}{Evaluation of training improvements under one-step generation with SiT-B/2 as $F^\theta$.} \vspace{-1em}~\label{tab:imgnetablation}
\resizebox{1\linewidth}{!}{
  
  \begin{tabular}{@{} llr @{}}
\toprule
\multicolumn{2}{l}{Training configuration} & FID50k \\
\midrule
& \multicolumn{1}{l}{MeanFlow under CFG. (Baseline)} & 6.09 \\
\rowcolor{gray!10} +A1 & Plug-in velocity ($p_{\text{plug-in}} = 1.0$) & 6.01 \\
 +A2 & Plug-in velocity ($p_{\text{plug-in}} = 0.5$) & 5.98 \\
\rowcolor{gray!10}+B1 & \begin{tabular}[c]{@{}l@{}}Plug-in velocity ($p_{\text{plug-in}} = 1.0$)\\ \& class-consistent batching\end{tabular} & 6.08 \\
 +B2 & \begin{tabular}[c]{@{}l@{}}Plug-in velocity ($p_{\text{plug-in}} = 0.5$)\\ \& class-consistent batching\end{tabular} & 5.96 \\
\rowcolor{gray!10}+C & Gradual time sampler & 5.99 \\
 +D & sCM training techniques & 5.95 \\
\midrule
\rowcolor{gray!10}& \multicolumn{1}{l}{\textbf{ESC} (Baseline + B2 + C + D)} & \textbf{5.77} \\
\bottomrule
\end{tabular}
  }\vspace{-1em}
\end{minipage}
% \begin{figure}[t]
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
\definecolor{codesign}{RGB}{0, 0, 255}
\definecolor{codefunc}{rgb}{0.85, 0.18, 0.50}
\definecolor{codegreen}{rgb}{0.0, 0.6, 0.4}
\lstdefinelanguage{PythonFuncColor}{
  language=Python,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{codeblue},  % for lines starting with "#"
  stringstyle=\color{orange},
  showstringspaces=false,
  basicstyle=\ttfamily\small,
  % Match function calls and color them
  literate=
    % functions with one arg
    {*}{{\color{codesign}* }}{1}
    {-}{{\color{codesign}- }}{1}
    % {=}{{\color{codesign}= }}{1}
    {+}{{\color{codesign}+ }}{1}
    % function call pattern (common names)
    {Normal}{{\color{codegreen}\textbf{Normal}}}{1}
    {softmax}{{\color{codefunc}softmax}}{1}
    {matmul}{{\color{codefunc}matmul}}{1}
        {randn_like}{{\color{codefunc}randn\_like}}{1}
    {log\_prob}{{\color{codefunc}log\_prob}}{1}
    {sum}{{\color{codefunc}sum}}{1}
    {stopgrad}{{\color{codefunc}stopgrad}}{1}
    {metric}{{\color{codefunc}metric}}{1}
}
\lstset{
  language=PythonFuncColor,
  backgroundcolor=\color{white},
  basicstyle=\fontsize{8.9pt}{9.9pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
}
\begin{minipage}{0.45\linewidth}
% \vspace{1em}
\begin{algorithm}[H]
\caption{{Calculation of Plug-in Velocity}.}~\label{alg:code}\vspace{-0.8em} 
\begin{lstlisting}
# x: training batch (B,D)
# t: sampled time
e = randn_like(x)
xt = (1-t) * x + t * e
x_ex, xt_ex = x[:,None,:], xt[None,:,:]
eps = (xt_ex - (1-t) * x_ex) / t

logp_fn = Normal(0, 1).log_prob
logp = sum(logp_fn(eps), dim=2)
weight = softmax(logp, dim=0)

v_cnd = eps - x_ex
v_plugin = matmul(weight.T, v_cnd)
\end{lstlisting}\vspace{-0.5em}
\end{algorithm}
\end{minipage}
% \end{figure}
\vspace{-1em}
\end{figure}
Replacing the conditional velocity $\bm{v}_{t|0}$ in Eq.~\ref{eq:bverror} with the ideal velocity obtained from the full training set the variance of the velocity term to $\mathcal{O}(1/N)$. As a result, since $N$ is usually a large number, according to Prop.~\ref{thm:errorbound}, employing the ideal velocity field can therefore provide more stable supervision during training and lower error in inference. However, its computation requires summing over the entire data set, which is infeasible for large-scale data such as ImageNet ($N=1,281,167$). To address this limitation, we adopt the \textit{plug-in velocity} during training instead, which reads $\bm{v}^*_t(\bm{x}_t|\{\bm{y}^{(i)}\}_{i=1}^{B})$. The above computation is restricted to a mini-batch $\{\bm{y}^{(i)}\}_{i=1}^{B}$ with pseudocode implementation provided in Algorithm~\ref{alg:code}. This can be viewed as a mixture of conditional velocities from the mini-batch samples, reducing the level of variance $\sigma_{\bm{v}_{t|0}}$ in Eq.~\ref{eq:bverror} to  $\mathcal{O}({1}/{B})$, at the minor cost of increased bias. Theoretically, we give further details on the validity of the training objective employing plug-in velocity in Prop.~\ref{app:thmplugin} in Appendix~\ref{app:proofplugin}.

\paragraph{Plug-in velocity under guidance training.}  From the comparison between Fig.~\ref{fig:elucidscimgcnd} and Fig.~\ref{fig:elucidscimgcfg}, it is evident that classifier-free guidance (CFG) is crucial for high-quality image generation~\citep{meanflow}. With CFG, the class-conditional velocity $\bm{v}_t(\bm{x}_t | \bm{x}_0, c)$ leverages instance-level supervision from the label $c$.  In contrast, 
\(
\bm{v}^*_t(\bm{x}_t | \{(\bm{y}^{(i)},c^{(i)})\}^B_{i=1}),
\)
is computed by averaging over randomly drawn mini-batches, which is likely to dilute or erase the class-specific signal. To this end, we employ a plug-in probability $p_{\text{plug-in}}$ that substitutes the conditional velocity with the plug-in velocity, as a trade-off between lowering variance during training and retaining class guidance. The other trick is \emph{class-consistent mini-batching}: When applying CFG during training, we ensure that each mini-batch is sampled within the same class. In multi-GPU training, the class labels of mini-batches across different processes are independent of each other.
% This trick guarantees the supervision of consistent class guidance, while the loss function for gradient updates still accounts for diverse class samples in every each training iteration.

\paragraph{Gradual time sampler from sCT to MeanFlow.} As discussed in Q.3 from Sec.~\ref{sec:elucidating}, we design a time-sampling schedule that gradually evolves with training iterations. 
During the first $K_{\text{fix0}}$ iterations, the sampler selects $r=0$ with probability $p_{\text{fix0}}$, 
and with probability $1-p_{\text{fix0}}$ follows the MeanFlow sampler shown in Table~\ref{tab:methodconclude}. 
The value of $p_{\text{fix0}}$ decays from $1.0$ to $0$ under a cosine schedule at the beginning of the training, 
so that after $K_{\text{fix0}}$ iterations the sampler fully adopts the MeanFlow's strategy, where $K_{\text{fix0}}$ is usually set to 20k in practice.

\paragraph{Adoption of training techniques.} Moreover, since sCT can be regarded as a variant of CTSC, several training strategies have already been explored in its original work, such as variational adaptive loss weighting~\citep{edm2} and tangent warmup~\citep{scm}. These techniques are also applicable to CTSC and bring performance improvements in the cases given in Appendix~\ref{app:scaleup}.
\section{Scaling-up Evaluation}

\label{sec:exp}
\begin{figure}[H]
\begin{minipage}{0.67\linewidth}\vspace{-1em}
  \centering
  \captionof{table}{Evaluation on ImageNet-256$\times$256. Values with \underline{underline} denote the best except shortcut models, values in \textbf{bold} is the best shortcut diffusion model under one-step generation.} 
  \vspace{-0.7em}~\label{tab:scaleup}
\resizebox{1\linewidth}{!}{
  
  % Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{cllrr}
\toprule
Family                        

& Method                                                          & Param.                & NFE                      & FID50k \\
\midrule
\multirow{3}{*}{%
  \rotatebox{90}{\makebox[0pt][c]{\color{black!60}GAN}}%
}
& {\color{black!60}BigGAN~\citep{biggan}}
& {\color{black!60}112M}
& {\color{black!60}1}
& {\color{black!60}6.95}
\\

& {\color{black!60}GigaGAN~\citep{gigagan}}
& {\color{black!60}569M}
& {\color{black!60}1}
& {\color{black!60}3.45}
\\

& {\color{black!60}StyleGAN-XL~\citep{stylegan}}
& {\color{black!60}166M}
& {\color{black!60}1}
& {\color{black!60}2.30}
\\
\midrule
\multirow{4}{*}{\rotatebox{90}{\makebox[0pt][c]{{\color{black!60}AR/Mask}}}}
& {\color{black!60}AR w/ VQGAN~\citep{ar}} 
& {\color{black!60}227M} 
& {\color{black!60}1024} 
& {\color{black!60}26.52} 
\\
& {\color{black!60}MaskGIT~\citep{maskgit}} 
& {\color{black!60}227M} 
& {\color{black!60}8} 
& {\color{black!60}6.18} 
\\
& {\color{black!60}VAR-d30~\citep{var}} 
& {\color{black!60}2B} 
& {\color{black!60}10$\times$2} 
& {\color{black!60}1.92} 
\\
& {\color{black!60}MAR-H~\citep{mar}} 
& {\color{black!60}943M} 
& {\color{black!60}256$\times$2} 
& {\color{black!60}1.55} 
\\
\midrule

% ---------------- Diff / Flow ----------------
\multirow{6}{*}{\rotatebox{90}{\makebox[0pt][c]{{\color{black!60}Diff/ Flow}}}}
& {\color{black!60}ADM~\citep{edm2}} 
& {\color{black!60}554M} 
& {\color{black!60}250$\times$2} 
& {\color{black!60}10.94} 
\\
& {\color{black!60}LDM-4-G~\citep{stablediff}} 
& {\color{black!60}400M} 
& {\color{black!60}250$\times$2} 
& {\color{black!60}3.60} 
\\
& {\color{black!60}SimDiff~\citep{simdiff}} 
& {\color{black!60}2B} 
& {\color{black!60}512$\times$2} 
& {\color{black!60}2.77} 
\\
& {\color{black!60}DiT-XL/2~\citep{dit}} 
& {\color{black!60}675M} 
& {\color{black!60}250$\times$2} 
& {\color{black!60}2.27} 
\\
& {\color{black!60}SiT-XL/2~\citep{sit}} 
& {\color{black!60}675M} 
& {\color{black!60}250$\times$2} 
& {\color{black!60}2.06} 
\\
& {\color{black!60}SiT-XL/2+REPA~\citep{repa}} 
& {\color{black!60}675M} 
& {\color{black!60}250$\times$2} 
& {\color{black!60}\underline{1.42}} 
\\
\midrule

\multirow{7}{*}{ \rotatebox{90}{\makebox[0pt][c]{Shortcut}}} & iCT~\citep{icm}                                                             & 675M                  & 1                        & 34.24  \\
                                                                                         & SCD~\citep{scd}                                                             & 675M                  & 1                        & 10.60  \\
                                                                                         & IMM~\citep{imm}                                                             & 675M                  & 1$\times2$ & 7.77   \\
                                                                                         & \multirow{2}{*}{MeanFlow~\citep{meanflow}}                                       & \multirow{2}{*}{676M} & 1                        & 3.43   \\
                                                                                         &                                                                 &                       & 2                        & 2.93   \\
                                                                                         & \begin{tabular}[c]{@{}l@{}}\textbf{ESC} (w/o-class-consist.)\end{tabular} & 676M                  & 1                        & 2.92   \\
                                                                                         & \begin{tabular}[c]{@{}l@{}}\textbf{ESC} (w/-class-consist.)\end{tabular} & 676M                  & 1                        & {2.85}
                                                                                        \\
                                                                                        & \textbf{ESC}+ (w/-class-consist.) & 676M & 1 & \textbf{2.53} \\
                                                                                        \bottomrule
\end{tabular}
  }\vspace{-1em}
\end{minipage}
\begin{minipage}{0.35\linewidth}
\begin{minipage}{1\linewidth}
\centering
\includegraphics[width=0.80\linewidth, trim=8 00 25 14, clip]{imgnet_esc.pdf}
\captionof{figure}{Convergence of FID50k.} ~\label{fig:scaleup}
\end{minipage}
\begin{minipage}{1\linewidth}
\centering
\setlength{\tabcolsep}{4pt}
\centering
\captionof{table}{Uncond. CIFAR-10.
}\vspace{-0.6em}
\resizebox{0.6\linewidth}{!}{\begin{tabular}{lrr}
\toprule
{method} & NFE & {FID} \\
\midrule
iCT &  1 & \textbf{2.83} \\
ECT &  1 & 3.60 \\
sCT &  1 & 2.97 \\
IMM &  1 & 3.20 \\
{MeanFlow} & 1 & 2.92 \\
\textbf{ESC} & 1 & \textbf{2.83} \\
\bottomrule
\end{tabular}}
\label{tab:cifar}
\end{minipage}
\end{minipage}
\end{figure}


\paragraph{Setting.} In this part, we evaluate the proposed ESC as an improved variant of CTSCs to illustrate its effectiveness at scale. We conduct a scaling-up experiment on ImageNet-256$\times$256 in latent space, and employ SiT-XL/2~($\sim$\texttt{676M param.}) as the backbone model. We follow the training setting of MeanFlow with CFG, where the model is trained from scratch with 240 epochs~($\sim$\texttt{1.2M} iterations). Furthermore, ESC+ is trained with 480 epochs~($\sim$\texttt{2.4M} iterations). In addition, for CIFAR-10~\citep{cifar}, all the shortcut models use the same U-Net~\citep{unet} architecture from \cite{sde}~($\sim$\texttt{55M param.}). The code repository is provided for reproducibility\footnote{\small{\url{https://github.com/EDAPINENUT/ExplicitShortCut/}}}. For further details on setting, please refer to Appendix~\ref{app:scaleup}.  

\paragraph{Benchmark comparison.} In Table~\ref{tab:scaleup}, 
we compare our results with previous methods by benchmarking the FID50k under one-step generation~(1-NFE).  In the context of single-step generation, the proposed techniques bring more improvements with the large-scale network architecture~(SiT-XL/2)  than with the basic one~(SiT-B/2), as ESC achieves state-of-the-art performance of an FID50k of 2.85 with 240 epochs and 2.53 with 480 epochs. This represents an improvement of 16.9\% and 26.2\% compared to the prior one-step result of 3.43 obtained by MeanFlow, respectively, and even better than the two-step generative fidelity of MeanFlow (FID50k 2.93). For visualization of images generated by ESC with different network architectures, please refer to Appendix~\ref{app:vis}. Moreover, Table~\ref{tab:cifar} gives unconditional generation results on CIFAR-10, showing that our improved models achieve competitive performance with prior approaches.
    For a full comparison including other families of methods, please refer to Appendix~\ref{app:fullcomp}. Notably, we find that the performance gains from ESC with SiT-XL/2 over MeanFlow baseline are much more significant than it with SiT-B/2, which we discuss in Appendix~\ref{app:xlbetterb}.
\vspace{-0.5em}
\paragraph{The time cost of plug-in velocity is minimal.} Computing plug-in velocity involves an $\mathcal{O}(B^2)$ weighted operation within each mini-batch, but with DDP training, per-device batch size is small ($B=16$ in our experiments). As a result, the extra overhead is negligible because profiling over 1M iterations shows $554$ ms/iter vs.~$558$ ms/iter for conditional vs.~plug-in velocity ($\approx 0.7\%$ increase). Despite a small batch size introducing larger estimation variance and bias relative to the ideal velocity, compared to the conditional velocity, it stabilizes training by theoretically reducing variance by $\mathcal{O}(1-1/B)$ at almost no additional computational cost and a minor increase in estimation bias.
\vspace{-0.5em}
\paragraph{Class-consistent mini-batching brings faster convergence.} While the final reported results show comparable performance with and without class-consistent mini-batching, we observe from Fig.~\ref{fig:scaleup} that the convergence of FID50k during training is substantially faster with the technique, where Appendix~\ref{app:convminibatch} gives full details. This suggests that the training technique is advantageous in scenarios requiring finetuning with limited training iterations. Exploring its broader applications will be a direction for future work.

% \paragraph{Two-step generation exhibits slow convergence. } We find that two-step generation (2-NFE) converges more slowly in CTSC, and similarly in MeanFlow, where its fidelity only surpasses one-step generation after 1000+ epochs as reported in its paper. This is mainly due to the model’s limited ability in early and mid-term training stages to accurately map intermediate noisy samples to their future evolution. Developing approaches that accelerate multi-step convergence while maintaining the high performance of one-step generation is an important direction for future work.
\vspace{-0.5em}
% \paragraph{Large models gain more performance from low variance training.} As shown, performance improvement for SiT-XL/2 over the MeanFlow is 16.9\%, while it is 5.3\% for SiT-B/2 architecture. We attribute the performance gap to two key factors: 
% \begin{itemize}
%     \item  \textbf{Optimization Dynamics.} In larger networks (e.g., XL/2), the representational capacity increases substantially, amplifying the impact of optimization stability. As shown in Figure~\ref{fig:cifartrainingcomp} in Appendix~\ref{app:trainingstability}, MeanFlow exhibits higher variance and less stable loss behavior during training, whereas ESC maintains stable optimization and is therefore more likely to converge to a better solution. In smaller models (e.g., B/2), the representational capacity is nearly saturated, leaving limited room for further improvement. In contrast, for larger models, ESC’s improved stability enables it to better exploit the additional capacity, resulting in more noticeable performance gains.
%     \item  \textbf{Statistical Generalization.} As the parameter space dimensionality increases, gradient noise also grows, making variance-reduction mechanisms—such as EMA, momentum, gradient clipping, or the proposed plug-in velocity—more beneficial. This observation aligns with the theoretical intuition in \cite{kaplan2020scalinglawsneurallanguage}, where the generalization gap (or overfitting) is linked to the variance term scaling as $\mathcal{O}(1/D)$ with $D$ the hidden dimension. Within the scaling law framework, bias dominates in smaller models, while variance becomes the main factor as the model scales up. To illustrate this, we compare the FID convergence curves of ESC-B/2 vs. MeanFlow-B/2 (trained for 600k iterations) and ESC-XL/2 vs. MeanFlow-XL/2 (trained for 1.2M iterations), as shown in Figure~\ref{fig:imagenetmfescfidconv} in Appendix~\ref{app:imgnetfidconv}. Empirically, in the smaller B/2 setting, both methods converge rapidly to similar FID values. However, in the larger XL/2 model, MeanFlow’s FID curve plateaus in the later training stages, while ESC continues to improve and reaches 2.85. This suggests that in large-scale models, variance dominates generalization behavior, and the variance reduction introduced by plug-in velocity significantly enhances final performance.
% \end{itemize}
% \vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
We focus on one-step shortcut models trained from scratch and propose a general design framework with theoretical justification of its validity. Building on this, we elucidate the design space of shortcut models through theoretical analysis and empirical evidence, and further propose improvements for continuous-time shortcut model training. Our improved model achieves state-of-the-art performance in image synthesis. More broadly, our work lowers the barrier to innovation in one-step diffusion and enables more systematic exploration of their design, with limitations discussed in Appendix~\ref{app:limitation}.
% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.
\section*{Acknowledgements}
This work was supported by National Science and Technology Major Project (No.~2022ZD0115101),
National Natural Science Foundation of China Project (No.~624B2115, No.~U21A20427), Project
(No.~WU2022A009) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake
University, Project (No.~WU2023C019) from the Westlake University Industries of the Future
Research Funding. In addition, we gratefully acknowledge the continuous support from DP Technology, Beijing AI for Science Institute, and Westlake University Center for High-performance Computing, for their sustained provision of computational resources for this project.

\section*{Ethics Statement}
This work investigates one-step diffusion for generative modeling at the methodological level. 
The datasets used in this study are publicly available benchmark datasets 
and do not contain sensitive or personally identifiable information (e.g., ImageNet, CIFAR-10). 

Potential risks include the possibility of misuse, such as generating 
misleading or harmful content, or propagating societal biases present 
in the training data. Our method itself does not explicitly address 
these issues, but we highlight that appropriate safeguards should be 
adopted in downstream applications, including content filtering, 
bias auditing, and domain-specific restrictions. 

Overall, we believe the contributions of this work pose minimal ethical risks 
and can positively impact the community by advancing the efficiency and 
effectiveness of one-step generative modeling.

\section*{Reproducibility Statement}
We have made every effort to ensure the reproducibility of our results. 
All datasets used in this work are publicly available (e.g., ImageNet, CIFAR-10). 
The preprocessing steps, model architectures, training hyperparameters, and evaluation 
protocols are described in detail in Sections~\ref{sec:elucidating} and~\ref{sec:exp}. 

To further facilitate reproducibility, we release our source code through \url{https://github.com/EDAPINENUT/ExplicitShortCut/}, and will release trained model checkpoints and experiment scripts upon publication. This will allow researchers to 
reproduce all reported results and extend our approach in future work. 

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\newpage
\appendix

% 不需要手动 addcontentsline；用 \part 会自动成为一个“分卷”
% \addcontentsline{toc}{section}{Appendix}  % ← 建议删掉
\renewcommand \thepart{}
\renewcommand\partname{} % 清掉 partname
\renewcommand{\mtcskip}{\vskip 8pt}
\part{Appendix}     % 这里会作为一个“Part”，但无编号（前面已 \renewcommand \thepart{}）
\vspace{6mm}
{\setlength{\parskip}{8pt}   % 条目之间额外空隙
 \setlength{\parindent}{0pt} % 避免缩进
 \parttoc
}
\newpage
\begin{figure}[ht]
    \centering
    \setlength{\tabcolsep}{0pt} % 去掉列间距
    \renewcommand{\arraystretch}{0} % 去掉行间距
    \begin{tabular}{cccccccc}
        \includegraphics[width=0.123\linewidth]{esc-b/000008.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000046.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000064.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000095.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000209.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000284.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000542.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000914.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-b/001685.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001007.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001072.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001182.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001220.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000969.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001449.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001748.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-b/001854.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/002125.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/002609.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/002816.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/002898.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/004179.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/006618.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/000759.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-b/039526.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/039751.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/048348.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/048722.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/049023.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001184.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/049806.png} &
        \includegraphics[width=0.123\linewidth]{esc-b/001051.png}
    \end{tabular}
    \caption{Images generated by ESC with SiT-B/2 trained on ImageNet-256$\times$256, with FID50k 5.77.} ~\label{fig:imgescb}
\end{figure}



\begin{figure}[ht]
    \centering
    \setlength{\tabcolsep}{0pt} % 去掉列间距
    \renewcommand{\arraystretch}{0} % 去掉行间距
    \begin{tabular}{cccccccc}
        \includegraphics[width=0.123\linewidth]{esc-xl/000008.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000046.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000064.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000095.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000209.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000284.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000542.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000914.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-xl/001685.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001007.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001072.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001182.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001220.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000969.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001449.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001748.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-xl/001854.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/002125.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/002609.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/002816.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/002898.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/004179.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/006618.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/000759.png} \\[-2pt]
        \includegraphics[width=0.123\linewidth]{esc-xl/039526.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/039751.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/048348.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/048722.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/049023.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001184.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/049806.png} &
        \includegraphics[width=0.123\linewidth]{esc-xl/001051.png}
    \end{tabular}
    \caption{Images generated by ESC with SiT-XL/2 trained on ImageNet-256$\times$256, with FID50k 2.85.} ~\label{fig:imgescxl}
\end{figure}


\section{Background of Diffusion Models}
\subsection{Stochastic Interpolants and Flow Map}~\label{app:preliminary}
Here we give a more formal definition of stochastic interplants and flow map:
\begin{definition}[Stochastic Interpolants \citep{stochasticinterp}]
    The stochastic interpolant $\bm{I}_t$ between probability densities $q$ and $p_1=\mathcal{N}(0,\bm{I})$ is the stochastic process given by
    \begin{equation}
        \bm{x}_t = \alpha_t \bm{x}_0 + \sigma_t \bm{z},
    \end{equation}
    where $\alpha_t,\sigma_t \in C^1([0,1])$ satisfy $\alpha_0=\sigma_1=1$ and $\alpha_1=\sigma_0=0$. We denote the distribution of $\bm{x}_t$ as $p_t$.
\end{definition}

\begin{proposition}[Probability Flow]
    For all $t \in [0,1]$, the probability density of $\bm{x}_t$ is the same as the probability density of the solution to
    \begin{equation}
        \dot{\bm{x}}_t = \bm{v}_t(\bm{x}_t), \quad  \bm{x}_0 \sim p_0(\bm{x}),
        \label{eq:pflow}
    \end{equation}
    \textit{where $\bm{v}: [0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}^d$ is the time-dependent velocity field (or drift) given by}
    \begin{equation}
        \bm{v}_t(\bm{x}) = \mathbb{E}_{\bm{x}_0\sim p_0, \bm{z}\sim\mathcal{N}(0,\bm{I})}[\dot{\bm{x}}_t \mid \bm{x}_t = \bm{x}].
    \end{equation}
\end{proposition}

More specifically, 
\begin{equation}
     \bm{v}_t(\bm{x}) = \dot{\alpha}_t \,\mathbb{E}(\bm{x}_0 \mid \bm{x}_t = \bm{x}) 
     + \dot{\sigma}_t \,\mathbb{E}(\bm{z} \mid \bm{x}_t = \bm{x})
     \label{eq:sivelocity}
\end{equation}

\begin{definition}[Flow Map \citep{flowmap, tutorial}]
    The flow map $\bm{X}_{s,t} : \mathbb{R}^d \to \mathbb{R}^d$ for Eq.~\ref{eq:pflow} is the unique map such that
    \begin{equation}
        {X}_{s,t}(\bm{x}_s) = \bm{x}_t, \quad \text{for all } (s,t) \in [0,1]^2, 
    \end{equation}
    where $(\bm{x}_t)_{t \in [0,1]}$ is any solution to the ODE Eq.~\ref{eq:pflow}.
\end{definition}

\begin{proposition}[Consistency Property \citep{flowmap}]~\label{app:thmconsist}
    The flow map $\bm{X}_{s,t}(\bm{x})$ satisfies the Consistency Property
    \begin{equation}
        {X}_{s,r}({X}_{t,s}(\bm{x})) = {X}_{t,r}(\bm{x})
        \label{eq:consistency}
    \end{equation}
    for all $(t,s,r,\bm{x}) \in [0,1]^3 \times \mathbb{R}^d$. In particular, 
    ${X}_{s,t}({X}_{t,s}(\bm{x})) = \bm{x}$  
    for all $(s,t,\bm{x}) \in [0,1]^2 \times \mathbb{R}^d$.
\end{proposition}

\subsection{Flow Map Solver}
\label{app:ddim}

\subsubsection{Euler Solver}
With a probability velocity field $\bm v_t(\bm x)$ which can be derived from a pre-defined probability path or approximated by $\bm v_t^{\theta}(\bm x)$ with a neural network, an Euler Solver can predict the Flow Map from $t$ to $r$ with
\begin{equation}
    \bm x_r = X_{t,r}(\bm x_t) = \bm x_t + \int_t^r\bm v_\tau (\bm x) d\tau .
\end{equation}
Besides, if the integral of $\bm v_\tau(\bm x)$ is given as $\bm u_{t,r}=  \frac{1}{r-t} \int_t^r\bm v_\tau d\tau$ or parameterized with $\bm u_{t,r}^{\theta}$, the flow map can be easily obtained with 
\begin{equation}
\bm x_r = X_{t,r}(\bm x_t) = \bm x_t + (r-t)\bm u(t,r).
\end{equation}

\subsubsection{DDIM Solver}

Let the forward process be defined by
\begin{equation}
    \bm{x}_r = \alpha(r)\bm{x}_0 + \sigma(r)\bm{\varepsilon}, 
    \qquad \bm{\varepsilon}\sim\mathcal{N}(\bm{0},\bm{I}),
\end{equation}
so that at any $t$ we have
\begin{equation}
    \bm{x}_t = \alpha(t)\bm{x}_0 + \sigma(t)\bm{\varepsilon}, 
    \qquad 
    \bm{v}_t = \dot{\alpha}(t)\bm{x}_0 + \dot{\sigma}(t)\bm{\varepsilon}.
\end{equation}

\paragraph{Conditional formulation.}
Conditioned on $\bm{x}_t$, the posterior distribution of $(\bm{x}_0,\bm{\varepsilon})$ 
is Gaussian, and hence both $\bm{x}_r$ and $\bm{v}_t$ can be written as linear 
functions of $\bm{x}_0,\bm{\varepsilon}$. 
Taking conditional expectations yields
\begin{equation}
    \mathbb{E}
    \begin{bmatrix} \bm{x}_t \\ \bm{v}_t \end{bmatrix}
    =
    \begin{bmatrix} \alpha_t & \sigma_t \\ \dot{\alpha}_t & \dot{\sigma}_t \end{bmatrix}
    \mathbb{E}\!\begin{bmatrix} \bm{x}_0 \\ \bm{\varepsilon} \end{bmatrix},
\end{equation}
and by inversion, we obtain
\begin{equation}
    \mathbb{E}
    \begin{bmatrix} \bm{x}_0 \\ \bm{\varepsilon} \end{bmatrix}
    =
    \frac{1}{\alpha_t \dot{\sigma}_t - \sigma_t \dot{\alpha}_t}
    \begin{bmatrix} \dot{\sigma}_t & -\sigma_t \\ -\dot{\alpha}_t & \alpha_t \end{bmatrix}
    \mathbb{E}
    \begin{bmatrix} \bm{x}_t \\ \bm{v}_t \end{bmatrix}.
\end{equation}

\paragraph{DDIM update.}
Substituting back into the expression for $\bm{x}_r$ gives
\begin{align}
    \mathbb{E}[\bm{x}_r \mid \bm{x}_t] 
    &= \alpha_r\,\mathbb{E}[\bm{x}_0\mid\bm{x}_t] + \sigma_r\,\mathbb{E}[\bm{\varepsilon}\mid\bm{x}_t] \\
    &= \bar{\alpha}(t,r)\,\bm{x}_t + \bar{\beta}(t,r)\,\bm{v}_t,
\end{align}
where the coefficients are
\begin{equation}
    \bar{\alpha}(t,r) = \frac{\alpha_r \dot{\sigma}_t - \sigma_r \dot{\alpha}_t}{\alpha_t \dot{\sigma}_t - \sigma_t \dot{\alpha}_t}, 
    \qquad
    \bar{\beta}(t,r) = \frac{-\alpha_r \sigma_t + \sigma_r \alpha_t}{\alpha_t \dot{\sigma}_t - \sigma_t \dot{\alpha}_t}.
\end{equation}

\paragraph{Cosine Path.}
In the cosine path, the schedule of $\alpha_t = \alpha(t)$ and $\sigma_t = \sigma(t)$ reads 
\[
\alpha(t) = \cos\left( \frac{\pi}{2} t \right), \quad \sigma(t) = \sin\left( \frac{\pi}{2} t \right)
\]
\[
\dot{\alpha}(t) = -\frac{\pi}{2} \sin\left( \frac{\pi}{2} t \right), \quad
\dot{\sigma}(t) = \frac{\pi}{2} \cos\left( \frac{\pi}{2} t \right)
\]

Then:
\begin{align}
    \bar{\alpha}(t, r) &= \cos\left( \frac{\pi}{2}(r - t) \right), \quad
\bar{\beta}(t, r) = \frac{2}{\pi}\sin\left( \frac{\pi}{2}(r - t) \right)  \label{eq:cosine_ddim}
\end{align}

\paragraph{Linear Schedule.}
In the linear path, the schedule of $\alpha_t = \alpha(t)$ and $\sigma_t = \sigma(t)$ reads 

\[
\alpha(t) = 1 - t, \quad \sigma(t) = t \Rightarrow
\dot{\alpha}(t) = -1, \quad \dot{\sigma}(t) = 1
\]

Then:
\[
\bar{\alpha}(t,r) = \frac{(1 - r)(1) - r(-1)}{(1 - t)(1) - t(-1)} = 1
\]
\[
\bar{\beta}(t,r) = \frac{- (1 - r) t + r (1 - t)}{1} = r - t
\]

Therefore, 
\[
\bar{\alpha}(t,r) =  1, \quad 
\bar{\beta}(t,r) = r - t
\]

\subsection{Derived Flow Path from preconditioner of EDM}
\label{app:flowpath}
The original establishment of EDM is based on the score-based diffusion model, while in this part, we aim to demonstrate that although in EDM, $\alpha_t$ and $\sigma_t$ do not satisfy 
\[
\alpha_0 = 1, \alpha_1 = 0; \sigma_0 = 0, \sigma_1=1,
\]
the preconditioner of EDM is equivalent to the cosine path in our paper, or namely TrigFlow in sCT, by using the change-of-variable. This part is mostly based on Appendix~B of TrigFlow proposed by \cite{scm}, while we use a more unified view from stochastic interpolants~\citep{stochasticinterp} and SiT~\citep{sit}.

\subsubsection{Score-based view of EDM.}
\paragraph{EDM forward diffusion.}
We draw $\bm{x}_0\sim p_{\text{data}}$ and $\bm{\varepsilon}\sim\mathcal{N}(\bm{0},\sigma_{\text{data}}\bm{I})$ and define
\begin{equation}
\label{eq:edm-forward}
\bm{x}_t \;=\; \alpha_t\,\bm{x}_0 \;+\; \sigma_t\,\bm{\varepsilon}, 
\end{equation}
where $\alpha_t>0$ and $\sigma_t>0$ are schedule functions determined by a noise scale $\sigma(t)$:
\begin{equation}
    \alpha_t \;=\; \frac{\sigma_\text{data}}{\sqrt{\sigma_\text{data}^2+\sigma(t)^2}}, 
    \qquad
    \sigma_t \;=\; \frac{\sigma(t)}{\sqrt{\sigma_\text{data}^2+\sigma(t)^2}}.\label{eq:eq30}
\end{equation}
Here $\sigma_\text{data}$ denotes the data standard deviation, and the EDM noise schedule is
\begin{equation}
    \sigma(t) \;=\;
    \Big(\sigma_{\max}^{1/\rho} + t\big(\sigma_{\min}^{1/\rho}-\sigma_{\max}^{1/\rho}\big)\Big)^{\rho},
    \qquad t\in[0,1],
\end{equation}
with typical choices $\sigma_{\min}\approx 2\times 10^{-3}$, 
$\sigma_{\max}\approx 80.0$, and $\rho=7$.  


\subsubsection{From EDM preconditioner to cosine path.}

\paragraph{Score parameterization.}
We may interpret EDM as a score-based model. Specifically, define the score
\begin{equation}
    \bm{s}_t(\bm{x}_t) := \nabla_{\bm{x}_t}\log p_t(\bm{x}_t),
\end{equation}
and approximate it by a neural network
\begin{equation}
    \varphi^\theta(\bm{x}_t,t) \;\approx\; \bm{s}_t(\bm{x}_t).
\end{equation}
Since the Gaussian corruption satisfies $\bm{\varepsilon}=-\sigma(t)\bm{s}_t(\bm{x}_t)$, 
the EDM predictor is written as
\begin{equation}
D^\theta(\bm{x}_t,t)
= c_{\mathrm{skip}}(t)\,\bm{x}_t 
+ c_{\mathrm{out}}(t)\,\varphi^\theta\!\big(c_{\mathrm{in}}(t)\bm{x}_t,\,c_{\mathrm{noise}}(t)\big),
\end{equation}
since $\frac{\sigma_{\text{data}}^2}{\sigma^2(t) + \sigma_{\text{data}}^2} (\bm{x}_0 + \sigma(t)\bm{\varepsilon}) = c_{\text{skip}}(t) \bm{x}_t $, and according to Eq.~\ref{eq:edm-forward} and Eq.~\ref{eq:eq30},
with scaling coefficients ensuring unit-normalized training targets:
\begin{equation}
\label{eq:edm-coeffs}
c_{\mathrm{in}}(t) = \frac{1}{\sigma_\text{data}},\qquad
c_{\mathrm{skip}}(t) = \alpha_t,\qquad
c_{\mathrm{out}}(t) = -\,\sigma_\text{data}\,\sigma_t.
\end{equation}
and the denoiser reduces to
\begin{equation}
{D}^\theta(\bm{x}_t,t)
= \alpha_t\,\hat{\bm{x}}_t
- \beta_t\,\sigma_\text{data}\,\varphi^\theta\!\big(\hat{\bm{x}}_t/\sigma_\text{data},\,c_{\mathrm{noise}}(t)\big).\label{eq:denoise}
\end{equation}

\paragraph{Cosine reparameterization.}
Since $\alpha_t^2+\beta_t^2=1$, we can introduce a cosine time variable $t'\in[0,1]$ such that
\begin{equation}
\alpha_t = \cos\!\Big(\tfrac{\pi}{2}t'\Big),\qquad
\beta_t = \sin\!\Big(\tfrac{\pi}{2}t'\Big),
\qquad
t' = \tfrac{2}{\pi}\arctan\!\Big(\tfrac{\beta_t}{\alpha_t}\Big) = \tfrac{2}{\pi}\arctan\!\Big(\tfrac{\sigma(t)}{\sigma_{\text{data}}}\Big).
\end{equation}
On this cosine path, we may equivalently define
\begin{equation}
\alpha(t') = \cos\!\Big(\tfrac{\pi}{2}t'\Big),\qquad
\sigma(t') = \sin\!\Big(\tfrac{\pi}{2}t'\Big), \label{eq:cosinesche}
\end{equation}
which again satisfies $\alpha(t')^2+\sigma(t')^2=1$.  
Moreover, since in EDM one typically samples $t \sim \log\mathcal{N}(P_{\mathrm{mean}}, P_{\mathrm{std}}^2)$, 
under the change of variables $t' = \tfrac{2}{\pi}\arctan(t)$, the resulting sampling matches the time parameterization used in CT and sCT (Table~\ref{tab:methodconclude}).

\subsubsection{From Score parameterization to Velocity}
In general, we can denote $t' \in[0,1]$ as $t$, with Eq.~\ref{eq:cosinesche}, which leads the denoising predictor of Eq.~\ref{eq:denoise} to 
\begin{equation}
    {D}^\theta(\bm{x}_t,t)
= \cos(\frac{\pi}{2}t)\,{\bm{x}}_t
- \sin(\frac{\pi}{2}t)\,\sigma_\text{data}\,{\varphi}^\theta\!\big(\bm{x}_t/\sigma_\text{data},\,c'_{\text{noise}}(t)\big),
\end{equation}
Since $ {D}^\theta(\bm{x}_t,t) $ aims to approximate $\bm{x}_0$, and if we write
\[\bm{v}^{\theta}(\bm{x}_t, t) = \frac{\pi}{2}\,\sigma_\text{data}\,
{\varphi}^\theta\!\big(\bm{x}_t/\sigma_\text{data},\,c'_{\text{noise}}(t)\big).\],
it reads
\[
{D}^\theta(\bm{x}_t,t)
= \cos(\frac{\pi}{2}t){\bm{x}}_t + \frac{2}{\pi}\sin(-\frac{\pi}{2}t) \bm{v}^{\theta}(\bm{x}_t, t) 
\]
the coefficient $\cos(\frac{\pi}{2}t)$ and $ \frac{2}{\pi}\sin(-\frac{\pi}{2}t) $ coincides to $\bar\alpha(t,0)$ and $\bar\beta(t,0)$ in Eq.~\ref{eq:cosine_ddim}, the parameterization of $\bm{v}_t^\theta(\bm{x}_t) = \frac{\pi}{2}\sigma_{\text{data}}F^{\theta}(\bm{x}_t,t)$ is equivalent to denoise the path of preconditioner in EDM schedule. The same evidence is also provided with Eq.~4 in \cite{scm}.


\section{Derivation of Flow Map Construction and Loss}
\label{app:flowmapconstruct}
\subsection{Consistency Training}
\label{app:ct}
In CTs and CMs~\citep{cm}, the original paper uses the EDM preconditioner as the components of the basic diffusion model. By using a neural network $\varphi^\theta$ to approximate the score function $\bm{s}_t(\bm{x})$, its target is to map any noised samples in $t$ to $0$ which in the flow map notation,  reads \[\hat X^\theta_{t,0}(\bm{x}_t)=f^\theta(\bm{x}_t) = c_\text{skip}(t) \bm{x}_t + c_\text{out} \sigma_\text{data}\varphi^{\theta}(\bm{x}_t),\] 
such that 
\begin{align}
    l_\text{cm} = d(f^\theta(\bm{x}_t) , \mathrm{sg}(f^\theta(\hat {\bm{x}}_s))), \label{eq:ddim_ori}
\end{align}
where  
\begin{equation}
\begin{aligned}
    t &= [\sigma_{\text{max} }^{1/\rho}+  \frac{\tau}{K}(\sigma_{\text{min} }^{1/\rho} - \sigma_{\text{max} }^{1/\rho})]^{\rho}\\
    s &= [\sigma_{\text{max} }^{1/\rho}+\frac{\tau+1}{K}(\sigma_{\text{min} }^{1/\rho} - \sigma_{\text{max} }^{1/\rho})]^{\rho} \\
    \tau &\sim \mathcal{U}[0,1,\ldots,K], ~\label{eq:cttimesampler}
\end{aligned} 
\end{equation}

and $\hat {\bm{x}}_s$ is on the same conditional flow path that generates $\bm{x}_t$.
By adopting the equivalence of EDM preconditioner to Cosine path, as shown in Appendix~\ref{app:flowpath}, we can write $F^\theta$ as the approximator of $\bm{v}_t$ under the change-of-variable, which reads
\begin{equation}
\begin{aligned}
    \bm{x}_0 &\sim p_0, \quad \bm{\varepsilon}\sim \mathcal{N}(0,1) \\
    \bm{x}_t &= \cos(\frac{\pi}{2}t)\bm{x}_0 + \sin(\frac{\pi}{2}t)\bm \varepsilon \\
    \bm{v}_{t|0} &= -\frac{\pi}{2}\sin(\frac{\pi}{2}t) \bm{x}_0 + \frac{\pi}{2}\cos(\frac{\pi}{2}t)\bm \varepsilon \\
    \bm{\hat{x}}_s &= \cos(\frac{\pi}{2}s)\bm{x}_0 + \sin(\frac{\pi}{2}s)\bm \varepsilon,
\end{aligned}
\end{equation}
Then, following the derivation of Eq.~\ref{eq:cosine_ddim}, 
by substituting the coefficients $\bar{\alpha}_{t,s}$ and $\bar{\beta}_{t,s}$, 
we can equivalently express
\begin{equation}
\begin{aligned}
\hat X_{t,s}(\bm{x}_t) = \bm{\hat{x}}_s &= \mathrm{DDIM}(\bm{x}_t, \bm{v}_{t|0}, t, s) \\
\hat X_{s,r}(\bm{\hat{x}}_s) =\bm{\hat{x}}_r &= \mathrm{DDIM}(\hat{\bm{x}}_s, F^\theta(\hat{\bm{x}}_s), s, r)\\
X^\theta_{t,r}(\bm{x}_t) = \bm{x}^\theta_r &= \mathrm{DDIM}(\bm{x}_t, F^\theta(\bm{x}_t), t, r) 
\end{aligned}
\end{equation}

Finally, by replacing $d$ in Eq.~\ref{eq:ddim_ori}, it reads
\begin{equation}
    \begin{aligned}
    l_\text{ct}(\bm{x}_t,r,s,t;\theta) &= \mathrm{LPIPS}(f^\theta(\bm{x}_t) , \mathrm{sg}(f^\theta(\hat {\bm{x}}_s))),  \\
    &= \mathrm{LPIPS}(X^\theta_{t,r}(\bm{x}_t) , \mathrm{sg}(\hat X_{s,r}(\hat{\bm{x}}_s))) \\
    &= \mathrm{LPIPS}\Big(\mathrm{DDIM}(\bm{x}_t, \bm{v}_t^{\theta}(\bm{x}_t), t, r), \;\mathrm{sg}\big(\mathrm{DDIM}(\hat{\bm{x}}_s, \bm{v}_s^{\theta}(\hat{\bm{x}}_s), s, r)\big)\Big),~\label{eq:ctloss_final}
\end{aligned}
\end{equation}

where $\hat{\bm{x}}_s = \mathrm{DDIM}(\bm{x}_t, \bm{v}_{t|0}, t,s)$, which coincides the description of CTs in Sec.~\ref{sec:examples}. Further, under the change-of-variable, the time sampler Eq.~\ref{eq:cttimesampler} will be of the form as described by Table~\ref{tab:methodconclude}.
\subsection{Shortcut Diffusion}
\label{app:scd}
In the original paper of SCD~\citep{scd}, it parameterizes the velocity with the neural network as 
\[
F^\theta(\bm{x}_t, t,r) =\bm{v}^\theta(\bm{x}_t,t,r),
\]
while we claim that the $\bm{v}^\theta(\bm{x}_t,t,r)$ is not the instantaneous one, because it requires the entries of both $t$ as the start point, and $r$ as the end point. Instead, we regard it as the average velocity, leading to our parameterization of
\[
F^\theta(\bm{x}_t, t,r) =\bm{u}_{t,r}^\theta(\bm{x}_t).
\]
In this way, 
$x_t$ is first sampled from a linear path, as 
\begin{equation}
\begin{aligned}
    \bm{x}_0 &\sim p_0, \quad \bm{\varepsilon}\sim \mathcal{N}(0,1) \\
    \bm{x}_t &= (1-t)\bm{x}_0 + t\bm \varepsilon \\
    \bm{v}_{t|0} &= - \bm{x}_0 + \bm \varepsilon ,
\end{aligned}
\end{equation} 
Then, the flow map is constructed via
\begin{equation}
\begin{aligned}
\hat X_{t,s}(\bm{x}_t) = \bm{\hat{x}}_s &= \bm{x}_t -h\bm{u}^{\theta}_{t,s}(\bm{x}_t) \\
\hat X_{s,r}(\bm{x}_s) = \bm{\hat{x}}_r &= \bm{x}_s -h\bm{u}^{\theta}_{s,r}(\bm{\hat x}_s) \\
X^\theta_{t,r}(\bm{x}_t) =  \bm{\hat{x}}^\theta_t &= \bm{x}_t -2h\bm{u}^{\theta}_{t,r}(\bm{x}_t)
\end{aligned}
\end{equation}
Finally,
according to the consistency property of flow map shown in Prop.~\ref{app:thmconsist}, and by setting $w = h^2$ the loss term for the regularization of velocity can be rewritten as 
\begin{equation}
\begin{aligned}
        l_{\mathrm{scd}}(\bm{x}_t, r,s,t; \theta) &=\frac{1}{4h^2}\cdot\left\| \bm{x}_t - {2h}\bm{u}^{\theta}_{t,r}(\bm{x}_t) - \mathrm{sg}\left(\bm{x}_t - {h}\bm{u}^{\theta}_{t,s}(\bm{x}_t)  - {h}\bm{u}^{\theta}_{s,r}(\bm{x}_t - {h}\bm{u}^{\theta}_{t,s}(\bm{x}_t) \right)  \right\|^2_2 \\
        &=\frac{1}{4h^2}\cdot\left\| \bm{x}_t - {2h}\bm{u}^{\theta}_{t,r}(\bm{x}_t) - \mathrm{sg}\left(\bm{x}_t - {h}\bm{u}^{\theta}_{t,s}(\bm{x}_t)  - {h}\bm{u}^{\theta}_{s,r}(\hat{\bm{x}}_s) \right)\right\|^2_2 \\
        &= \left\| 
\bm{u}^{\theta}_{t,r}(\bm{x}_t)
- \frac{1}{2}\mathrm{sg}\!\left( 
\bm{u}^{\theta}_{t,s}(\bm{x}_t) 
+ \bm{u}^{\theta}_{s,r}(\bm{\hat x}_s) 
\right)
\right\|_2 ^2,
\end{aligned}
\label{eq:appscdloss}
\end{equation}
where $r,s,t$ are equi-spaced, such that $t-s = s-r =h$, which coincides Eq.~\ref{eq:scdloss} in Sec.~\ref{sec:examples}. Specifically, in the time sampler, it defines the total step $K$ with 
$T=\lfloor \log_2 K \rfloor$. For each sample, it draws 
$2h \in \{2^{-0},2^{-1},\dots,2^{-(T-1)}\}$ and 
$e\sim\mathcal{U}(0,1)$, then computes
\begin{equation}
    t = \tfrac{1}{K}\Big\lfloor 2h K + e\cdot(K-2h K+1)\Big\rfloor, 
    \qquad 
    r = t - 2h, 
    \qquad 
    s = t - h,
\end{equation}
as the time  sampler for $\{r,s,t\}$. We denote the time sampler as $(t,h) \sim \mathrm{Uniform}\log_{2}(t,h)$, and $s = t - h$, $r = t - 2h$.
\subsection{Inductive Moment Matching}
\label{app:imm}


Given a mini-batch of size $B$, IMM first draws $\{(\bm{x}_0^{(i)}, \bm{\varepsilon}^{(i)})\}_{i=1}^B$, 
and partition them into groups of size $M$. Within each group, a triplet $(r,s,t)$ 
is sampled, where $(r,t)$ are drawn uniformly from $[0,1]$ with $s$ separated by a fixed difference, as 
\begin{equation}
    \begin{aligned}
        t &\sim\mathcal{U}[0,1] \\
        n_s &= \frac{1}{1-t} - \frac{1}{2^\gamma} \\
        s &=\frac{n_s}{n_s + 1} \\
        r &\sim \mathcal{U}[0,t],
    \end{aligned}
\end{equation}
where $\gamma$ is usually set as 12 according to its code implementation.
Each group thus defines $M$ correlated particle samples that share the same flow interpolation times.
For each group of $M$ particles, IMM constructs an $M \times M$ kernel matrix based on a 
positive definite kernel function as metrics $d(\cdot, \cdot)$ (\emph{e.g.}, RBF). The objective is
\begin{equation}
\mathcal{L}_{\mathrm{imm}}(\theta) = 
\mathbb{E}_{\bm{x}_t,\bm{x}'_t, \bm{x}_r,\bm{x}'_r, r,s,t}
\Big[
w(r,t)\,\big(
\mathrm{ker}(\bm{z}_{t,r}, \bm{z}'_{t,r})
+ \mathrm{ker}(\bm{z}_{s,r}, \bm{z}'_{s,r})
- \mathrm{ker}(\bm{z}_{t,r}, \bm{z}'_{s,r})
- \mathrm{ker}(\bm{z}'_{t,r}, \bm{z}_{s,r})
\big)
\Big],\label{eq:immmmd}
\end{equation}
where 
\begin{align*}
    \bm{z}_{t,r} &= \mathrm{DDIM}(\bm{x}_t,\bm{v}^\theta_t(\bm x_t), t, r), \\
    \bm{z}'_{t,r} &= \mathrm{sg}(\mathrm{DDIM}(\bm{x}_t,\bm{v}^\theta_t(\bm x_t), t, r)),
\end{align*}
    

and $w(r,t)$ is a prior weighting. Intuitively, samples are repelled by intra-group pairs 
(\emph{e.g.}\ $\bm{z}_{t,r}$ vs.\ $\bm{z}'_{t,r}$) while attracted towards inter-group matches 
($\bm{z}_{t,r}$ vs.\ $\bm{z}'_{s,r}$). This ensures both intra-sample diversity and inter-sample alignment.

In practice, a batch of size $B$ is divided into $B/M$ groups, 
and the IMM loss is computed as an average over these groups.
For kernels, RBF and negative pseudo-Huber kernels are common choices for $\mathrm{ker}(\cdot,\cdot)$, 
which guarantee moment matching up to all orders.

    
Further, we bridge the IMM loss with the common flow map learning objective in the following. In Eq.~\ref{eq:immmmd}, it gives the group kernel function, according to Appendix. C.3 in \cite{imm}, we can write the loss here as 
\begin{equation}
    \begin{aligned}
        &\mathcal{L}_{\mathrm{imm}}(\theta) \\
        =& 
\mathbb{E}_{\bm{x}_t,\bm{x}'_t, \bm{x}_r,\bm{x}'_r, r,s,t}
\Big[
w(r,t)\,\big(
\mathrm{ker}(\bm{z}_{t,r}, \bm{z}'_{t,r})
+ \mathrm{ker}(\bm{z}_{s,r}, \bm{z}'_{s,r})
- \mathrm{ker}(\bm{z}_{t,r}, \bm{z}'_{s,r})
- \mathrm{ker}(\bm{z}'_{t,r}, \bm{z}_{s,r})
\big)
\Big],\ \\
=& \mathbb{E}_{r,s,t}
\Big[w(r,t) (\mathbb{E}_{\bm{x}_t,\bm{x}'_t, \bm{x}_r,\bm{x}'_r}    \Big[
        \langle
        \mathrm{ker}(\bm{z}_{t,r}, \cdot),
        \mathrm{ker}(\bm{z}'_{t,r}, \cdot)
        \rangle
        +
        \langle
        \mathrm{ker}(\bm{z}_{s,r}, \cdot),
        \mathrm{ker}(\bm{z}'_{s,r}, \cdot)
        \rangle
        \\ & \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad- 
        \langle
        \mathrm{ker}(\bm{z}_{t,r}, \cdot),
        \mathrm{ker}(\bm{z}'_{s,r}, \cdot)
        \rangle
        -
        \langle
        \mathrm{ker}(\bm{z}'_{t,r}, \cdot),
        \mathrm{ker}(\bm{z}_{s,r}, \cdot)
        \rangle
    \Big])
\Big] \\
=& \mathbb{E}_{r,s,t}
\Bigg[
    w(r,t)
    \Big\langle
    \mathbb{E}_{\bm{x}_t}
    \big[\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s)), \cdot)\big],\\
    & \quad \quad\quad\quad\quad\quad
    \mathbb{E}_{\bm{x}_t'} 
    \big[\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t'), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s')), \cdot)\big]
    \Big\rangle 
\Bigg] \\
    =&  \mathbb{E}_{r,s,t}
\Bigg[
    w(r,t)     \Big\|
    \mathbb{E}_{\bm{x}_t}
    \big[\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s)), \cdot)\big]
    \Big\|_{\mathcal{H}}^2
\Bigg]
    \end{aligned}
\end{equation}
where $\hat{\bm{x}}_s = \mathrm{DDIM}({\bm{x}}_t, \bm{v}_{t|0},t,s) $ is estimated with conditional velocity, and $\Big\|
    \mathbb{E}_{\bm{x}}
    \big[\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s)), \cdot)\big]
    \Big\|_{\mathcal{H}}^2$ is the Maximum Mean Discrepancy commonly defined
on Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$ with a positive definite kernel in IMM.
Then, according to Jensen’s inequality, 
\begin{equation}
    \begin{aligned}
       & \mathbb{E}_{r,s,t}
\Bigg[
    w(r,t)     \Big\|
    \mathbb{E}_{\bm{x}_t}
    \big[\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s)), \cdot)\big]
    \Big\|_{\mathcal{H}}^2
\Bigg] \\
\leq & \mathbb{E}_{r,s,t, \bm{x}_t} \Bigg[w(r,t) \big\|\mathrm{ker}(X_{t,r}^{\theta}(\bm{x}_t), \cdot)
    - 
    \mathrm{ker}(\mathrm{sg}(X_{s,r}^{\theta}(\hat{\bm{x}}_s)), \cdot)\big\|_\mathcal{H}^2 \Bigg]
    \end{aligned}
\end{equation}
In this way, we define $d(\bm{x},\bm{y})$ as RKHS discrepancy $\big\|\mathrm{ker}(\bm{x}, \cdot)
    - 
    \mathrm{ker}(\bm{y}, \cdot)\big\|_\mathcal{H}^2 $, it reads
    \begin{equation}
        \mathcal{L}_{\mathrm{imm}}(\theta) \leq \mathbb{E}_{r,s,t\sim p(\tau), \,\bm{x}_t\sim p_t}
\big[
    w(r,t) \cdot d(    X^\theta_{t,r}(\bm{x}_t),\mathrm{sg}(\hat X_{s,r}\circ \hat X_{t,s}(\bm{x}_t))
)
\big]
    \end{equation}
    Therefore, minimizing Eq.~\ref{eq:flowmapconsist} is equivalent to upper-bounding the IMM loss. 

\subsection{MeanFlow}
\label{app:MeanFlow}

As MeanFlow takes the Linear flow path, we can easily obtain
\begin{equation}
    \mathrm{DDIM}(\bm{x}_t,\bm{v}_t,t,s) = \bm{x}_t + (s-t)\bm{v}_t,
\end{equation}
which means the DDIM solver and Euler solver are the same. With the flow map construction, we can write the corresponding terms into $\| \cdot \|^2$ of $d(\cdot, \cdot )$ as follows:
\begin{equation}
    \begin{aligned}
            & \left( \bm{x}_t + (r-t)\bm{u}_{t,r}^\theta(\bm{x}_t) \right) -\left( \bm{x}_t + (s-t) \bm{v}_t  + (r-s)\bm{u}_{s,r}^\theta \left( \bm{x}_s \right) \right)  \\
    =&(r-t)\bm{u}_{t,r}^\theta(\bm{x}_t) - (s-t) \bm{v}_t - (r-s)\bm{u}_{s,r}^\theta \left( \bm{x}_s \right) .
    \end{aligned}
\end{equation}By substituting $s=t-dt$ and normalized by $dt$, we get
\begin{equation}
\begin{aligned}
    & ((r-t)\bm{u}_{t,r}^\theta(\bm{x}_t) + dt \cdot \bm{v}_t - (r-t+dt)\bm{u}^\theta_{t-dt,r} \left( \bm{x}_{t-dt} \right)  ) / dt\\
    =& dt \cdot \left( \bm{v}_t + \frac{d\left[(r-t)\bm{u}^\theta_{t,r}(\bm{x}_t)\right]}{dt} \right) /dt\\
    =&  \bm{v}_t - \bm{u}_{t,r}^\theta(\bm{x}_t) - (t-r)\frac{d}{dt} \bm{u}^\theta_{t,r}(\bm{x}_t).
\end{aligned}
\end{equation}
However, the marginal velocity $\bm{v}_t$ is inaccessible in training, so it can be replaced by the conditional velocity $\bm{v}_{t|0}$.
From Eq. 6 in \citet{meanflow}, by adding the adaptive loss term $w$, this loss coincides with the training objective of MeanFlow in Eq.~\ref{eq:MeanFlowloss}, as 
\begin{equation}
    l(\bm{x}_t, r, t-dt, t;\theta) = w\cdot \left\|\bm{u}^\theta_{t,r}(\bm{x}_t)
-\mathrm{sg}\left(\bm{v}_{t|0} + (r-t) \frac{d\bm{u}^{\theta}_{t,r}(\bm{x}_t)}{dt}\right) \right\|^2
\end{equation}
 Further, MeanFlow adopts an adaptively weighted squared L2 loss. 
Given the regression error $\Delta = u_\theta - u_{\text{tgt}}$, where $u_{\text{tgt}} = \mathrm{sg}\left(\bm{v}_{t|0} + (r-t) \frac{d\bm{u}^{\theta}_{t,r}(\bm{x}_t)}{dt}\right)$, the squared L2 loss is 
$\|\Delta\|_2^2$.  
To stabilize training, MeanFlow reweights $\|\Delta\|_2^2$ with 
\begin{equation}
    w = \frac{1}{(\|\Delta\|_2^2 + c)^p},
\end{equation}
where $c>0$ avoids division by zero and $p$ controls the weighting ($p=0.5$ recovers a Pseudo-Huber style loss).  
The final loss is defined as $\mathrm{sg}(w)\cdot \mathcal{L}$, where $\mathrm{sg}(\cdot)$ denotes the stop-gradient operator.


\subsection{s-Consistency Training}
\label{app:sct}
We here simplify the derivation with $\sigma_\text{data} = 1$. 
According to the Eq.~\ref{eq:ctloss_final}, we can write the corresponding terms with squared $l_2$-distance with $s = t - \Delta t$ and $l(\bm{x}_t, r,s,t;\theta)$ normalized by $\Delta t$, as the following:
\begin{equation}
\begin{aligned}
    &w(t)\lim_{\substack{s=t-\Delta t \\ \Delta t \to 0}}\frac{1}{{\Delta t}}\|\mathrm{DDIM}(\bm{x}_t,\bm{v}^\theta_t,t,r) - \mathrm{sg} (\mathrm{DDIM}\left(\mathrm{DDIM} (\bm{x}_t,\bm{v}_t,t,s),\bm{v}^\theta_s,s,r\right)) \|^2\\
    =& w(t)\lim_{\substack{s=t-\Delta t \\ \Delta t \to 0}}\frac{1}{{\Delta t}}\| \mathrm{DDIM}(\bm{ x}_t,\bm{v}^\theta_t,t,r) - \mathrm{sg}( \mathrm{DDIM}(\hat{\bm{ x}}_s,\bm{v}^\theta_s,s,r))\|^2  \\
     \\
    =&w(t)\lim_{\substack{s=t-\Delta t \\ \Delta t \to 0}}\left(\mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right) - \mathrm{sg}(\mathrm{DDIM}(\hat{\bm{x}}_s,\bm{v}^{\theta}_s,s,r))\right)^\mathsf{T} \cdot \\&\quad \quad \quad \quad \quad \quad\frac{\mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right) - \mathrm{sg} (\mathrm{DDIM}(\hat{\bm{x}}_s,\bm{v}^\theta_s,s,r))}{\Delta t} \\
    \approx &w(t) (\mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right) - \mathrm{sg}(\mathrm{DDIM}(\hat{\bm{x}}_s,\bm{v}^\theta_s,s,r)))^\mathsf{T} \mathrm{sg}\left(\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt}\right) \\ \label{eq:sct_ddim}
\end{aligned}
\end{equation}
By fixing $r=0$, from Eq.~\ref{eq:cosine_ddim},  it can be obtain that the  gradient of Eq.~\ref{eq:sct_ddim} w.r.t. $\theta$ is   
\begin{equation}
    \begin{aligned}
        & w(t)\nabla_\theta \left[(\mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right) - \mathrm{sg}(\mathrm{DDIM}(\hat{\bm{x}}_s,\bm{v}^\theta_s,s,r)))^\mathsf{T} \mathrm{sg}\left(\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt}\right)\right]\\
        =&w(t) \nabla_\theta \left[\mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)^\mathsf{T}\mathrm{sg}\left(\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt}\right )\right] \\
        =& w(t)\nabla_\theta  \left[\left(\cos(\frac{\pi}{2}t) \bm{x}_t - \sin(\frac{\pi}{2}t)\bm{v}^\theta_t\right)^\mathsf{T} \mathrm{sg}\left(\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt}\right)\right] \\
        =&\nabla_\theta  \left(\bm{v}^\theta_t \right)^\mathsf{T}\mathrm{sg}\left(  -\sin(\frac{\pi}{2}t) w(t)\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt} \right) \\
        =& \nabla_\theta \|\bm{v}^\theta_t -\mathrm{sg}\left(\bm{v}^\theta_t  + w'(t)\frac{d \mathrm{DDIM}\left(\bm{x}_t,\bm{v}^\theta_t,t,r\right)}{dt}\right)\|^2
    \end{aligned}
\end{equation}
where $w(t) = \frac{1}{\tan(\frac{\pi}{2}t)}$, and $w'(t) = -\sin(\frac{\pi}{2}t) w(t) = -\cos{(\frac{\pi}{2}t)}$, we prove that this flow map construction corresponds to the original loss with the specific time sampler, and the derived loss is in the same form as Eq.~\ref{eq:sctloss} where we rewrite $w' $ by $w$.


\section{Proof of Theorems and Propositions}
\subsection{Proof of Equivariance of MeanFlow and sCT-linear (Remark.~\ref{thm:remark1})}
\label{app:remark1}
\emph{Sketch of proof.} First note that under linear paths, $\hat X_{t,0}^{\theta}(\bm{x}_t) = \mathrm{DDIM}(\bm{x}_t, \bm{v}^{\theta}_t(\bm{x}_t), t, 0) = \bm{x}_t - t\bm{v}_t^\theta(\bm{x}_t)$. As for the training objective, with $w(t) = 1$ and linear path, Eq.~\ref{eq:sctloss} can be easily written as $l(\bm{x}_t,r,t-dt,t;\theta)\Big|_{r=0} = \| \bm{v}^\theta_{t}(\bm{x}_t) - \mathrm{sg}( \bm{v}_{t} + (r-t)\frac{d}{dt} \bm{v}^\theta_{t}(\bm{x}_t))\|_2^2\Big|_{r=0}$. Since in sCT, $r$ is fixed to $0$, parameterization of the neural network can be invariant to $r$, leading to $\bm{v}^\theta_{t}(\bm{x}_t) = F^{\theta}(\bm{x}_t, t, 0)$. Thus, in sampling, by replacing $\bm{u}_{t,0}$ and $\bm{v}_t$  with $F^{\theta}$ in Eq.~\ref{eq:meanvelo} and~\ref{eq:ddimv}, respectively, the sampling processes are the same when following linear paths.

\subsection{Proof of Error Bound (Theorem~\ref{thm:errorbound})}
\label{app:thmbound}

In this section, we aim to prove the error bound of DTSC\&CTSC. Specifically, the theorem is stated as follows. 
\begin{theoremgray}
    [Error bound of DTSC\&CTSC] \label{thm:apperrorbound} Assume the marginal velocity of the flow path satisfies the one-sided Lipschitz condition, where
    $$
    \exists\, C_t \in L^1[0,1] \;:\;
\bigl(\bm{v}_t(\bm x)-\bm{v}_t(\bm y)\bigr)\cdot(\bm x-\bm y) \geq -C_t \| \bm x-\bm y \|^2,
\quad \text{for all } (t,\bm x,\bm y)\in[0,1]\times\mathbb{R}^d\times\mathbb{R}^d.
    $$
  Assume $X_{t,s}^\theta$ are twice continuously differentiable with bounded second derivatives, the weighting function $w(r,s,t)$ is non-negative and bounded. For DTSC, also assume $p(r=0)>0$, 1st-step satisfies $\hat X_{t,t}(\bm{x}_t) = \bm{x}_t$, and $\exists t_1 \leq \cdots \leq t_N\ \ \text{s.t}\ \ p(0,t_n,t_{n+1})>0, w(0,t_n,t_{n+1}) > 0$.
  
  Under $d(\bm{x}, \bm{y}) = \Vert \bm{x} - \bm{y}\Vert_2^2$, given $\bm{x}_1 \sim p_1$, let $p_0$ the density of $\bm{x}_0$, and ${p}^{\theta}_0$ the density of $\bm{x}^\theta_0 = X^\theta_{1,0}(\bm{x}_1)$ that is estimated by neural network with parameter $\theta$,  then
  \begin{align*}
   &W_2^2(p_0, p^\theta_0) \leq C^1_1 \mathcal{L}_\text{dtsc}(\theta) + C^1_2(t-s), \\
   &W_2^2(p_0, p^\theta_0) \leq C_1^2 \mathcal{L}_\text{ctsc}(\theta), 
  \end{align*}
  where we write the training objective in Eq.~\ref{eq:flowmapconsist} as $\mathcal{L}_\bullet(\theta) = \mathbb{E}_{r,s,t\sim p(\tau), \,\bm{x}_t\sim p_t}[
    l_\bullet(\bm{x}_t,r,s,t;\theta)], 
$ with $\bullet \in \{\text{ctsc}, \text{dtsc}\}$, and $W_2(\cdot,\cdot)$ is the Wasserstein-2 distance.
\end{theoremgray}
We note that MeanFlow loss and sCT loss are all $\mathcal{L}_\text{ctsc}(\theta)$, and CT loss and SCD loss are all $\mathcal{L}_\text{dtsc}(\theta)$. IMM's loss is calculated across different conditional paths, as finally bounded by the $\mathcal{L}_\text{dtsc}(\theta)$ as shown in Appendix~\ref{app:imm}. The mentioned previous methods all satisfy the assumptions about $w(r,s,t)$, $p(r,s,t)$, and $\hat X_{t,t}(\bm{x}_t)=\bm{x}_t$. As for the assumption of $d(\bm{x}, \bm{y}) = \Vert \bm{x} - \bm{y}\Vert_2^2$, it holds for all the mentioned methods except CT, which takes LPIPS as the metric function. The convergence of $\mathcal{L}_\text{ct}$ has already been proved by \cite{cm}.

We prove the theorem in three steps: (i) establish the error bound for DTSC; (ii) derive the start point differential CTSC bound; and (iii) further derive the end point differential CTSC bound.

\subsubsection{Error Bound of  DTSC}

\begin{lemma}
Assume $d$ and $X_{t,s}^\theta$ are both twice continuously differentiable with bounded second derivatives, the weighting function $w(r,s,t)$ is non-negative and bounded, and 1st-step satisfies $\hat X_{t,t}(\bm{x}_t) = \bm{x}_t$. We define a loss $\mathcal{L}_1$ as follows:
\begin{align}
    \mathcal{L}_1(\theta) \coloneqq \mathbb{E}\left[ w(r,s,t)\, d\big(\, X^{\theta}_{t,r}(\bm{x}_t), \mathrm{sg}(X^{\theta}_{s,r}({\bm{x}}_s)) \big) \right]. \label{eq:l1}
\end{align}
Then, 
\begin{align*}
    \mathcal{L}_\text{dtsc}(\theta) = \mathcal{L}_1(\theta) + \mathcal{O}(t-s),
\end{align*}
where $\mathcal{L}_\text{dtsc}$ is the discrete-time shortcut models' loss.
\end{lemma}

\begin{proof}
As 
\begin{align*}
    \bm{x}_t = \bm{x}_s + (t-s)\bm{v}_s + \mathcal{O}(t-s),
\end{align*}
and here we define the $\theta^-$ as the parameters in the model which stop-grad operates, for notational simplicity.
By using Taylor expansion, we can get that
\begin{align*}
\mathcal{L}_1(\theta) 
&= \mathbb{E}\left[ w(r,s,t)\, d\big( X^{\theta}_{t,r}(\bm{x}_t), X^{\theta^-}_{s,r}(\bm{x}_s) \big) \right] \\
&= \mathbb{E}\left[ w(r,s,t)\, d\Big(  X^{\theta}_{s,r}(\bm{x}_s) 
+ \partial_s X^{\theta}_{s,r}(\bm{x}_s)(t-s) 
+ \partial_x X^{\theta}_{s,r}(\bm{x}_s)(t-s)\bm{v}_s 
+ {o}(t-s) \right. \\
&\quad \quad\quad\quad\quad\quad\quad\left.,X^{\theta^-}_{s,r}(\bm{x}_s) \Big) \right] \\
&= \mathbb{E}\left[ w(r,s,t)\, d\Big(\, X^{\theta}_{s,r}(\bm{x}_s) 
+ \partial_s X^{\theta}_{s,r}(\bm{x}_s)(t-s) 
+ \mathcal{O}(t-s),
X^{\theta^-}_{s,r}(\bm{x}_s) \Big) \right] \\
&= \mathbb{E}\left[ w(r,s,t)\, \Big( d\big( X^{\theta}_{s,r}(\bm{x}_s),\, X^{\theta^-}_{s,r}(\bm{x}_s) \big) + \right.  \left. \partial_1 d\big( X^{\theta}_{s,r}(\bm{x}_s),\, X^{\theta^-}_{s,r}(\bm{x}_s) \big) \partial_s X^{\theta^-}_{s,r}(\bm{x}_s)(t-s) \Big) \right] \\
&\quad + \mathcal{O}(t-s).
\end{align*}
As for $\hat X_{t,s}(\bm{x}_t)$, there are three ways to calculate it as stated in Sec. \ref{sec:flowmap}. Ways in Eq. \ref{eq:velocityint} and Eq. \ref{eq:ddimv} are numerical solvers, so we have $\hat X_{t,s}(\bm{x}_s) = \bm{x}_s + \mathcal{O}(t-s)$. Eq. \ref{eq:meanvelo} also satisfies this equation since we have the assumption that $\hat X_{t,t}(\bm{x}_t) = \bm{x}_t$ and $X_{t,s}$ is twice continuously differentiable with bounded second derivative.

With a similar derivation, we also have 
\begin{align*}
\mathcal{L}_\text{dtsc} (\theta) 
&= \mathbb{E}\left[ w(r,s,t)\, d\big( X^{\theta^-}_{s,r}(\hat X_{t,s}(\bm{x}_t)),\, X^{\theta}_{t,r}(\bm{x}_t) \big) \right] \\
&= \mathbb{E}\left[ w(r,s,t)\, d\Big( 
X^{\theta^-}_{s,r}(\bm{x}_s),
X^{\theta}_{s,r}(\bm{x}_s) 
+ \partial_s X^{\theta}_{s,r}(\bm{x}_s)(t-s) 
+ \mathcal{O}(t-s)\Big) \right] \\
&= \mathbb{E}\left[ w(r,s,t)\, \Big( d\big( X^{\theta^-}_{s,r}(\bm{x}_s),\, X^{\theta}_{s,r}(\bm{x}_s) \big) 
+ \right. \left. \partial_1 d\big( X^{\theta^-}_{s,r}(\bm{x}_s),\, X^{\theta}_{s,r}(\bm{x}_s) \big)\partial_s X^{\theta^-}_{s,r}(\bm{x}_s)(t-s) \Big) \right] \\
&\quad + \mathcal{O}(t-s).
\end{align*}
By subtracting the two equations, we obtain
\begin{align*}
    \mathcal{L}_\text{dtsc} (\theta) = \mathcal{L}_1 (\theta) + \mathcal{O}(t-s).
\end{align*}
\end{proof}

\begin{theorem} \label{thm:dtscbound} 
Assume $X_{t,s}^\theta$ is twice continuously differentiable with bounded second derivatives, the weighting function $w(r,s,t)$ is non-negative and bounded. Also, assume $p(r=0)>0$, 1st-step satisfies $\hat X_{t,t}(\bm{x}_t) = \bm{x}_t$, and $\exists t_1 \leq \cdots \leq t_N\ \ \text{s.t}\ \ p(0,t_n,t_{n+1})>0, w(0,t_n,t_{n+1}) > 0$.
Under $d(\bm{x}, \bm{y}) = \Vert \bm{x} - \bm{y}\Vert_2^2$,
\begin{equation}
  W_2^2(p_1, p^\theta_1) \leq C_1 \mathcal{L}_\text{dtsc}(\theta) + C_2 (t-s),
\end{equation}
where $C_1=\sum_{n=1}^{N-1} \frac{1}{p(0,t_n,t_{n+1})w(0,t_n,t_{n+1})}$, and $W_2(\cdot,\cdot)$ is the Wasserstein-2 distance.
\end{theorem}

\begin{proof}
Since we have proved that the difference between $\mathcal{L}_\text{dtsc}$ and $\mathcal{L}_1$ is $\mathcal{O}(s-t)$, we only need to prove
\begin{align*}
    W_2^2(p_1, p^\theta_1) \leq C \mathcal{L}_1(\theta).
\end{align*}
Because for $r=0$, $s_0,t_0\ \ \text{s.t}\ \ p(0,s_0,t_0)>0, w(0,s_0,t_0)>0$,
\begin{align*}
\mathcal{L}_1(\theta)
&= \mathbb{E}\big[ w(r,s,t) d\big( X^{\theta^-}_{s,r}(\bm{x}_s), X^{\theta}_{t,r}(\bm{x}_t) \big) \big] \\
&\geq p(0,s_0,t_0) w(0,s_0,t_0) d\big( X^{\theta^-}_{s_0,0}(\bm{x}_{s_0}), X^{\theta}_{t_0,0}(\bm{x}_{t_0}) \big) , 
\end{align*}
we have 
\begin{align}
\mathbb{E} \left[ d\big( X^{\theta^-}_{s_0,0}(\bm{x}_{s_0}), X^{\theta}_{t_0,0}(\bm{x}_{t_0}) \big) \right] \leq \frac{1}{p(0,s_0,t_0) w(0,s_0,t_0)} \mathcal{L}_1(\theta). \label{eq:s0t0}
\end{align}
We define
\begin{align*}
e_{t,0} \coloneqq  X_{t,0}(\bm{x}_{t}) - X^{\theta}_{t,0}(\bm{x}_{t}) .
\end{align*}
Then,
\begin{align*}
e_{t_n,0} 
&= X_{t_n,0}(\bm{x}_{t_n}) - X^{\theta}_{t_n,0}(\bm{x}_{t_n}) \\
&= X_{t_{n+1},0}(\bm{x}_{t_{n+1}}) - X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}})  + X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}}) - X^{\theta}_{t_n,0}(\bm{x}_{t_n}) \\
&= e_{t_{n+1},0} 
+ \Big( X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}}) - X^{\theta}_{t_{n},0}(\bm{x}_{t_n}) \Big) \\
\end{align*}
Consequently,
\begin{align*}
e_{1,0} = e_{0,0} + \sum_{n=1}^{N-1} \Big(X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}}) - X^{\theta}_{t_{n},0}(\bm{x}_{t_n}) \Big) ,
\end{align*}
where $e_{0,0}=0$. Using Eq. \ref{eq:s0t0}, we can get
\begin{align*}
W_2^2(p_0, p^{\theta}_0) &\leq \mathbb{E} \Vert e_{n,0} \Vert_2^2 \\
&\leq \sum_{n=1}^{N-1} \mathbb{E}\Vert X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}}) - X^{\theta}_{t_{n},0}(\bm{x}_{t_n}) \Vert^2_2 \\
&= \sum_{n=1}^{N-1} \mathbb{E} \left[ d\big( X^{\theta}_{t_{n+1},0}(\bm{x}_{t_{n+1}}) , X^{\theta}_{t_{n},0}(\bm{x}_{t_n}) \big) \right] \\
&\leq \sum_{n=1}^{N-1} \frac{\mathcal{L}_1(\theta)}{p(0,t_n,t_{n+1})w(0,t_n,t_{n+1})} \\
&= \left(\sum_{n=1}^{N-1} \frac{1}{p(0,t_n,t_{n+1})w(0,t_n,t_{n+1})} \right) \mathcal{L}_1(\theta).
\end{align*}

With Eq. \ref{eq:l1}, we finally have the Theorem
\end{proof}

\subsubsection{Error Bound of Start Point Differential CTSC}
Next, we prove the error bound of the start point differential CTSC. The derivation is adopted from \cite{flowmap}.

\begin{theorem}\label{thm:ctscbound1} 
% Assume the marginal velocity of the flow path satisfies the one-sided Lipschitz condition, where
%     $$
%     \exists\, C_t \in L^1[0,1] \;:\;
% \bigl(\bm{v}_t(\bm x)-\bm{v}_t(\bm y)\bigr)\cdot(\bm x-\bm y) \geq -C_t \| \bm x-\bm y \|^2,
% \quad \text{for all } (t,\bm x,\bm y)\in[0,1]\times\mathbb{R}^d\times\mathbb{R}^d.
%     $
When $s \to t$, the CTSC loss can be written as 
\[
\mathcal{L}_{\text{ctsc-s-to-t}} =  \mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2
\]
  Under $d(\bm{x}, \bm{y}) = \Vert \bm{x} - \bm{y}\Vert_2^2$, then 
  \begin{align*}
   &W_2^2(p_0, p^\theta_0) \leq C_3 \mathcal{L}_\text{ctsc-s-to-t}(\theta), 
  \end{align*}
  where $C_3=e$, and $W_2(\cdot,\cdot)$ is the Wasserstein-2 distance.    
\end{theorem}

\begin{proof}
Firstly, from the chain rule, 
\begin{align*}
    \frac{d}{dt} X^\theta_{t,r}(\bm{x}_t) = \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t),
\end{align*}
we can simply use $\bm{u}_{t,r}^\theta$ as the model output, and write the term into the expectation of $\mathcal{L}_\text{ctsc-s-to-t}$ as
\begin{align*}
    &\Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2 \\
=&   \Vert \frac{d}{dt} X^\theta_{t,r}(\bm{x}_t)  \Vert_2^2 \\
=&\|\bm{v}_t + \frac{d}{dt}(r-t)\bm{u}_{t,r}^\theta({\bm{x}_t }) \|_2^2 \\
=& \|\bm{u}_{t,r}(\bm{x}_t) - \bm{v}_t - (r-t) \frac{d\bm{u}_{t,r}(\bm{x}_t)}{dt}\|_2^2
\end{align*}
which coincides to the MeanFlow loss $l_{\mathrm{mf}}$ in Eq.~\ref{eq:MeanFlowloss}. While in Remark.~\ref{thm:remark1}, sCT loss is equivalent to MeanFlow loss in linear paths, the CTSC loss when $s \to r$ is also of the same form as claimed. The cosine path version of sCT loss is a variant, so we did not include it in this stage, and will consider it as our future work.
Then, we first define that
\begin{align*}
E_{t,r} \coloneqq \mathbb{E} \Vert X_{t,r}(\bm{x}_t) - X^\theta_{t,r}(\bm{x}_t) \Vert_2^2 .
\end{align*}
After differentiation, we can get
\begin{align*}
    -\frac{d E_{t,r}}{dt}  &= -\mathbb{E} \left[ 2\left( X_{t,r}(\bm{x}_t) - X^\theta_{t,r}(\bm{x}_t) \right) \left( \frac{d X_{t,r}(\bm{x}_t)}{dt} - \frac{d X^\theta_{t,r}(\bm{x}_t)}{dt} \right) \right] \\
    &= \mathbb{E} \left[ 2\left( X_{t,r}(\bm{x}_t) - X^\theta_{t,r}(\bm{x}_t) \right) \left( \frac{d X^\theta_{t,r}(\bm{x}_t)}{dt} \right) \right] \\
    &= \mathbb{E} \left[  2\left( X_{t,r}(\bm{x}_t) - X^\theta_{t,r}(\bm{x}_t) \right) \left( \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \right) \right] \\
    &\leq E_{t,r} + \mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2 ,
\end{align*}
So,
\begin{align*}
    -e^t\partial_tE_{t,r} - e^tE_{t,r} \leq  e^t\mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2 \\
    -\partial_t e^tE_{t,r} \leq e^t \mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2\\
\end{align*}
With $E_{r,r} =0$, we have 
\begin{align*}
    E_{t,r} &\leq \int_{r}^t e^{\tau-t}\mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2d\tau \\
    &\leq e^{1}\int_{r}^t\mathbb{E} \Vert \partial_t X^\theta_{t,r}(\bm{x}_t)+ \bm{v}_t\cdot\nabla X_{t,r}^\theta(\bm{x}_t) \Vert_2^2d\tau\\
    &\leq e \mathcal{L}_{\text{ctsc-s-to-r}}.
\end{align*}
By setting $C_3 = e$, the theorem is proved.
\end{proof}
\subsubsection{Error Bound of End Point Differential CTSC}
Finally, we provide the proof of the error bound of the endpoint differential CTSC.

\begin{theorem} \label{thm:ctscbound2} 
Assume the marginal velocity of the flow path satisfies the one-sided Lipschitz condition, where
    $$
    \exists\, C_t \in L^1[0,1] \;:\;
\bigl(\bm{v}_t(\bm x)-\bm{v}_t(\bm y)\bigr)\cdot(\bm x-\bm y) \geq -C_t \| \bm x-\bm y \|^2,
\quad \text{for all } (t,\bm x,\bm y)\in[0,1]\times\mathbb{R}^d\times\mathbb{R}^d.
    $$
When $s \to r$, the CTSC loss can be written as 
\[\mathcal{L}_\text{ctsc-s-to-r}(\theta) = \mathbb{E} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 \]
  Under $d(\bm{x}, \bm{y}) = \Vert \bm{x} - \bm{y}\Vert_2^2$, then
  \begin{align*}
   &W_2^2(p_0, p^\theta_0) \leq C_3 \mathcal{L}_\text{ctsc-s-to-r}(\theta), 
  \end{align*}
  where $C_3=e^{1+2\int_0^1 |C_t|dt}$, and $W_2(\cdot,\cdot)$ is the Wasserstein-2 distance.  
\end{theorem}

\begin{proof}
Using the one-sided Lipschitz condition, we can get
\begin{align*}
- (X_{t,r}(\bm{x}) - X_{t,r}(\bm{y})) (\bm{v}_r(X_{t,r}(\bm{x})) - \bm v_r(X_{t,r}(\bm{y}))) \leq 2C_t \Vert X_{t,r}(\bm{x}) - X_{t,r}(\bm y) \Vert_2^2.
\end{align*}
We then define
\begin{align*}
    E_{t,r} \coloneqq \mathbb{E}_{\bm{x}} \Vert X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \Vert_2^2.
\end{align*}
With differentiation, we have
\begin{align*}
-\frac{d E_{t,r}}{dr} &= -2\mathbb{E}_{\bm{x}}\left[ \big(X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \big) \big( \frac{d X_{t,r}(\bm x_t)}{dr} - \frac{d X_{t,r}^\theta(\bm x_t)}{dr} \big) \right] \\
&= -2\mathbb{E}_{\bm{x}}\left[ \big(X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \big) \big( \bm v(X_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \big) \right] \\
&= -2\mathbb{E}_{\bm{x}}\left[ \big(X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \big) \big( \bm v(X^\theta_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \big) \right] \\
&\quad - 2\mathbb{E}_{\bm{x}}\left[ \big(X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \big) \big( \bm v(X_{t,r}(\bm x_t)) - \bm v(X^\theta_{t,r}(\bm x_t)) \big) \right] \\
&\leq \mathbb{E}_{\bm{x}} \Vert X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \Vert_2^2 + \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \Vert_2^2 \\
&\quad - 2\mathbb{E}_{\bm{x}}\left[ \big(X_{t,r}(\bm x_t) - X_{t,r}^\theta(\bm x_t) \big) \big( \bm v(X_{t,r}(\bm x_t)) - \bm v(X^\theta_{t,r}(\bm x_t)) \big) \right] \\
&\leq E_{t,r} + \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \Vert_2^2 - 2 C_t E_{t, r} \\
&= (1 - 2 C_t)E_{t,r} + \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \Vert_2^2 .
\end{align*}
So, 
\begin{align*}
\partial_r(-e^{r-2\int_t^r C_\tau d\tau} E_{t,r}) \leq e^{r-2\int_t^r C_\tau d\tau}\mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,r}(\bm x_t)) - \partial_r X_{t,r}^\theta(\bm x_t) \Vert_2^2.
\end{align*}
With $E_{t,t}=0$, we have
\begin{align*}
E_{t,r} 
&\leq \int_r^t e^{-r+\tau+2\int_\tau^r C_\gamma d\gamma} \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 d\tau \\
&\leq e^{-r+1+2\int_r^t |C_\tau| d\tau} \int_r^t \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 d\tau. 
\end{align*}
Therefore, when $t=1, r=0$, we have
\begin{align*}
    E_{1,0} \leq e^{1+2\int_0^1|C_t|dt} \int_0^1 \mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 d\tau .
\end{align*}
Finally, due to 
\begin{align*}
    W_2^2(p_0, p_0^\theta) \leq \mathbb{E}\Vert X_{1,0}(x_1) - X_{1,0}^\theta(x_1) \Vert_2^2
\end{align*}
and
\begin{align*}
\mathcal{L}_\text{ctsc-s-to-r}(\theta) 
&= \mathbb{E} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 \\
&= \int_{[0,1]^2} w(t,t,r)\mathbb{E}_{\bm{x}} \Vert \bm v(X^\theta_{t,\tau}(\bm x_t)) - \partial_\tau X_{t,\tau}^\theta(\bm x_t) \Vert_2^2 dtdr,
\end{align*}
we obtain
\begin{align*}
    W_2^2(p_0, p_0^\theta) &\leq \mathbb{E}\Vert X_{1,0}(\bm x_1) - X_{1,0}^\theta(\bm x_1) \Vert_2^2 \\
    &\leq e^{1+2\int_0^1|C_t|dt} \mathcal{L}_\text{ctsc-s-to-r}(\theta).
\end{align*}
By setting $C_3 = e^{1+2\int_0^1|C_t|dt} $, the theorem is proved.
\end{proof}

\subsection{Optimal Path of Shortcut Model~(Q.1. in Sec.~\ref{sec:elucidating})}
\label{app:thmlinearopt}
Previous works have claimed that the cosine path is optimal for diffusion models from the perspective of Fisher information metric \citep{santos2023usingornsteinuhlenbeckprocessunderstand}. Here, we provide the analysis of the optimal path for one-step models under the Fisher information metric.

We first briefly introduce the Fisher information metric. Treating probability distributions $p(\gamma)$ as a smooth manifold, the Fisher information metric defines a Riemannian geometry that enables the computation of distances between them. Specifically, the definition is 
\[
I(\gamma)_{ij} = \mathbb{E}_{X \sim p_\gamma} \left[ 
\frac{\partial}{\partial \gamma_i} \log p_\gamma(X) 
\frac{\partial}{\partial \gamma_j} \log p_\gamma(X) 
\right].
\]
When the distribution family is exponential as
\[
p(\bm x|\gamma) = h(\bm x)\exp(\eta(\gamma)^T T(\bm x)-\psi(\gamma)),
\]
the Fisher information metric becomes \citep{Karczewski2025SpacetimeGO}
\[
\mathcal{I}_{\gamma} 
= \left( \frac{\partial \eta(\gamma)}{\partial \gamma} \right)^{\!\top} 
  \left( \frac{\partial \mu(\gamma)}{\partial \gamma} \right),
\]
where
\[
\mu(\gamma) = \mathbb{E}[T(x)\mid \gamma] 
= \int T(x) p(x \mid \gamma) dx
\]
is the expectation parameter.
Then we can naturally define the optimal schedule as the geodesic between two distributions, which leads to the theorem below.
\begin{theorem}\citep{zhang2025cosine}
The optimal schedule under the metric $I(\gamma) \in \mathbb{R}$ is generated by $\varphi^*$ of the form
\[
\varphi^*(\gamma) = \Lambda^{-1}(\Lambda \gamma), \quad \text{where} \quad 
\Lambda(s) = \int_0^s \sqrt{I(r)} dr.
\]
\end{theorem}
With these preparations, we now turn to the one-step diffusion. We point out that, for the one-step diffusion, since our goal becomes modeling the average velocity, we no longer consider the manifold of $p(\bm x_t)$, but rather that of $p(\bm u_{0,t}(\bm x_0))=p(\bm x_t - \bm x_0)$. Then, we claim that the linear path is the optimal conditional schedule as follows.
\begin{theorem}
For $\forall \bm x_0$, the linear schedule is the optimal schedule considering $\{p(\bm u_{0,t}\mid \bm{x}_0, t)\}$, i.e,
\[
\gamma = \Lambda^{-1}(\Lambda\gamma), \quad \text{where} \quad \gamma=(\bm x_0,t).
\]
\end{theorem}

\begin{proof}
\begin{align*}
p(\bm u_{0,t}\mid \bm x_0, t)
&= p(\frac{\bm x_t-\bm x_0}{t} \mid \bm x_0, t) \\
&= p(\frac{\alpha_t \bm x_0 - \bm x_0 + \sigma_t \epsilon}{t} \mid \bm x_0,t) \\
&= \mathcal{N}(\bm u_{0,t};\frac{\alpha_t \bm x_0 - \bm x_0}{t}, \frac{\sigma_t^2}{t^2}I) \\
&= \frac{t}{(2\pi)^{d/2}\sigma_t^d}
\exp\big(-\frac{t^2}{2\sigma_t^2}(\Vert \bm u_{0,t} \Vert^2 
- \frac{2\alpha_t-2}{t}\bm u_{0,t}^T \bm x_0 
+ \big(\frac{\alpha_t-1}{t}\big)^2 \Vert \bm x_0 \Vert^2)\big)
\end{align*}
So $p(\bm u_{0,t}\mid\bm x_0, t)$ is exponential, and we have
\begin{align*}
\eta(\bm x_0,t) 
&=-\frac{t^2}{2\sigma_t^2}\big(-\frac{2\alpha_t-2}{t}\bm x_0, 1\big) \\
&=\big( \frac{t(\alpha_t-1)}{\sigma_t^2}\bm x_0, -\frac{t^2}{2\sigma_t^2} \big),
\end{align*}
and
\begin{align*}
T(\bm u_{0,t}) = (\bm u_{0,t}, \Vert \bm u_{0,t} \Vert^2).
\end{align*}
Then 
\begin{align*}
\mu(\bm x_0, t) &= \mathbb{E}[T(\bm u_{0,t}) \mid \bm x_0, t] \\
&= \int T(\bm u_{0,t})p(\bm u_{0,t}\mid \bm x_0,t)d\bm u_{0,t} \\
&= \int (\bm u_{0,t}, \Vert \bm u_{0,t} \Vert^2) \mathcal{N}(\bm u_{0,t};\frac{\alpha_t \bm x_0 - \bm x_0}{t}, \frac{\sigma_t^2}{t^2}I)  d\bm u_{0,t} \\
&= \big( \frac{\alpha_t\bm x_0 - \bm x_0}{t}, 
\big(\frac{\alpha_t\bm x_0 - \bm x_0}{t}\big)^2+\frac{\sigma_t^2}{t^2} \big).
\end{align*}
So
\begin{align*}
\frac{\partial \eta(\bm x_0,t)}{\partial(\bm x_0,t)}
&= \begin{pmatrix}
\dfrac{t(\alpha_t-1)}{\sigma_t^2} 
& \dfrac{(\alpha_t-1+t\alpha_t)\sigma_t^2 - 2t(\alpha_t-1)\dot\sigma_t\sigma_t}{\sigma_t^4} x_0 \\
0 
& -\dfrac{4t\sigma_t^2 - 4\dot\sigma_t \sigma_t t^2}{4\sigma_t^4}
\end{pmatrix},
\end{align*}
\begin{align*}
\frac{\partial \mu(\bm x_0,t)}{\partial(\bm x_0,t)}
&= \begin{pmatrix}
\dfrac{\alpha_t-1}{t} 
& \dfrac{\dot\alpha_t t - \alpha_t + 1}{t^2} \bm x_0 \\
\big(\dfrac{\alpha_t-1}{t}\big)^2 \cdot 2\bm x_0 
& \dfrac{\dot\alpha_t t - \alpha_t + 1}{t^2}\cdot 2\dfrac{\alpha_t-1}{t} \bm x_0 + \dfrac{2\dot\alpha_t\alpha_t t^2 - 2t\sigma_t^2}{t^4}
\end{pmatrix} \\
&= \begin{pmatrix}
\dfrac{\alpha_t-1}{t} 
& \dfrac{\dot\alpha_t t - \alpha_t + 1}{t^2} \bm x_0 \\
2\big(\dfrac{\alpha_t-1}{t}\big)^2 \bm x_0 
& \dfrac{2(\dot\alpha_t t - \alpha_t + 1)(\alpha_t-1)}{t^3} \bm x_0 + \dfrac{2\dot\alpha_t\alpha_t t^2 - 2t\sigma_t^2}{t^4}
\end{pmatrix}.
\end{align*}
Substituting the linear schedule $\alpha_t = 1 - t$ and $\sigma_t = t$, we obtain
\begin{align*}
\frac{\partial \eta(\bm x_0,t)}{\partial(\bm x_0,t)}
&=  \begin{pmatrix}
\dfrac{t(- t)}{t^2} & \dfrac{-2t \cdot t^2 - 2t(1-t)t + 2t^2}{t^4} \bm x_0 \\
0 & \dfrac{t^3 - t^3}{t^4}
\end{pmatrix} \\
&= \begin{pmatrix}
-1 & \dfrac{-2t^3 - 2t^2 + 2t^3 + 2t^2}{t^4} \bm x_0 \\
0 & 0
\end{pmatrix} \\
&= \begin{pmatrix}
-1 & 0 \\
0 & 0
\end{pmatrix}
\end{align*}
and
\begin{align*}
\frac{\partial \mu(\bm x_0,t)}{\partial(\bm x_0,t)}
&= \begin{pmatrix}
\dfrac{1 - t - 1}{t} & 
\dfrac{-t - 1 + t + 1}{t^2} \bm x_0 \\
2\bm x_0 & 
\dfrac{-t - 1 + t + 1}{t^2} \cdot 2\dfrac{1-t - 1}{t} \bm x_0 + \dfrac{2t^3 - 2t^3}{t^4}
\end{pmatrix} \\
&= \begin{pmatrix}
-1 & 0 \\
2\bm x_0 & 0
\end{pmatrix}.
\end{align*}
Based on these two equations, we get
\begin{align*}
I(\bm x_0,t)
&= \begin{pmatrix}
-1 & 0 \\
0 & 0
\end{pmatrix}^T
\begin{pmatrix}
-1 & 0 \\
2\bm x_0 & 0
\end{pmatrix}\\
&= \begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix},
\end{align*}
which means the metric under the linear path is uniform. So the probability $p(\bm u_{0,t} | \bm{x}_0, t)$ travels at a constant rate, leading to the optimum.
\end{proof}


\subsection{Proof of Inference Error Analysis (Prop.~\ref{thm:infererror})}
\label{sec:thminfererror}
We first give a detailed version of Prop.~\ref{thm:infererror} as following,
\begin{propositiongray}
    [Inference error analysis] \label{app:thmomfererror} \small
\label{thm:infererrordetail}
Under mild regularity conditions shown in Appendix~\ref{app:assumptions}, 
the Wasserstein-2 distance of the shortcut model with one-step generation is bounded as:
\begin{align}
&W_2^2(p_0, p^\theta_0)
\leq  2\left(\mathrm{BV_{\text{ctsc}}} +8 \mathrm{Var}\big[\frac{d}{dt} \bm u_{t,r}^\theta(\bm x_t)\big] 
+ 8\sigma_{\bm{v}_{t|0}}^2 \right) \Big|_{r=0,t=1}, \label{eq:bverror-app}\\
&W_2^2(p_0, p^\theta_0) \leq 2\Big( \mathrm{BV_{\text{dtsc}}} 
+ 8\delta_2^2\,\mathrm{Var}\big[\bm u_{s,r}^\theta(\bm x_t)\big]
+8(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\text{dtsc}}^2\Big)\Big|_{r=0,t=1}, 
% &W_2^2(p_0, p^\theta_0) \leq 2\Big( \mathrm{BV_{\text{scd}}}
% + 8\delta_2^2\,\mathrm{Var}\big[\bm u_{s,r}^\theta(\bm x_t)\big]
% +8(1+\ell^2\delta_2^2)\delta_1^2\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,s)\big]\Big)\Big|_{r=0,t=1},
\end{align}
where $\mathrm{BV_{\bullet}} = \mathrm{Bias}^2_{\bullet\text{-tgt}} + \mathrm{Bias}^2_{\bullet\text{-loss}} +2\mathrm{Var}\!\big[\bm u^\theta(\bm x_1,t,r)\big]$  with $\bullet\in\{\mathrm{ctsc},\mathrm{dtsc}\}$, and $\mathrm{Bias}^2_{\bullet\text{-tgt}}$ and $\mathrm{Bias}^2_{\bullet\text{-loss}}$ are defined as
\begin{equation}
    \begin{aligned}
        \mathrm{Bias}^2_{\bullet\text{-tgt}} &= \mathbb{E} [\| \bm{u}_{t,r}(\bm{x}_t) - Y_{\bullet}\|_2^2] \\
        \mathrm{Bias}^2_{\bullet\text{-loss}} &= \mathbb{E} [l_\bullet(\bm{x}_t,t,r;\theta)]        
    \end{aligned}
\end{equation}
$ Y_{\bullet}$ is the two flow map target when $Y_{\text{ctsc}}$ in each model, \emph{i.e.} 
\begin{align*}
    Y_{\text{ctsc}} &= \frac{d}{dt} \bm u^\theta(\bm x_t,t,r)\;-\;\bm v_{t|0}(\bm x_t| \bm x_0), \\
    Y_{\text{dtsc}} &= \delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\;+\;\delta_2\,\bm u_{s,r}^\theta\!\big(\bm x_t+\delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\big) \qquad \text{if use CT loss ,}\\
    Y_{\text{dtsc}} &= \delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\;+\;\delta_2\,\bm u^\theta\!\big(\bm x_t+\delta_1\,\bm u_{s,r}^\theta(\bm x_t,t,s)\big) \qquad \text{if use SCD loss.}
\end{align*}
$l_\bullet(\bm{x}_t,r,s,t;\theta)$ is the term in the expectation of different training objectives as given in Sec.~\ref{sec:examples};
$\delta_1 = t-s$, $\delta_2 = s-r$;  $\ell$ is local Lipschitz constant of $\bm u^\theta$; $\sigma_{\bm{v}_{t|0}}^2$ is the variance of the conditional velocity, defined by $
\sigma_{\bm{v}_{t|0}}^2 \coloneqq \mathrm{Var}(\bm v_{t|0}(\bm x_t| \bm x_0)|)$; $\sigma_{\text{dtsc}}^2 = \sigma_{\bm{v}_{t|0}}^2$ when using CT's flow map targets, or $\sigma_{\text{dtsc}}^2 = \mathrm{Var}\!\big[\bm u_{t,s}^\theta(\bm x_t)\big]$ when using SCD's targets.
\end{propositiongray}
\subsubsection{Assumptions for Prop.~\ref{thm:infererror}}
\label{app:assumptions}

We state the regularity conditions required for the theorem.

\begin{assumption}[Velocity variance]
\label{assump:noise}
The conditional velocity $\bm v_{t|0}$ approximates the ground-truth velocity $\bm v_t$, such that we can write\[
\bm v_{t|0}(\bm x_t| \bm x_0) = \bm v_t(\bm x_t)  + \bm \eta_t,
\]
where we assume the discrepancy $\bm \eta_t$ has variance $\sigma_{\bm{v}_{t|0}}^2$.
Because $ E[\bm \eta_t | \bm x_0] = \bm 0$ (take expectation on both sides), so
 $\mathrm{Var}(\bm{v}_{t|0}) = \sigma_{\bm{v}_{t|0}}^2$
\end{assumption}

\begin{assumption}[Lipschitz continuity]
\label{assump:lipschitz}
There exist constants $\ell$ such that for any 
$\bm h$,
\[
\|\bm u^\theta(\bm x_t+\bm h, r,s) - \bm u^\theta(\bm x_t, r,s)\|
\;\le\; \ell\,\|\bm h\|,
\]
with $\ell$ independent of $(\bm x_t,t,r,s)$.
\end{assumption}

% \begin{assumption}[Variance comparability across horizons]
% \label{assump:variance}
% For any fixed input $\bm x_t$ and nearby time indices $(t,s,r)$,
% the variances of the model outputs at horizons $t$ and $r$ (with the same anchor $s$) 
% are comparable up to a constant factor $\kappa \ge 1$. That is,
% \[
% \frac{1}{\kappa}\,
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,t,s)\big]
% \;\le\;
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,s,r)\big]
% \;\le\;
% \kappa\,
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,t,s)\big],
% \]
% which can be simply written as 
% \[
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,r,s)\big]
% \;\asymp\; \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,t,s)\big],
% \]
% \end{assumption} 


% \begin{assumption}[Controlled cross-covariance]
% \label{assump:cross}
% The covariance between model outputs and flow-map targets is bounded in magnitude. 
% For each objective $\bullet \in \{\mathrm{ctsc},\mathrm{ct},\mathrm{scd}\}$,
% \[
% \big|\,\mathrm{tr}\,\mathrm{Cov}\!\big(\bm u^\theta(\cdot), Y_\bullet\big)\,\big|
% \;\le\; \rho_\bullet\,
% \sqrt{\mathrm{tr}\,\mathrm{Cov}[\bm u^\theta(\cdot)]}\,
% \sqrt{\mathrm{tr}\,\mathrm{Cov}[Y_\bullet]},
% \]
% for some correlation constant $0 \le \rho_\bullet < 1$.
% \end{assumption}

% \begin{assumption}[Local time smoothness]
% \label{assump:time-lip}
% There exists $L_t<\infty$ such that for all $(\bm x_t,t,r)$ in a neighborhood,
% \[
% \big\|d \bm u^\theta(\bm x_t,t,r) /dt\big\| \;\le\; L_t
% \qquad\text{(a.e.\ in the sense of distributions).}
% \]
% Equivalently, $\|\bm u^\theta(\bm x_{t'},t',r)-\bm u^\theta(\bm x_t,t,r)\|
% \le L_t\,|t'-t|$ for $t'$ near $t$.
% \end{assumption}


\subsubsection{Lemma Used for proof}


\begin{lemma}[Bias--variance--covariance(BV-CV) decomposition]
For random vectors $A,B\in \mathbb{R}^d$ with finite second moments,
\begin{equation}
\label{eq:bvc-app}
\mathbb E\|A-B\|^2
= \|\mathbb EA-\mathbb EB\|^2
+ \mathrm{tr}\,\mathrm{Cov}[A]
+ \mathrm{tr}\,\mathrm{Cov}[B]
- 2\,\mathrm{tr}\,\mathrm{Cov}[A,B].
\end{equation}
\end{lemma}
\begin{proof}
Denote $A$ and $B$'s expectations by 
$\mu_A = \mathbb{E}A$ and $\mu_B = \mathbb{E}B$. We start by expanding
\[
\mathbb{E}\|A-B\|^2 = \mathbb{E}\big[(A-B)^\top (A-B)\big]
= \mathbb{E}\|A\|^2 + \mathbb{E}\|B\|^2 - 2\,\mathbb{E}[A^\top B].
\]

Each term can be decomposed as follows:
\begin{align*}
\mathbb{E}\|A\|^2 
&= \operatorname{tr}(\mathbb{E}[AA^\top]) 
= \operatorname{tr}\!\left(\operatorname{Cov}[A] + \mu_A \mu_A^\top \right) \\
&= \operatorname{tr}\,\operatorname{Cov}[A] + \|\mu_A\|^2, \\
\mathbb{E}\|B\|^2 
&= \operatorname{tr}(\mathbb{E}[BB^\top]) 
= \operatorname{tr}\!\left(\operatorname{Cov}[B] + \mu_B \mu_B^\top \right) \\
&= \operatorname{tr}\,\operatorname{Cov}[B] + \|\mu_B\|^2, \\
\mathbb{E}[A^\top B] 
&= \operatorname{tr}(\mathbb{E}[AB^\top]) 
= \operatorname{tr}\!\left(\operatorname{Cov}[A,B] + \mu_A \mu_B^\top \right) \\
&= \operatorname{tr}\,\operatorname{Cov}[A,B] + \mu_A^\top \mu_B.
\end{align*}

Substituting these expressions back, we obtain
\[
\mathbb{E}\|A-B\|^2
= \|\mu_A - \mu_B\|^2 
+ \operatorname{tr}\,\operatorname{Cov}[A]
+ \operatorname{tr}\,\operatorname{Cov}[B]
- 2\,\operatorname{tr}\,\operatorname{Cov}[A,B].
\]

This establishes the bias–variance–covariance identity.
\end{proof}

\begin{lemma}[Variance lower bound under local bi-Lipschitz]
\label{lem:shift-var-lb}
Let $f(\bm x)\coloneqq \bm u^\theta(\bm x,r,s)$ and assume local bi-Lipschitz: 
there exists $c>0$ such that for all sufficiently small $\bm h$,
\[
\|f(\bm x+\bm h)-f(\bm x)\|\ \ge\ c\,\|\bm h\|.
\]
Fix $\bm x_t$ and let $W$ be a random vector. Define
\[
Z \;=\; f(\bm x_t+\delta_1 W)-f(\bm x_t).
\]
Then, conditioning on the $\sigma$-field that renders $\bm x_t$ deterministic,
\[
\operatorname{tr}\,\operatorname{Cov}(Z| \bm x_t) \ \ge\ c^2\,\delta_1^2\,\operatorname{tr}\,\operatorname{Cov}(W| \bm x_t).
\]
Consequently,
\[
\operatorname{tr}\,\operatorname{Cov}(Z)\ \ge\ c^2\,\delta_1^2\,\mathbb E\big[\operatorname{tr}\,\operatorname{Cov}(W| \bm x_t)\big].
\]
\end{lemma}

\begin{proof}
Set $\bar W \coloneqq \mathbb E[W| \bm x_t]$ and write
\[
Z \;=\; \underbrace{f(\bm x_t+\delta_1 W)-f(\bm x_t+\delta_1 \bar W)}_{Z'} 
\;+\; \underbrace{f(\bm x_t+\delta_1 \bar W)-f(\bm x_t)}_{\text{constant given }\bm x_t}.
\]
Adding a constant does not change variance, hence 
$\operatorname{tr}\,\operatorname{Cov}(Z| \bm x_t)=\operatorname{tr}\,\operatorname{Cov}(Z'| \bm x_t)$.
By the bi-Lipschitz lower bound,
\[
\|Z'\|\;=\;\big\|f\big(\bm x_t+\delta_1(W-\bar W)\big)-f(\bm x_t)\big\|
\ \ge\ c\,\delta_1\,\|W-\bar W\|.
\]
Squaring and taking the conditional expectation,
\[
\mathbb E\big[\|Z'\|^2| \bm x_t\big]\ \ge\ c^2\,\delta_1^2\,\mathbb E\big[\|W-\bar W\|^2| \bm x_t\big]
\;=\; c^2\,\delta_1^2\,\operatorname{tr}\,\operatorname{Cov}(W| \bm x_t).
\]
Since $\operatorname{tr}\,\operatorname{Cov}(Z'| \bm x_t)\le \mathbb E[\|Z'\|^2| \bm x_t]$, we obtain
\(
\operatorname{tr}\,\operatorname{Cov}(Z| \bm x_t)=\operatorname{tr}\,\operatorname{Cov}(Z'| \bm x_t)\ \ge\ c^2\,\delta_1^2\,\operatorname{tr}\,\operatorname{Cov}(W| \bm x_t).
\)
Taking expectation in $\bm x_t$ and using the law of total variance gives the second claim.
\end{proof}

% \begin{lemma}[Variance bound for $(r-t)\,\partial_t \bm u^\theta$]
% \label{lem:time-leg}
% Under Assumption~\ref{assump:time-lip},
% \[
% \operatorname{tr}\,\operatorname{Cov}\!\big[(r-t)\,\partial_t \bm u^\theta(\bm x_t,t,r)\big]
% \;\le\; (r-t)^2\,L_t^2.
% \]
% \end{lemma}

% \begin{proof}
% By Assumption~\ref{assump:time-lip}, 
% $\|(r-t)\,\partial_t \bm u^\theta(\bm x_t,t,r)\|
% \le |r-t|\,L_t$ pointwise. Hence
% $\mathbb E\| (r-t)\,\partial_t \bm u^\theta \|^2 \le (r-t)^2 L_t^2$,
% and therefore
% $\operatorname{tr}\,\operatorname{Cov}\big[(r-t)\,\partial_t \bm u^\theta\big]
% \le \mathbb E\| (r-t)\,\partial_t \bm u^\theta \|^2
% \le (r-t)^2 L_t^2$.
% \end{proof}

\subsubsection{Proof for Theorem~\ref{thm:infererror}}

Write $\delta_1=s-t$, $\delta_2=r-s$ and note that in inequalities below we only use $\delta_1,\delta_2$.

% \paragraph{Step 0. Notation.} \label{app:notation}
% Throughout this section we use the symbols 
% $\lesssim$ and $\gtrsim$ to denote inequalities that hold 
% up to a universal constant factor independent of 
% $(\bm x_t,t,r,s)$ and $\theta$. 
% That is, $A \lesssim B$ means there exists $C>0$ such that 
% $A \le C B$, and $A \gtrsim B$ means $A \ge c B$ for some $c>0$. 
% Let 
% \[
% X \;=\; (\bm x_t,t,s,r)
% \]
% denote the random input quadruple, where the randomness comes from the data distribution 
% of $\bm x_0$ and the conditional law of $\bm x_t | \bm x_0$.
% For a given $X$, we write
% \[
% Y_{\mathrm{ctsc}},\quad Y_{\mathrm{ct}},\quad Y_{\mathrm{scd}}
% \]
% for the corresponding flow map targets defined in Sec.~\ref{sec:designframe}. 

\paragraph{Step 1. Upper bound for CTSC (MeanFlow and sCT with linear path).}
Here we consider the sampling error from $t$ to $r$, as 
\[E_{t,r} = \mathbb{E}[\|X_{t,r}(\bm{x}_t) - X_{t,r}^{\theta} (\bm{x}_t) \|_2^2].
\]
It can be written as 
\begin{align*}
&\mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) \|_2^2] \\
 =&   \mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) -(r-t)Y_{\text{ctsc}} +(r-t)Y_{\text{ctsc}}\|_2^2] \\
 \leq& 2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}_{t,r}(\bm{x}_t) - Y_{\text{ctsc}} \|_2^2 +  2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}^\theta_{t,r}(\bm{x}_t) - Y_{\text{ctsc}} \|^2_2
\end{align*}
where $Y_{\text{ctsc}} = (\delta_1+\delta_2)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\;- \bm v_{t|0}(\bm x_t| \bm x_0)$.

First, consider the first term take
\[
A \;=\; \bm u_{t,r}(\bm x_t),\qquad
B \;=\; Y_{\mathrm{ctsc}} \;=\; (\delta_1+\delta_2)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\;-\;\bm v_{t|0}(\bm x_t| \bm x_0).
\]
Applying Eq. \ref{eq:bvc-app},
\[
 2(\delta_1 + \delta_2) ^2\mathbb E\,\|A-B\|_2^2
=  2(\delta_1 + \delta_2) ^2(\underbrace{\|\mathbb EA-\mathbb EB\|^2}_{\mathrm{Bias}^2_{\text{ctsc-tgt}}}
+ \mathrm{Var}[A]
+ \mathrm{Var}[B]
-2\,\mathrm{Cov}(A,B)).
\]

According to 
\(
-2\,\mathrm{Cov}(A,B)\ \leq \mathrm{Var}[A]+\mathrm{Var}[B]
\), we can get
\begin{align}
&\mathbb E\,[\|A-B\|_2^2]\\
\;\leq\;& \mathrm{Bias}^2_{\text{ctsc-tgt}}
+ 2\mathrm{Var}[\bm{u}_{t,r}]+ 2\mathrm{Var}[Y_{\text{ctsc}}]
\\
    =&\mathrm{Bias}^2_{\text{ctsc-tgt}} + 2\mathrm{Var}[Y_{\text{ctsc}}],
\end{align}
because $\mathrm{Var}[\bm{u}_{t,r}] = 0$.

Then, by Assumption~\ref{assump:noise} ($\mathrm{Var}[\bm v_{t|0}]= \sigma_{\bm{v}_{t|0}}^2$),
\begin{align*}
\mathrm{Var}[Y_{\text{ctsc}}]
&= \mathrm{Var}\!\big[(\delta_1+\delta_2)\,\frac{d}{dt} \bm u^\theta_{t,r} - \bm v_{t|0}\big] \\
&\le\; 2\,\mathrm{Var}\!\big[(\delta_1+\delta_2)\,\frac{d}{dt} \bm u^\theta_{t,r}\big]
+ 2\,\mathrm{Var}[\bm v_{t|0}] \\
&=\; 2(\delta_1+\delta_2)^2 \mathrm{Var}\big[\frac{d}{dt} \bm u^\theta_{t,r}\big] + 2\sigma_{\bm{v}_{t|0}}^2,
\end{align*}
Therefore, 
\begin{align*}
& \mathbb{E}\|\bm{u}_{t,r}(\bm{x}_t) - Y_{\text{ctsc}} \|^2 \\
\leq& \mathrm{Bias}^2_{\text{ctsc-tgt}} + 4(\delta_1+\delta_2)^2 \mathrm{Var}\big[\frac{d}{dt} \bm u^\theta_{t,r}\big] + 4\sigma_{\bm{v}_{t|0}}^2    
\end{align*}

Secondly, take
\[
A \;=\; \bm u^\theta_{t,r}(\bm x_t),\qquad
B \;=\; Y_{\mathrm{ctsc}} \;=\; (\delta_1+\delta_2)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\;-\;\bm v_{t|0}(\bm x_t| \bm x_0).
\]
and it coincides that
$ \mathbb{E}\|\bm{u}^\theta_{t,r}(\bm{x}_t) - Y_{\text{ctsc}} \|^2_2 = \mathbb{E}[l_{\text{ctsc}}]$.
Applying Eq. \ref{eq:bvc-app}, we have
\[
\mathbb E\,[l_{\text{ctsc}}]
= \underbrace{\|\mathbb EA-\mathbb EB\|^2}_{\mathrm{Bias}^2_{\text{ctsc-loss}}}
+ \mathrm{Var}[A]
+ \mathrm{Var}[B]
-2\,\mathrm{Cov}(A,B).
\]
It can also be easily to obtain
\[
\mathbb E\,[l_{\mathrm{ctsc}}]
\;\leq\;\mathrm{Bias}^2_{\text{ctsc-loss}}
+ 2\mathrm{Var}\!\big[\bm u^\theta_{t,r}(\bm x_t)\big]
+ 4(\delta_1+\delta_2)^2 \mathrm{Var}\big[\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\big]
+ 4\sigma_{\bm{v}_{t|0}}^2,
\]

In summary, 
\begin{align*}
    &E_{t,r} \\
    \leq&  2(\delta_1 + \delta_2) ^2\left(\mathrm{Bias}^2_{\text{ctsc-tgt}} + \mathrm{Bias}^2_{\text{ctsc-loss}} +2\mathrm{Var}\!\big[\bm u^\theta_{t,r}(\bm x_t)\big] +8(\delta_1+\delta_2)^2 \mathrm{Var}\big[\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\big] 
+ 8\sigma_{\bm{v}_{t|0}}^2 \right)
\end{align*}
Specifically, when $t=1, r=0$, 
\begin{align*}
    &E_{1,0} 
    \leq  2\left(\mathrm{Bias}^2_{\text{ctsc-tgt}} + \mathrm{Bias}^2_{\text{ctsc-loss}} +2\mathrm{Var}\!\big[\bm u^\theta_{t,r}(\bm x_1)\big] +8 \mathrm{Var}\big[\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)\big] 
+ 8\sigma_{\bm{v}_{t|0}}^2 \right) \Big|_{r=0,t=1}
\end{align*}

\paragraph{Step 2. Upper bound for CT.}
Then, let's still consider 
\[E_{t,r} = \mathbb{E}[\|X_{t,r}(\bm{x}_t) - X_{t,r}^{\theta} (\bm{x}_t) \|_2^2],
\]
by setting $Y_{\mathrm{ct}} \;=\frac{1}{\delta_1+\delta_2}\left(\; \delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\;+\;\delta_2\,\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\big)\right)$, which equals to
\begin{align*}
&\mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) \|_2^2] \\
 =&   \mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) -(r-t)Y_{\text{ct}} +(r-t)Y_{\text{ct}}\|_2^2] \\
 \leq& 2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}_{t,r}(\bm{x}_t) - Y_{\text{ct}} \|_2^2 +  2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}^\theta_{t,r}(\bm{x}_t) - Y_{\text{ct}} \|^2_2
\end{align*}
Firstly, set
\[
A \;=\; \bm u_{t,r}(\bm x_t),\qquad
B \;=\; Y_{\mathrm{ct}} \;=\; \frac{1}{\delta_1 + \delta_2} \big( \delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\;+\;\delta_2\,\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\big) \big).
\]
By Cauchy-Schwarz,
\(
-2\,\mathrm{Cov}(A,B)
\)
is again absorbed into the $\lesssim$ notation. So we have
\begin{align}
&\mathbb E\,[\|A-B\|_2^2]\\
\;\leq\;& \mathrm{Bias}^2_{\text{ct-tgt}}
+ 2\mathrm{Var}[\bm{u}_{t,r}]+ 2\mathrm{Var}[Y_{\text{ct}}]
\\
    =&\mathrm{Bias}^2_{\text{ct-tgt}} + 2\mathrm{Var}[Y_{\text{ct}}],
\end{align}

For $\mathrm{Var}[Y_{\text{ct}}]$, expand the second term as
\[
\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm v_{t|0}\big)
= \bm u^\theta_{s,r}(\bm x_t)\;+\;\Big(\bm u^\theta_{s,r}(\bm x_t+\delta_1\,\bm v_{t|0})-\bm u^\theta_{s,r}(\bm x_t)\Big).
\]
Apply Lemma~\ref{lem:shift-var-lb} with 
\(f(\bm x)=\bm u^\theta_{s,r}(\bm x)\) and 
\(W=\bm v_{t|0}(\bm x_t| \bm x_0)\). 
Conditioning on $\bm x_t$ and using the Lipschitz upper constant \(\ell>0\), we obtain
\[
\mathrm{Var}\!\Big(\bm u^\theta_{s,r}(\bm x_t+\delta_1\,\bm v_{t|0})-\bm u^\theta_{s,r}(\bm x_t)\,\Big|\bm x_t\Big)
\ \leq \ \ell^2 \,\delta_1^2\,\mathrm{Var}\!\big(\bm v_{t|0}(\bm x_t| \bm x_0)\,\big|\,\bm x_t\big).
\]
Taking expectation in $\bm x_t$ yields
\[
\mathrm{Var}\!\Big[\bm u^\theta_{s,r}(\bm x_t+\delta_1\,\bm v_{t|0})-\bm u^\theta_{s,r}(\bm x_t)\Big]
\leq \ \ell^2\,\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2.
\]

Therefore,
\[
\mathrm{Var}[Y_{\text{ct}}]
\ \leq \ \frac{1}{(\delta_1 + \delta_2)^2}
\Big( 2\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2
\;+\;
2\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
\;+\;
2l^2\,\delta_1^2\,\delta_2^2\,\sigma_{\bm{v}_{t|0}}^2\Big).
\]
Hence, we get
\[
\mathbb{E}\|\bm{u}_{t,r}(\bm{x}_t) - Y_{\text{ct}} \|_2^2
\leq \,
\mathrm{Bias}^2_{\text{ct-tgt}}
+ \frac{4}{(\delta_1 + \delta_2)^2}
\Big( 
\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
\;+\;
(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2\Big).
\]

Secondly, set
\[
A \;=\; \bm u^\theta_{t,r}(\bm x_t),\qquad
B \;=\; Y_{\mathrm{ct}} \;=\; \frac{1}{\delta_1 + \delta_2} \big( \delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\;+\;\delta_2\,\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm v_{t|0}(\bm x_t| \bm x_0)\big) \big).
\]
Then $ \mathbb{E}\|\bm{u}^\theta_{t,r}(\bm{x}_t) - Y_{\text{ct}} \|^2_2 = \mathbb{E}[l_{\text{ct}}]$.
Easily following the above derivation, we can obtain
\[
\mathbb E[l_{\text{ct-loss}}]
\leq \mathrm{Bias}^2_{\text{ct-loss}}
+ 2\mathrm{Var}\big[\bm u^\theta_{t,r}(\bm x_t)\big]
+ \frac{4}{(\delta_1 + \delta_2)^2}
\Big( 
\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
+(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2\Big).
\]
To sum up, we have
\begin{align*}
&E_{t,r} \leq 2(\delta_1 + \delta_2)^2 \Big( \mathrm{Bias}^2_{\text{ct-tgt}} + \mathrm{Bias}^2_{\text{ct-loss}}
+ 2\mathrm{Var}\big[\bm u^\theta_{t,r}(\bm x_t)\big] \Big. \\
&\qquad\qquad\qquad\qquad \Big.
+ \frac{8}{(\delta_1 + \delta_2)^2}
\Big( 
\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
+(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2\Big)\Big).
\end{align*}
When $t=0, r=1$, the inequality becomes
\begin{align*}
&E_{1,0} \leq 2\Big( \mathrm{Bias}^2_{\text{ct-tgt}} + \mathrm{Bias}^2_{\text{ct-loss}}
+ 2\mathrm{Var}\big[\bm u^\theta_{t,r}(\bm x_t)\big] \Big. \\
&\qquad\qquad \Big.
+ 8\delta_2^2\,\mathrm{Var}\big[\bm u^\theta_{s,r}(\bm x_t)\big]
+8(1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2\Big)\Big|_{r=0,t=1}.
\end{align*}
\paragraph{Step 3. Upper bound for SCD.}
In this case, consider 
\[E_{t,r} = \mathbb{E}[\|X_{t,r}(\bm{x}_t) - X_{t,r}^{\theta} (\bm{x}_t) \|_2^2],
\]
by setting $Y_{\mathrm{scd}} \;=\frac{1}{\delta_1+\delta_2}\left(\; \delta_1\,\bm{u}^\theta_{t,s}(\bm{x}_t)\;+\;\delta_2\,\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm u^\theta_{t,s}(\bm x_t)\big)\right)$, which equals to
\begin{align*}
&\mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) \|_2^2] \\
 =&   \mathbb{E}[\|(r-t)\bm{u}_{t,r}(\bm{x}_t) - (r-t)\bm{u}_{t,r}^{\theta} (\bm{x}_t) -(r-t)Y_{\text{scd}} +(r-t)Y_{\text{scd}}\|_2^2] \\
 \leq& 2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}_{t,r}(\bm{x}_t) - Y_{\text{scd}} \|_2^2 +  2(\delta_1 + \delta_2) ^2 \mathbb{E}\|\bm{u}^\theta_{t,r}(\bm{x}_t) - Y_{\text{scd}} \|^2_2
\end{align*}

We can find that the only difference between SCD and CT is the second term in $Y_{\mathrm{scd}}$. So we only need to analyze this term:
\[
\bm u^\theta_{s,r}\!\big(\bm x_t+\delta_1\,\bm u^\theta_{t,s}(\bm x_t)\big)
= \bm u^\theta_{s,r}(\bm x_t)\;+\;
\Big(\bm u^\theta_{s,r}(\bm x_t+\delta_1\,\bm u^\theta_{t,s}(\bm x_t)) - \bm u^\theta_{s,r}(\bm x_t)\Big).
\]
% By Assumption~\ref{assump:variance}, 
% \(
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,s,r)\big]
% \asymp 
% \mathrm{tr}\,\mathrm{Cov}\!\big[\bm u^\theta(\bm x_t,t,s)\big].
% \)
By Assumption~\ref{assump:lipschitz} (Lipschitz continuity), this term contributes a variance of order 
\(\ell^2\,\delta_1^2\,\mathrm{Var}[\bm u^\theta_{t,s}(\bm x_t)]\).
Hence,
\[
\mathrm{Var}[Y_{\mathrm{scd}}]
\;\leq\; \delta_1^2\,\mathrm{Var}\!\big[\bm u^\theta_{t,s}(\bm x_t)\big]
\;+\;\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
\;+\;\ell^2\,\delta_1^2\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{t,s}(\bm x_t)\big].
\]

Similar to the derivation of CT, we can get
\begin{align*}
&E_{t,r} \leq 2(\delta_1 + \delta_2)^2 \Big( \mathrm{Bias}^2_{\text{scd-tgt}} + \mathrm{Bias}^2_{\text{scd-loss}}
+ 2\mathrm{Var}\big[\bm u^\theta_{t,r}(\bm x_t)\big] \Big. \\
&\qquad\qquad\qquad\qquad \Big.
+ \frac{8}{(\delta_1 + \delta_2)^2}
\Big( 
\delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta_{s,r}(\bm x_t)\big]
+(1+\ell^2\delta_2^2)\delta_1^2\mathrm{Var}\!\big[\bm u^\theta_{t,s}(\bm x_t)\big]\Big)\Big).
\end{align*}
When $t=0, r=1$, the inequality becomes
\begin{align*}
&E_{1,0} \leq 2\Big( \mathrm{Bias}^2_{\text{scd-tgt}} + \mathrm{Bias}^2_{\text{scd-loss}}
+ 2\mathrm{Var}\big[\bm u^\theta_{t,r}(\bm x_t)\big] \Big.\\
&\qquad\qquad \Big.
+ 8\delta_2^2\,\mathrm{Var}\big[\bm u^\theta_{s,r}(\bm x_t)\big]
+8(1+\ell^2\delta_2^2)\delta_1^2\mathrm{Var}\!\big[\bm u^\theta_{t,s}(\bm x_t)\big]\Big)\Big|_{r=0,t=1}.
\end{align*}

\paragraph{Step 4. Conclusion}
Since 
\begin{align*}
    W_2^2(p_0, p_0^\theta) &\leq \mathbb{E}\Vert X_{1,0}(\bm x_1) - X_{1,0}^\theta(\bm x_1) \Vert_2^2  = E_{1,0}
\end{align*} 
The proposition is proved.

% \paragraph{Discussion.}
% For sufficiently small but nonzero $|\delta_1|$, the three bounds can be compared term by term.

% \noindent
% The CTSC upper bound contains
% \[
% \mathbb E\,[l_{\mathrm{ctsc}}]
% \;\lesssim\; \mathrm{Bias}^2_{\mathrm{ctsc}}
% + \mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,r)\big]
% + (\delta_1+\delta_2)^2 \mathrm{Var}\big[\partial_t \bm u^\theta(\bm x_t,t,r)\big]
% + \sigma_{\bm{v}_{t|0}}^2,
% \]
% in addition to its bias term.

% \noindent
% The CT lower bound contains
% \[
% \mathbb E\,[l_{\mathrm{ct}}]
% \;\lesssim\; \mathrm{Bias}^2_{\mathrm{ct}}
% + (\delta_1+\delta_2)^2\,\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,r)\big]
% + \delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,s,r)\big]
% + (1+\ell^2\delta_2^2)\delta_1^2\,\sigma_{\bm{v}_{t|0}}^2.
% \]

% \noindent
% The SCD lower bound contains
% \begin{align*}
% \mathbb E\,[l_{\mathrm{scd}}]
% &\lesssim \mathrm{Bias}^2_{\mathrm{scd}}
% + (\delta_1+\delta_2)^2\,\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,r)\big]
% + \delta_2^2\,\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,s,r)\big]
% \\
% &\qquad\qquad\qquad\qquad\qquad\qquad 
% + (1+\ell^2\,\delta_2^2\,)\delta_1^2\,\mathrm{Var}\!\big[\bm u^\theta(\bm x_t,t,s)\big],
% \end{align*}

% \noindent

% CTSC carries the conditional velocity variance burden $\sigma_{\bm{v}_{t|0}}^2$ 
% and the time-smoothness correction $(\delta_1+\delta_2)^2L_t^2$.
% Consequently, CTSC is strictly preferable whenever
% \[
% \sigma_{\bm{v}_{t|0}}^2 + (\delta_1+\delta_2)^2L_t^2 \;\ll\; \delta_2^2\,\mathrm{Var}[\bm u^\theta],
% \]
% for instance, when the conditional velocity is accurate and the model is smooth in $t$,
% while $s-r$ is not vanishingly small.

\subsection{Proof of Ideal Velocity and its Bias-Variance Analysis (Prop.~\ref{thm:idealvel})}
\label{app:thmidealvel}

\subsubsection{The Form of Ideal Velocity}
\begin{proof}
By definition, 
    \begin{align*}
        \bm{v}_t = \int \bm{v}_t(\bm{x}_t | \bm{x}_0) \frac{p_t(\bm{x}_t | \bm{x}_0)}{p_t(\bm{x}_t)} p_0(\bm{x}_0) d\bm x_0
    \end{align*}
We aim to rewrite
\begin{equation}
\bm v_t(\bm x_t)
= \int \bm v_t(\bm x_t \mid \bm x_0) 
\frac{p_t(\bm x_t \mid \bm x_0)\,p_0(\bm x_0)}{p_t(\bm x_t)} \, d\bm x_0
= \mathbb{E}_{p(\bm x_0 \mid \bm x_t)}\!\left[ \bm v_t(\bm x_t \mid \bm x_0) \right].
\end{equation}

The forward (noising) process is linear Gaussian:
\begin{equation}
\bm x_t = \alpha_t \bm x_0 + \sigma_t \bm\varepsilon,
\qquad \bm\varepsilon \sim \mathcal N(\bm 0, \bm I),
\end{equation}
so that
\begin{equation}
p_t(\bm x_t \mid \bm x_0)
= \mathcal N\!\left(\bm x_t;\, \alpha_t \bm x_0,\, \sigma_t^2 \bm I\right).
\end{equation}

Assume the conditional velocity has the form
\begin{equation}
\bm v_t(\bm x_t \mid \bm x_0)
= \hat\alpha_t \bm x_0 + \hat\sigma_t \bm\varepsilon.
\end{equation}
Since
\(
\bm\varepsilon = (\bm x_t - \alpha_t \bm x_0)/\sigma_t,
\)
we can eliminate the noise and write
\begin{align}
\bm v_t(\bm x_t \mid \bm x_0)
&= \tfrac{\hat\sigma_t}{\sigma_t}\, \bm x_t
+ \Bigl(\hat\alpha_t - \tfrac{\hat\sigma_t \alpha_t}{\sigma_t}\Bigr)\, \bm x_0 \\
&\triangleq a_t \bm x_t + b_t \bm x_0.
\end{align}

Suppose the prior $p_0$ is empirical:
\begin{equation}
p_{{0}}(\bm{y}) = \frac{1}{N}\sum_{i=1}^{N} \mathds{1}_{\bm{y}_i}(\bm{y}).
\end{equation}
Then the marginal and posterior are finite mixtures:
\begin{align}
p_t(\bm x_t)
&= \tfrac{1}{N}\sum_{i=1}^N 
\mathcal N\!\left(\bm x_t;\, \alpha_t \bm y_i,\, \sigma_t^2 \bm I\right), \\
p(\bm x_0=\bm y_i \mid \bm x_t)
&= \frac{w_i(\bm x_t)}{\sum_{j=1}^N w_j(\bm x_t)},
\qquad
w_i(\bm x_t) \triangleq 
\mathcal N\!\left(\bm x_t;\, \alpha_t \bm y_i,\, \sigma_t^2 \bm I\right).
\end{align}

Taking the expectation of the linear form yields
\begin{equation}
\bm v_t(\bm x_t)
= a_t \bm x_t + b_t\,\mathbb{E}[\bm x_0 \mid \bm x_t].
\end{equation}
From Bayes’ rule,
\begin{equation}
\mathbb{E}[\bm x_0 \mid \bm x_t]
= \sum_{i=1}^N \pi_i(\bm x_t)\,\bm y_i,
\qquad
\pi_i(\bm x_t) = \frac{w_i(\bm x_t)}{\sum_{j=1}^N w_j(\bm x_t)}.
\end{equation}
In conclusion, under the empirical prior, 
$\bm v_t(\bm x_t)$ is obtained as a posterior-weighted average of the 
conditional velocities associated with each training sample $\bm y_i$:
\begin{equation}
\bm v^*_t(\bm x_t) 
= \sum_{i=1}^N p_0(\bm x_0=\bm y_i \mid \bm x_t)\,
\bm v_t(\bm x_t \mid \bm y_i)
.
\end{equation}

\end{proof}


\subsubsection{The Bias and Variance of Plug-in Velocity}
\label{app:bvanalysisplugin}
% \begin{lemma}\label{lem:weight-tv}
% Define the normalization constant as 
% \[
% Z(q)\coloneqq \int p(\bm{x}_t \mid \bm{y})q(\bm{y})d\bm{y}.
% \]
% For the weight function
% \[
% w_q(\bm{y}) := \frac{q(\bm{y})p(\bm{x}_t \mid \bm{y})}{Z(q)},
% \]
% if there are two distributions $q$ and $r$ with finite normalization constant
% \[
% Z(q), Z(r) \ge z_0 > 0,
% \]
% and $q,r$ admit densities bounded by $M$:
% $\|q\|_\infty,\|r\|_\infty\le M$.
% Then, with $\mathrm{TV}(q,r)=\tfrac12\!\int|q-r|$,
% \begin{align}
% \label{eq:Z-tv-gauss}
% |Z(q)-Z(r)| 
% &\le \frac{2}{(2\pi)^{d/2}\sigma_t^d}\,\mathrm{TV}(q,r),\\
% \label{eq:w-tv-l1-gauss}
% \int |w_q(\bm{y})-w_r(\bm{y})|\,d\bm{y}
% &\le 2\,\mathrm{TV}(q,r)\!\left(
% \frac{1}{(2\pi)^{d/2}\sigma_t^d\,z_0}
% +\frac{M}{(2\pi)^d\,\sigma_t^{2d}\,z_0^{2}}
% \right),\\
% \label{eq:w2-tv-l1-gauss}
% \int |w_q(\bm{y})^2-w_r(\bm{y})^2|\,d\bm{y}
% &\le 4\,\mathrm{TV}(q,r)\!\left(
% \frac{M}{(2\pi)^d\,\sigma_t^{2d}\,z_0^{2}}
% +\frac{M^{2}}{(2\pi)^{3d/2}\,\sigma_t^{3d}\,z_0^{3}}
% \right).
% \end{align}
% \end{lemma}

% \begin{proof}
% (1) Let $K_t(\bm{y}):=p(\bm{x}_t\mid \bm{y})=\mathcal N(\bm{x}_t;\bm{y},\sigma_t^2 I)$. From
% \[
% |Z(q)-Z(r)|=\Big|\int K_t(\bm{y})\,(q-r)(\bm{y})\,d\bm{y}\Big|,
% \]
% and the bound $K_t(\bm{y})\le \frac{1}{(2\pi)^{d/2}\sigma_t^d}$, we obtain
% \[
% |Z(q)-Z(r)| \le \frac{1}{(2\pi)^{d/2}\sigma_t^d}\int|q-r|
% = \frac{2}{(2\pi)^{d/2}\sigma_t^d}\,\mathrm{TV}(q,r),
% \]
% which proves \eqref{eq:Z-tv-gauss}.

% (2) For $w_q-w_r$,
% \[
% w_q(\bm{y})-w_r(\bm{y})
% =K_t(\bm{y})\!\left(\frac{q(\bm{y})-r(\bm{y})}{Z(q)}
% + r(\bm{y})\frac{Z(r)-Z(q)}{Z(q)\,Z(r)}\right).
% \]
% Taking $L^1$ norms and using $Z(\cdot)\ge z_0$, $K_t\le \frac{1}{(2\pi)^{d/2}\sigma_t^d}$,
% $\|r\|_\infty\le M$, and \eqref{eq:Z-tv-gauss}, we get
% \[
% \begin{aligned}
% \int |w_q-w_r|
% &\le \frac{1}{(2\pi)^{d/2}\sigma_t^d\,z_0}\int|q-r|
% + \frac{1}{(2\pi)^{d/2}\sigma_t^d}\frac{M}{z_0^2}\,|Z(r)-Z(q)| \\
% &\le 2\,\mathrm{TV}(q,r)\!\left(
% \frac{1}{(2\pi)^{d/2}\sigma_t^d\,z_0}
% +\frac{M}{(2\pi)^d\,\sigma_t^{2d}\,z_0^{2}}
% \right),
% \end{aligned}
% \]
% which yields \eqref{eq:w-tv-l1-gauss}.

% (3) Since $0\le w_q,w_r \le \frac{1}{(2\pi)^{d/2}\sigma_t^d}\frac{M}{z_0}$,
% \[
% |w_q^2-w_r^2|=|w_q-w_r|\,(w_q+w_r)
% \le \frac{2}{(2\pi)^{d/2}\sigma_t^d}\frac{M}{z_0}\,|w_q-w_r|.
% \]
% Integrating both sides and plugging in \eqref{eq:w-tv-l1-gauss}, we obtain
% \[
% \int |w_q^2-w_r^2|
% \le \frac{2M}{(2\pi)^{d/2}\sigma_t^d z_0}\cdot
% 2\,\mathrm{TV}(q,r)\!\left(
% \frac{1}{(2\pi)^{d/2}\sigma_t^d\,z_0}
% +\frac{M}{(2\pi)^d\,\sigma_t^{2d}\,z_0^{2}}
% \right),
% \]
% which simplifies to \eqref{eq:w2-tv-l1-gauss}.
% \end{proof}

Under mild assumptions and with the Bias-Variance Decomposition, we can analyze $\mathbb{E}\|\bm v^*_t-\bm v_t\|^2$, which consists of the bias term and variance term. The proposition below shows that although there is an increase in bias of order $\mathcal{O}(1/N)$, the variance is significantly reduced by $\mathcal{O}(1-1/N)$.

\begin{proposition}[Bias-Variance Decomposition of Ideal Velocity]~\label{app:thmvelbv} Assume there are the empirical distribution $p_\text{emp}$ on any $\{\bm y^{(i)}\}_{i=1}^N$ and the data distribution $p_0$ has the finite normalization constant
\[
Z(p_\text{emp}\mid\{\bm y\}_{i=1}^N), Z(p_0) \ge z_0 > 0.
\]
Suppose $\exists M_1,M_2>0, s>\frac{d}{2}, s.t.\|\bm v_{t}(\bm x_t \mid \bm x_0)\|_{\mathcal{C}^s}\leq M_1$, and $\|\bm{v}_t(\bm{x}_t)\| \leq M_2$.
Then we have
\begin{align*}
\mathbb{E}\|\bm v^*_t-\bm v_t\|^2
\leq C\left(\frac{M_1^2+2M_2^2}{N} + \frac{4\operatorname{Var}\left[ \bm{v}_{t}(\bm{x}_t|\bm x_0) \right] }{N} \right).
\end{align*}
\end{proposition}
\begin{proof}
According to Eq. \ref{eq:bvc-app}
\[
\mathbb{E}\|A-B\|^2
= \|\mu_A - \mu_B\|^2 
+ \operatorname{Var}[A]
+ \operatorname{Var}[B]
- 2\operatorname{Cov}[A,B],
\]
and the Cauchy-Schwarz inequality
\[
-2\,\mathrm{Cov}(A,B)\ \leq \mathrm{Var}[A]+\mathrm{Var}[B],
\]
we have
\[
\mathbb{E}\|\bm v_t - \bm v^*_t \|^2
\leq (\mathbb{E}\bm v_t  - \mathbb{E}\bm v^*_t )^2
+ 2\operatorname{Var}[v^*_t]
+ 2\operatorname{Var}[v_t]
\]
So we analyze the bias and variance of $\bm v^*_t$ below.

\textbf{Bias of plug-in velocity.}
\begin{align*}
|\mathbb{E}[\bm v^*] - \mathbb{E}[\bm v_t]|
&\leq \frac{1}{z_0}|\mathbb{E}[p_\text{emp}(\bm x_0)p(\bm x_t\mid\bm x_0)\bm v_{t}(\bm x_t\mid\bm x_0) 
- p_0 (\bm x_0)p(\bm x_t\mid\bm x_0)\bm v_{t}(\bm x_t\mid\bm x_0)]| \\
&\leq M_1\mathbb{E}[\|p_\text{emp} - p_0\|_{C_1^s}]
\end{align*}
where
\begin{align*}
\|\nu\|_{C_1^s}
:= \sup \Big\{ \int f\, d\nu : f \in C^s(\Omega),\; \|f\|_{C^s} \le 1 \Big\}.
\end{align*}
The previous work \citep{Kloeckner2018EmpiricalMR} has proven that
\begin{align*}
\mathbb{E}[\|p_\text{emp} - p_0\|_{C_1^s}] 
\leq \frac{C}{\sqrt{N}},
\end{align*}
so we finally get
\begin{align*}
|\mathbb{E}[\bm v^*] - \mathbb{E}[\bm v_t]|^2
\leq \frac{C M_1^2}{N}.
\end{align*}

\textbf{Variance of plug-in velocity.}
    Let \[
    Z(\bm x_t, \{\bm y^{(i)}\}_{i=1}^N, t)\coloneqq\int p(\bm{x}_t\mid\bm x_0)p_\text{emp}(\bm x_0)d\bm x=\frac{1}{N}\sum^N_{i=1}p(\bm{x}_t\mid\bm y^{(i)}).
    \] 
    Then, under the empirical distribution, we can write 
    \begin{align*}
    \bm{v}^*_t 
    &= \frac{1}{NZ(\bm x_t, \{\bm y^{(i)}\}_{i=1}^N, t)} \sum _{i=1}^{N} \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) p(\bm{x}_t|\bm y^{(i)}) \\
    \end{align*}
    which leads to the variance of $\bm{v}^*_t$ as 
    \begin{align*}
    \operatorname{Var}[\bm{v}^*_t]
    &\leq \frac{C}{N^2} \sum _{i=1}^{N} \operatorname{Var}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) p(\bm{x}_t|\bm y^{(i)}) \right] \\
    &= \frac{C}{N^2} \sum _{i=1}^{N} \left(\mathbb{E}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)})^2 p(\bm{x}_t|\bm y^{(i)})^2 \right]
    - \left(\mathbb{E}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) p(\bm{x}_t|\bm y^{(i)}) \right]\right)^2 \right) \\
    &\leq \frac{C}{N^2} \sum _{i=1}^{N} \mathbb{E}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)})^2 p(\bm{x}_t|\bm y^{(i)})^2 \right] \\
    &\leq \frac{C'}{N^2} \sum _{i=1}^{N} \mathbb{E}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)})^2 \right] \\
    &\leq \frac{C'}{N^2} \sum _{i=1}^{N} \left(\operatorname{Var}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) \right] 
    + \left(\mathbb{E}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) \right]\right)^2\right) \\
    &= \frac{C'}{N^2} \sum _{i=1}^{N} \left(\operatorname{Var}\left[ \bm{v}_{t}(\bm{x}_t|\bm y^{(i)}) \right] 
    + \bm{v}_t(\bm{x}_t)^2 \right) \\
    &= \frac{C'}{N}\left(\operatorname{Var}\left[ \bm{v}_{t}(\bm{x}_t|\bm x_0) \right] 
    + M_2^2 \right).
    \end{align*}
\end{proof}
\subsection{The Convergence of CTSC Loss Employing Plug-in Velocity~(Sec.~\ref{sec:improvement})}
\label{app:proofplugin}

\begin{lemma} \label{lemma:weight}
Define the normalization constant as 
\[
Z(q)\coloneqq \int p(\bm{x}_t \mid \bm{y})q(\bm{y})d\bm{y}.
\]
For the weight function
\[
w_q(\bm{y}) := \frac{q(\bm{y})p(\bm{x}_t \mid \bm{y})}{Z(q)},
\]
if there are two distribution $q$ and $r$ with finite normalization constant
\[
Z(q), Z(r) \ge z_0 > 0,
\]
the following inequalities holds
\begin{align*}
&|w_q(\bm{y})-w_r(\bm{y})|
\le \Big(\frac{1}{(2\pi)^{d/2}\sigma_t^d z_0} + \frac{L}{(2\pi)^{d/2}\sigma_t^d z_0^2}\Big)W_1(q,r) \\
&|w_q(\bm{y})^2-w_r(\bm{y})^2| 
\le \Big(\frac{1}{(2\pi)^{d}\sigma_t^2d z_0^2} + \frac{L}{(2\pi)^{d}\sigma_t^2d z_0^3}\Big)W_1(q,r).
\end{align*}
\end{lemma}

\begin{proof}
First, since $p(\bm{x}_t\mid \bm{y}) = \mathcal{N}(\bm{x}_t; \bm{y}, \sigma_t^2 I)$
There exist constants $L>0$ such that
\[
0 < p(\bm{x}_t\mid \bm{y}) \le \frac{1}{(2\pi)^{d/2}\sigma_t^d}, \qquad |p(\bm{x}_t\mid \bm{y})-p(\bm{x}_t\mid \bm{y}')|\le L \|\bm{y}-\bm{y}'\|.
\]
Denote $p(\bm{x}_t\mid \bm{y})$ as $K_t(\bm{y})$. We decompose
\[
w_q(\bm{y})-w_r(\bm{y})
=K_t(\bm{y})\Big(\frac{q(\bm{y})}{Z(q)}-\frac{r(\bm{y})}{Z(r)}\Big)
=K_t(\bm{y})\Big(\frac{q(\bm{y})-r(\bm{y})}{Z(q)} + r(\bm{y})\frac{Z(r)-Z(q)}{Z(q)Z(r)}\Big).
\]
Then, by Kantorovich–Rubinstein duality,
\[
|Z(q)-Z(r)| = \Big|\int K_t\,d(q-r)\Big| \le LW_1(q,r).
\]
Also, using $Z(q),Z(r)\ge z_0$ and $K_t(\bm{y})\le \frac{1}{(2\pi)^{d/2}\sigma_t^d}$, we have
\begin{align*}
|w_q(\bm{y})-w_r(\bm{y})|
&\le \frac{1}{(2\pi)^{d/2}\sigma_t^d z_0}|q(\bm{y})-r(\bm{y})| + \frac{1}{(2\pi)^{d/2}\sigma_t^d z_0^2}|Z(q)-Z(r)| \\
&\le \Big(\frac{1}{(2\pi)^{d/2}\sigma_t^d z_0} + \frac{L}{(2\pi)^{d/2}\sigma_t^d z_0^2}\Big)W_1(q,r).
\end{align*}
Next, since $0 \le w_q(\bm{y})\le \frac{1}{\sqrt{2\pi}\sigma_t}/z_0$, we bound the squared difference:
\begin{align*}
|w_q(\bm{y})^2-w_r(\bm{y})^2| 
&= |w_q(\bm{y})-w_r(\bm{y})|(w_q(\bm{y})+w_r(\bm{y})) \\
&\le \frac{1}{(2\pi)^{d/2}\sigma_t^d z_0}\Big(\frac{1}{(2\pi)^{d/2}\sigma_t^d z_0} + \frac{L}{(2\pi)^{d/2}\sigma_t^d z_0^2}\Big)W_1(q,r) \\
&= \Big(\frac{1}{(2\pi)^{d}\sigma_t^2d z_0^2} + \frac{L}{(2\pi)^{d}\sigma_t^2d z_0^3}\Big)W_1(q,r).
\end{align*}

\end{proof}

\begin{proposition}~\label{app:thmplugin}
Denote the empirical distribution as $p_\text{emp}$, then the difference between the training loss employing plug-in velocity and marginal velocity can be bounded by the Wasserstein distance between $p_\text{emp}$ and $p_0$ as
\begin{align*}
\big|\mathcal L_{\text{plug-in}}(\theta)-\mathcal L_{\text{marginal}}(\theta)\big|
\le C W(p_\text{emp},p_0).
\end{align*}

\end{proposition}

\begin{proof}
Substituting the form of plug-in velocity in Eq. \ref{eq:idealvel}, we have
\begin{align*}
\mathcal{L}_\text{plug-in} (\theta)
&= \mathbb{E} \big[ \Vert \bm u_{t,r}(\bm x_t) -(r-t)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)- \bm{v}^*_t(\bm{x}_t| \{\bm{y}^{(i)}\}_{i=1}^N) \Vert^2 \big] \\
&= \mathbb{E} \Big[ \Vert \bm u_{t,r}(\bm x_t) -(r-t)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t) \Big. \\
&\qquad \Big. - \sum_{i}^{N}\frac{\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(i)},\sigma^2_t\bm{\mathrm{I}})}{\sum_{j}^{N}\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(j)},\sigma^2_t\bm{\mathrm{I}})}(\dot\alpha_t\bm{y}^{(i)} + \frac{\dot\sigma_t}{\sigma_t}(\bm{x}_t - \alpha_t \bm{y}^{(i)}) ) \Vert^2 \Big] \\
&= \mathbb{E} \Big[ \sum_{i}^{N} \Big( \Big( \frac{\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(i)},\sigma^2_t\bm{\mathrm{I}})}{\sum_{j}^{N}\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(j)},\sigma^2_t\bm{\mathrm{I}})} \Big)^2 \cdot \Big. \\
&\qquad \Big. \Vert \bm u_{t,r}(\bm x_t) -(r-t)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)- (\dot\alpha_t\bm{y}^{(i)} + \frac{\dot\sigma_t}{\sigma_t}(\bm{x}_t - \alpha_t \bm{y}^{(i)}) ) \Vert^2 \Big) \Big].
\end{align*}
We denote
\begin{align*}
&w(\bm x_t, \bm y, \{\bm{y}^{(i)}\}_{i=1}^N, t) \coloneqq \frac{\mathcal{N}(\bm{x}_t;\alpha_t\bm{y},\sigma^2_t\bm{\mathrm{I}})}{\sum_{j}^{N}\mathcal{N}(\bm{x}_t;\alpha_t\bm{y}^{(j)},\sigma^2_t\bm{\mathrm{I}})}, \\
&l(\bm x_t,\bm y, t, r, \theta) \coloneqq \Vert \bm u_{t,r}(\bm x_t) -(r-t)\,\frac{d}{dt} \bm u^\theta_{t,r}(\bm x_t)- (\dot\alpha_t\bm{y} + \frac{\dot\sigma_t}{\sigma_t}(\bm{x}_t - \alpha_t \bm{y}) ) \Vert^2,
\end{align*}
then
\begin{align*}
\mathcal{L}_\text{plug-in} (\theta)
&= \mathbb{E} \Big[ \sum_{i=1}^N \Big( w^2(\bm x_t, \bm y^{(i)}, \{\bm{y}^{(i)}\}_{i=1}^N, t) l(\bm x_t,\bm y^{(i)}, t, r, \theta) \Big) \Big].
% &= \mathbb{E}_{\bm y} \Big[ \mathbb{E} \Big[ \sum_{i=1}^N \Big( w(\bm x_t, \bm y^{(i)}, \{\bm{y}^{(i)}\}_{i=1}^N, t)^2 l(\bm x_t,\bm y^{(i)}, t, r, \theta) \Big) \Big] \Big]. 
\end{align*}
Recall that $p_\text{emp}(\bm{y}) = \frac{1}{N}\sum_{i=1}^{N} \mathds{1}_{\bm{y}_i}(\bm{y}) $, then we can obtain
\begin{align*}
w(\bm x_t, \bm y, \{\bm{y}^{(i)}\}_{i=1}^N, t) 
= w_{p_\text{emp}}(\bm x_t, \bm y).
\end{align*}
And the training loss with marginal velocity is
\begin{align*}
\mathcal{L}_\text{marginal} (\theta)
&= \mathbb{E} \Big[ \sum_{i=1}^N \Big( w^2_{p_0}(\bm x_t, \bm y) l(\bm x_t,\bm y^{(i)}, t, r, \theta) \Big) \Big].
\end{align*}

Finally, by the boundedness of $\ell$ and Lemma \ref{lemma:weight}, we get
\begin{align*}
\big|\mathcal L_{\text{plug-in}}(\theta)-\mathcal L_{\text{marginal}}(\theta)\big|
&=\mathbb E \Big[ \sum_{i=1}^N\big(w_q^2-w_r^2\big)\ell(x_t,y^{(i)},t,r,\theta)\Big] \\
&\le C \sup_y |w_q^2-w_r^2| \\
&\le C W_1(p_\text{emp},p_0).
\end{align*}

\end{proof}

\begin{remarkgray}
We point out that the Wasserstein-1 distance between the empirical distribution and the true distribution decreases as the number of data samples increases, which has been established in previous literature \citep{fournier2015rate}.
\end{remarkgray}

\section{Experimental Details}
\subsection{Details for Empirical Analysis of Fig.~\ref{fig:elucidsc}}
\label{app:elucidexpsetting}
We conduct experiments on unconditional generation on CIFAR-10, conditional generation on ImageNet-256$\times$256 with or without the classifier-free-guidance setting. We here give more details of each method's setting of implementation.
\paragraph{Uncond. CIFAR-10.} We use a unified setting with batch size as 512 and iteration number as 800k ($\sim$8000 epochs). For stability, we adopt exponential moving average~(EMA) to update the model parameters, 
with decay ratio set to either $0.99995$ or $0.9999$. We find that 0.9999 ema decay usually performs better under 800k iteration with batchsize 512.  We report results using the best-performing EMA setting.  For all the  experimental trials, we trained them with Nvidia-A100$\times8$.
The detailed hyperparameter configurations for each model are as follows:
\begin{itemize}
    \item \textbf{CT} and \textbf{CT-linear}. 
    Both variants adopt LPIPS as the loss metric, where the difference lies in the choice of time path: the former uses a cosine path while the latter employs a linear path. We set the learning rate in training to 2e-4. 
    Following the official JAX implementation,  we adopt a progressive time sampler such that the scale $K_{\min}$ is initialized at $2$ and gradually increased to a maximum of $K_{\max}=150$. 
    This implies that the interval $[\sigma_{\min}, \sigma_{\max}]$ is partitioned according to
    \[
        \Big\{ [ \sigma_{\max} + \tfrac{h}{K} \big( \sigma_{\min}^{1/\rho} - \sigma_{\max}^{1/\rho} \big) ]^{\rho} \Big\}_{ h=1}^K,
    \]
    with $\sigma_{\min}=0.002$, $\sigma_{\max}=80.0$. After the change-of-variable, a $\frac{2}{\pi}\arctan()$ is operated to scale the time from [0.002, 80] to [0,1] in cosine path, while in linear path, the sampled time is normalized by $\frac{t}{t+1} \in [0,1]$. 
    In addition, a curriculum learning strategy is introduced to regulate the evolution of $K$ with respect to the training iterations. 
    When updating the model inside $\mathrm{sg}(\cdot)$ via EMA, a decay rate $r_{\text{ema}}$ is employed to further stabilize training. In detail, at training step $j \in \{1,\dots,J\}$ with total steps $J=800$k, 
the progressive scale $K(j)$ and the corresponding EMA decay rate $r_{\text{ema}}(j)$ 
are computed as
\begin{align*}
    K(j) &= \left\lceil 
        \sqrt{\;\frac{j}{J}\big((K_{\max}+1)^2 - K_{\min}^2\big) + K_{\min}^2}
        \;-\;1
    \right\rceil + 1, \\
    r_{\text{ema}}(j) &= \exp\!\left(
        -\frac{-\log(r_{\text{ema-min}})\,K_{\min}}{K(J)}
    \right).
\end{align*}
Here $K(j)$ is lower bounded by $1$, and $r_{\text{ema}}(j)$ smoothly 
interpolates between $r_{\text{ema-min}} = 0.9$ and $1$ 
as training progresses.
    \item \textbf{SCD}. For SCD, since the official release does not include the configuration for training on CIFAR, we use the same hyperparameter settings as those used for ImageNet in the official release.  $K$ defined as the total number of steps that we divide the time interval into is set as $128$, and the $p_\text{teq} = 0.25$ as the probability of training the average velocity with instantaneous conditional velocity supervision as described in Sec.~\ref{sec:examples}. We set the learning rate in training as 1e-4.
    \item \textbf{IMM}. Unlike the summary in Table~\ref{tab:methodconclude}, IMM here employs a cosine path with an EDM preconditioner. $M$ as the group size is set as 4 and $\gamma=12$ for calculating the difference between $s$ and $t$, as its default configuration. For the grouped kernel function, it is implemented by the RBF kernel. We set the learning rate in training to 1e-4.
    \item \textbf{sCT} and \textbf{sCT-linear}. In time sampler, $P_\text{mean} = -1$ and $P_\text{std}=1.4$. Tangent warmup iteration for gradient ratio is set as 10000. We set the learning rate in training to 1e-4. Besides, the variational adaptive weighting techniques are not employed for better understanding the modularized contribution of each models, while the tangent normalization is employed in the sCT for stabler training, but not implemented in sCT-linear.
    \item \textbf{MeanFlow}. In time sampler, $P_\text{mean} = -2.0$ and $P_\text{std}=2.0$. The $p_\text{teq} = 0.25$ as the probability of training the average velocity with instantaneous conditional velocity supervision. We set the learning rate in training to 6e-4. The power for adaptive weighting is 0.75.
\end{itemize}
Moreover, for CIFAR-10, to enable a fairer comparison, we keep the models identical except for the time sampler. Specifically, we disable adaptive loss in MeanFlow, variational adaptive weighting in sCT, and tangent warmup, and instead use a squared $l_2$ loss with a learning rate of 2e-4. Under this setting, with $p_{\text{teq}}=0.25$, we obtain FID50k of 4.64 for MeanFlow and 4.81 for sCT-linear on CIFAR-10, which also validates our conclusion in the Sec.~\ref{sec:elucidating}.

\paragraph{Cond. ImageNet.}
In this setting, we include the class label as part of the network input for conditional training. Since CTs require LPIPS as their loss metric, replacing it with a squared $l_2$ loss on latents causes training to diverge. For all the  experimental trials, we trained them with Nvidia-A100$\times8$. Therefore, we do not report CTs results in the latent space. For the other models, the settings are as follows:
\begin{itemize}
    \item \textbf{SCD}. The configuration is identical to that used for CIFAR-10.
    \item \textbf{IMM}. It is implemented with a linear path in latent space. $M$ as the group size is set as 4 and $\gamma=12$.  We observed that \textbf{IMM fails to converge} (FID does not decrease to a reasonable range) on SiT-B/2 when the \(B\in\{512,1024\}\).
Convergence appears only when we increase batch size $B$ to \(2048\), at which point the model begins to generate valid images.
This phenomenon is consistent with IMM's grouped loss: with group size \(M=4\), each mini-batch provides only \(B/M\) \emph{independent group-level supervision signals} for backpropagation.
Consequently, \(B=2048\) yields \(2048/4=512\) effective signals, which seems to be a practical threshold for stable training in our setup.
Therefore, in Fig.~\ref{fig:elucidscimgcnd}, we report IMM with \(\text{bsz}=2048\); the corresponding training epochs are scaled by the grouping factor, i.e., \(4\times 240=960\) epochs, to match the effective number of parameter updates. Others are the same as the setting for CIFAR-10.
    \item \textbf{sCT} and \textbf{sCT-linear}. We use the same hyperparameter setting as for CIFAR-10, since the original paper uses a U-Net in the pixel space, we cannot use the provided official configuration.
    \item \textbf{MeanFlow}. In time sampler, $P_\text{mean} = -0.4$ and $P_\text{std}=1.0$. The $p_\text{teq} = 0.75$. We set the learning rate in training to 1e-4. The power for adaptive weighting is 1.0.
\end{itemize}
\paragraph{CFG. ImageNet.}
For one-step generation, our training setting with CFG follows MeanFlow, as it introduces a mixing scale $\kappa$ and defines the velocity under CFG as
\begin{equation}
    \bm{v}^{\text{cfg}}(\bm x_t,t \mid c)
= \omega \bm v_{t|0}(\bm{x}_t \mid c) +
		\kappa \bm{u}_{t,t}^{\text{cfg}}(\bm x_t \mid {c}) +
		(1-\omega-\kappa)\bm{u}_{t,t}^{\text{cfg}}(\bm x_t). \label{eq:uwithcfg}
\end{equation}

This satisfies the original CFG formulation with an effective guidance scale $\kappa$. 
As it is proposed to bridge the instantaneous velocity and average velocity under classifier-free guidance, applying this technique directly to sCT, which models the instantaneous velocity, is not entirely straightforward. However, for sCT-linear, since we have shown its near equivalence to MeanFlow, the CFG training technique can be directly adopted. In addition, for IMM, applying CFG requires two NFEs during inference to compute $\bm v^{\text{cfg}}$. As our focus is on one-step generation, \emph{i.e.}, 1-NFE, we therefore do not include IMM in the comparisons. 

In addition, we adopt the best hyperparameter configuration recommended in the official MeanFlow implementation with DiT-B/2 while our network is changed into SiT-B/2, \emph{i.e.}, $\omega=1.0$, $\kappa=0.5$, class-dropout$=0.1$ and CFG triggered if $t$ in $[0,1]$, while keeping all other settings identical to those used for Cond. ImageNet. As an improved variant of SiT over DiT, it leads the FID50k to 6.09, better than 6.17 as reported to the original paper.
\subsection{Details for Empirical Analysis of  Table~\ref{tab:imgnetablation}}
\label{app:imgnetablation}
Here, we adopt the exact same parameter setting as MeanFlow with SiT-B/2.


\textbf{sCM training techniques.} In addition, in ESC, we, following sCM~\citep{scm} and EDMv2~\citep{edm2}, introduce a variational weighting output head, where the output of the time embedder is passed through a linear layer to a one-dimensional scalar as the adaptive weighting function, which reads $w_\text{adpt}^\psi(t,r)$ and is then used to reweight the original loss in Eq.~\ref{eq:MeanFlowloss}. We keep the the SiT architecture blocks untouched, while architectural
improvements are orthogonal and possible. Moreover, a ratio $r_\text{grad} = \min\{\frac{\text{iter}}{K_\text{grad}}, 1\}$ for tangent warmup is implemented for mitigating some gradient spikes during training, where $K_\text{grad}$ is set as 10k, the same as sCT training. 
\begin{align*}
     l_{\mathrm{esc}}(\bm{x}_t, r, t-dt, t;\theta) =&\frac{e^{w_\text{adpt}^\psi(t,r)}}{D} \cdot w\cdot \left\|\bm{u}^\theta_{t,r}(\bm{x}_t)
-\mathrm{sg}\left(\bm{v}_{t|0} + r_\text{grad}\cdot (r-t) \frac{d\bm{u}^{\theta}_{t,r}(\bm{x}_t)}{dt}\right)  \right\|_2^2 \\ &- w_\text{adpt}^\psi(t,r), 
\end{align*}

In this way, we gives the full hyper-parameter setting for Table~\ref{tab:imgnetablation}, as conclude in left column of Table ~\ref{app:tabparamablation}. For all the experimental trials with network architecture SiT-B/2, we trained them with Nvidia-A100$\times8$. 
\begin{table}[h]
\centering
\caption{Configurations on ImageNet $256\times256$ for Table~\ref{tab:imgnetablation}, (w/-cc) means `with-class-consistent' and (w/o-cc) means  `without-class-consistent'.} \vspace{-1em}
\label{app:tabparamablation}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccc}
\toprule
\textbf{Experiment}& \multicolumn{8}{c}{Sec.~\ref{sec:improvement}} & \multicolumn{2}{c}{Sec.~\ref{sec:exp}}\\
\cmidrule(lr){2-9} \cmidrule(lr){10-11} 
\textbf{Configs} & MeanFlow        & A1 & A2 & B1 & B2 & C & D & ESC &ESC(w/-cc)&ESC(w/o-cc)\\
\midrule
\textbf{Architecture} & \multicolumn{8}{c}{{B/2}} & \multicolumn{2}{c}{{XL/2}} \\
params (M)      &   \multicolumn{8}{c}{131}   &\multicolumn{2}{c}{{676}}\\
FLOPs (G)        &  \multicolumn{8}{c}{23.1} &\multicolumn{2}{c}{119.0}\\
depth            & \multicolumn{8}{c}{12}  &\multicolumn{2}{c}{{28}}\\
hidden dim        & \multicolumn{8}{c}{768} &\multicolumn{2}{c}{{1152}}\\
heads             & \multicolumn{8}{c}{12}    &\multicolumn{2}{c}{{16}}\\
patch size        & \multicolumn{8}{c}{2$\times$2}&\multicolumn{2}{c}{{2$\times$2}}\\
\midrule
\multicolumn{9}{l}{\textbf{Training and optimization}}  &&\\
epochs            & \multicolumn{8}{c}{240} &\multicolumn{2}{c}{240}  \\
batch size         & \multicolumn{8}{c}{512} &\multicolumn{2}{c}{256}\\
dropout            & \multicolumn{8}{c}{0.0} &\multicolumn{2}{c}{0.0}\\
optimizer          & \multicolumn{8}{c}{Adam~\citep{kingma2017adammethodstochasticoptimization}} &\multicolumn{2}{c}{Adam}\\
lr schedule        & \multicolumn{8}{c}{constant} & \multicolumn{2}{c}{constant} \\
lr                 & \multicolumn{8}{c}{0.0001} &\multicolumn{2}{c}{0.0001}\\
Adam ($\beta_1,\beta_2$) & \multicolumn{8}{c}{(0.9, 0.95)}&\multicolumn{2}{c}{(0.9, 0.95)}  \\
weight decay       & \multicolumn{8}{c}{0.0} &\multicolumn{2}{c}{0.0}\\
ema decay          & \multicolumn{8}{c}{0.9999} &\multicolumn{2}{c}{0.9999} \\
\midrule
\multicolumn{9}{l}{\textbf{Time sampler}} &&\\
 $p_\text{teq}$          & \multicolumn{8}{c}{0.75}  &\multicolumn{2}{c}{0.75}\\
$(r,t)$ sampler              & \multicolumn{8}{c}{lognorm(-0.4, 1.0)} &\multicolumn{2}{c}{lognorm(-0.4, 1.0)}\\
power for adaptive weight $w$       & \multicolumn{8}{c}{1.0} &\multicolumn{2}{c}{1.0}\\
\midrule
\multicolumn{9}{l}{\textbf{CFG settings}} &&\\
  $\omega$ in Eq.~\ref{eq:uwithcfg}         &  \multicolumn{8}{c}{1.0} &\multicolumn{2}{c}{0.2}\\
  $\kappa$ in Eq.~\ref{eq:uwithcfg}          & \multicolumn{8}{c}{0.5} &\multicolumn{2}{c}{0.92}\\
  cls-cond drop  &  \multicolumn{8}{c}{0.1} &\multicolumn{2}{c}{0.1}\\
  triggered if $t$ is in     &\multicolumn{8}{c}{[0.0,1.0]} &\multicolumn{2}{c}{[0.0,0.75]}\\
\midrule
\multicolumn{9}{l}{\textbf{ESC improvments}} &&\\
  $p_\text{plug-in}$         &  0.0 &{1.0}&0.5&1.0&0.5&0.0&0.0 &0.5 &0.2& 0.2\\
  $K_\text{grad}$        & 1 &1&1&1&1&1&10k&10k &00k& 00k\\
  $K_\text{fix0}$  &   1 &1&1&1&1&20k&1&20k &20k&20k\\
  class-consistent batching &   No &No&No&Yes&Yes&No&No&Yes&No&Yes\\
  variational adaptive weighting &   No &No&No&No&No&No&Yes&Yes&Yes&Yes\\
\bottomrule
\end{tabular}%
} \vspace{-1em}
\end{table}


\subsection{Details for Scaling-up Evaluation in Sec.~\ref{sec:exp}}
\label{app:scaleup}

\paragraph{CIFAR-10.} 
We conduct class-unconditional generation experiments on CIFAR-10. 
Following the official MeanFlow setting, we adopt the Adam optimizer with a learning rate of $6\times 10^{-4}$, batch size $1024$, and momentum parameters $(\beta_1,\beta_2)=(0.9,0.999)$. 
We use a dropout rate of $0.2$, no weight decay, and an EMA decay factor of $0.99995$. 
Training is performed for 800k iterations, including a 10k warm-up phase. 
For time sampling, we draw $(r,t)\sim\mathrm{LogNorm}(-2.0,\,2.0)$, with probability $75\%$ that $r \neq t$. 
The adaptive weighting exponent is set to $p=0.75$. 
Data augmentation follows the protocol of~\cite{edm}, except that vertical flipping and rotation are disabled.  

Regarding our proposed improvements, we observed that variational adaptive weighting from EDM2 did not yield further gains and was therefore not adopted. 
Instead, we found that setting the plug-in probability $p_{\text{plug-in}} \in [0.2,0.5]$ improved training stability, although a performance gap remained. 
Moreover, we set $K_\text{fix0}=20$k and $K_\text{grad}=10$k. 
All CIFAR-10 experiments with U-Net architectures were conducted on 8 Nvidia A100 GPUs.

\paragraph{ImageNet-256$\times$256.} 
For large-scale evaluation, we adopt SiT-XL/2 as the backbone of our improved CTSC variant, denoted as ESC. 
The hyperparameters follow the default configuration recommended by MeanFlow under the CFG setting, with details provided in the right column of Table~\ref{app:tabparamablation}. 
In practice, we find that the tangent normalization technique does not further brings performance improvements in the continuous-time shortcut model with linear path in training SiT-XL/2. 
All ImageNet experiments with SiT-XL/2 were trained on 16 Nvidia A100 GPUs.

\subsection{Visualization Examples for ESC}
\label{app:vis}
We provide visualization results of ESC-generated images on ImageNet-256$\times$256 with different network architectures: SiT-B/2 (Figure~\ref{fig:imgescb}) and SiT-XL/2 (Figure~\ref{fig:imgescxl}). 
All samples are generated in a single step using the same noise initialization from the latent prior and identical class labels for classifier guidance. 
Additional CIFAR-10 examples generated by ESC at different training epochs are shown in Figure~\ref{fig:imagescifar2}.
\begin{figure*}[htb] 
\centering
\subfigure[MeanFlow-B/2]{
    \includegraphics[height=0.33\linewidth, trim=8 0 00 20, clip]{imgnet_mf_limit.pdf}
    \label{fig:mfb2-nfe}
}
\subfigure[ESC-B/2]{
    \includegraphics[height=0.33\linewidth, trim=8 0 00 20, clip]{imgnet_esc_b2_limit.pdf}
    \label{fig:escb2-nfe}
}
\subfigure[ESC-XL/2]{
    \includegraphics[height=0.33\linewidth, trim=8 0 00 20, clip]{imgnet_esc_limit.pdf}
    \label{fig:escxl2-nfe}
}
\vspace{-0.5em}
\caption{Comparison of FID50k under 1-NFE and under 2-NFE during training among different methods.}
\label{fig:2nfe}
\end{figure*}


\subsection{Algorithm for Plug-in Velocity Calculation.}~\label{app:velocity}
\begin{algorithm}[H]
\caption{Calculation of Plug-in Velocity}
\label{alg:plugin_velocity}
\begin{algorithmic}[1]
\Require Training batch $\bm{x} \in \mathbb{R}^{B \times D}$, sampled time $t$
\State Sample noise $\bm{e} \sim \mathcal{N}(0, I)$
\State Compute noised samples: $\bm{x}_t = (1 - t)\bm{x} + t\bm{e}$
\State For all sample pairs $(i,j)$ in the batch, compute
\[
\boldsymbol{\varepsilon}_{i,j} = \frac{\bm{x}_{t,j} - (1 - t)\bm{x}_i}{t}
\]
\State Evaluate log-probabilities:
\[
\log p_{i,j} = \sum_{d=1}^{D} \log \mathcal{N}({\varepsilon}_{i,j,d}; 0, 1)
\]
\State Compute normalized weights along index $i$:
\[
w_{i,j} = \frac{\exp(\log p_{i,j})}{\sum_{i'} \exp(\log p_{i',j})}
\]
\State Compute conditional velocity:
\[
\bm{v}_{\mathrm{cnd}, i,j} = \boldsymbol{\varepsilon}_{i,j} - \bm{x}_i
\]
\State Aggregate to obtain plug-in velocity:
\[
\bm{v}_{\text{plug-in}, j} = \sum_i w_{i,j} \, \bm{v}_{\mathrm{cnd}, i,j}
\]
\Ensure $\bm{v}_{\text{plug-in}} = \{\bm{v}_{\text{plug-in}, j}\}_{j=1}^B$
\end{algorithmic}
\end{algorithm}
\subsection{Full Comparison of ESC vs. other SOTA benchmarks}\label{app:fullcomp}
We further include comparisons with the current state-of-the-art diffusion and autoregressive models for completeness, as shown in Table~\ref{tab:fullcomp} for ImageNet 256$\times$256, and Table~\ref{tab:fullcompcifar} for CIFAR10.
\begin{table}[h]
\caption{Evaluation of ESC and other benchmarks under one/few-step generation on ImageNet-256$\times$256. Underline means overall the best, while bold means the best in shortcut models. } \label{tab:fullcomp} \centering
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{cllrr}
\toprule
Family & Method & Param. & NFE & FID50k \\
\midrule

\multirow{3}{*}{\rotatebox{90}{\makebox[0pt][c]{GAN}}} 
& BigGAN~\citep{biggan} & 112M & 1 & 6.95 \\
& GigaGAN~\citep{gigagan} & 569M & 1 & 3.45 \\
& StyleGAN-XL~\citep{stylegan} & 166M & 1 & 2.30 \\

\midrule

\multirow{4}{*}{\rotatebox{90}{\makebox[0pt][c]{{AR/Mask}}}}
& AR w/ VQGAN~\citep{ar} & 227M & 1024 & 26.52 \\
& MaskGIT~\citep{maskgit} & 227M & 8 & 6.18 \\
& VAR-d30~\citep{var} & 2B & 10$\times$2 & 1.92 \\
& MAR-H~\citep{mar} & 943M & 256$\times$2 & 1.55 \\
\midrule

\multirow{6}{*}{\rotatebox{90}{\makebox[0pt][c]{{Diff/ Flow}}}}
& ADM~\citep{edm2} & 554M & 250$\times$2 & 10.94 \\
& LDM-4-G~\citep{stablediff} & 400M & 250$\times$2 & 3.60 \\
& SimDiff~\citep{simdiff} & 2B & 512$\times$2 & 2.77 \\
& DiT-XL/2~\citep{dit} & 675M & 250$\times$2 & 2.27 \\
& SiT-XL/2~\citep{sit} & 675M & 250$\times$2 & 2.06 \\
& SiT-XL/2+REPA~\citep{repa} & 675M & 250$\times$2 & \underline{1.42} \\
\midrule
\multirow{7}{*}{\rotatebox{90}{\makebox[0pt][c]{Shortcut}}}
& iCT~\citep{icm} & 675M & 1 & 34.24 \\
& SCD~\citep{scd} & 675M & 1 & 10.60 \\
& IMM~\citep{imm} & 675M & 1$\times$2 & 7.77 \\
& \multirow{2}{*}{MeanFlow~\citep{meanflow}} & \multirow{2}{*}{676M} & 1 & 3.43 \\
& & & 2 & 2.93 \\
& \textbf{ESC (w/o-class-consist.)} & 676M & 1 & 2.92 \\
& \textbf{ESC (w/-class-consist.)} & 676M & 1 & {2.85} \\
& \textbf{ESC+ (w/-class-consist.)} & 676M & 1 & \textbf{2.53} \\
\bottomrule
\end{tabular}
}
\vspace{-1em}
\end{table}
\begin{table}[]
\setlength{\tabcolsep}{4pt}
\centering
\small
\caption{Full comparison on unconditional generation on. CIFAR-10.
}~\label{tab:fullcompcifar}\vspace{-1.1em}
\begin{tabular}{clrr}
\toprule
Family & {method} & NFE & {FID} \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\makebox[0pt][c]{{Distill}}}} 
&Diff-Instruct~\citep{diffinstruct} & 1 & 4.53 \\
&DMD~\citep{dmd} & 1 & 2.66 \\
&SID~\citep{sid} & 1 & \textbf{1.92} \\
\midrule
\multirow{6}{*}{\rotatebox{90}{\makebox[0pt][c]{{Shortcut}}}} 
&iCT~\citep{icm} &  1 & \underline{2.83} \\
&ECT~\citep{geng2025consistency} &  1 & 3.60 \\
&sCT~\citep{scm} &  1 & 2.97 \\
&IMM~\citep{imm} &  1 & 3.20 \\
&{MeanFlow}~\citep{meanflow} & 1 & 2.92 \\
&\textbf{ESC} & 1 & \underline{2.83} \\
\bottomrule
\end{tabular}
\end{table}


\subsection{Details of ESC-XL/2 convergence with and without class-consistent mini-batching}\label{app:convminibatch}
Here we give the convergence of FID with ESC-XL with or without class-consistent in the complete training process, as shown in Figure~\ref{fig:imagenetescxlfidconvclasscons}.
\begin{figure*}
    \centering
        \subfigure{
            \includegraphics[width=0.60\linewidth, trim=10 00 30 00, clip]{rebuttal/imgnet_esc_full.pdf}
            \label{fig:}
            }\hspace{-0.5em}
        \caption{Convergence of FID with ESC-XL with or without class-consistent.}
        \label{fig:imagenetescxlfidconvclasscons}
\end{figure*}


\section{Further Analysis}
    
\subsection{Plug-in Velocity Stabilizes the Training}\label{app:stabletrain}
To figure out whether the plug-in velocity helps to stabilize the training of shortcut models, here we give the training loss vs. iteration steps for MeanFlow and MeanFlow with Plug-in Velocity. We show the comparison of the first 200k iteration in Figure~\ref{fig:cifartrainingcomp}, where all the training setting are the same in our paper with batch size set as 512.  It further illustrates that incorporating the plug-in velocity significantly stabilizes the training of MeanFlow.
\begin{figure*}[]
    \centering
        \subfigure[MeanFlow without Plug-in Velocity (MeanFlow)]{
            \includegraphics[width=0.485\linewidth, trim=140 40 130 120, clip]{rebuttal/esc-loss-mf.pdf}
            \label{fig:}
            }\hspace{-0.5em}
        \subfigure[MeanFlow with Plug-in Velocity (ESC)]{
            \includegraphics[width=0.485\linewidth, trim=140 40 130 120, clip]{rebuttal/esc-loss-mf-mv.pdf}
            \label{fig:}
            }
        \caption{Stable loss fluctuation with plug-in velocity.}
        \label{fig:cifartrainingcomp}
\end{figure*}


\subsection{Large Models Gain More Performance from Low Variance Training} \label{app:xlbetterb}
As shown, performance improvement for SiT-XL/2 over the MeanFlow is 16.9\%, while it is 5.3\% for SiT-B/2 architecture. We attribute the performance gap to two key factors: 
\begin{itemize}
    \item  \textbf{Optimization Dynamics.} In larger networks (e.g., XL/2), the representational capacity increases substantially, amplifying the impact of optimization stability. As shown in Figure~\ref{fig:cifartrainingcomp}, MeanFlow exhibits higher variance and less stable loss behavior during training, whereas ESC maintains stable optimization and is therefore more likely to converge to a better solution. In smaller models (e.g., B/2), the representational capacity is nearly saturated, leaving limited room for further improvement. In contrast, for larger models, ESC’s improved stability enables it to better exploit the additional capacity, resulting in more noticeable performance gains.
    \item  \textbf{Statistical Generalization.} As the parameter space dimensionality increases, gradient noise also grows, making variance-reduction mechanisms—such as EMA, momentum, gradient clipping, or the proposed plug-in velocity—more beneficial. This observation aligns with the theoretical intuition in \cite{kaplan2020scalinglawsneurallanguage}, where the generalization gap (or overfitting) is linked to the variance term scaling. Within the scaling law framework, bias dominates in smaller models, while variance becomes the main factor as the model scales up. To illustrate this, we compare the FID convergence curves of ESC-B/2 vs. MeanFlow-B/2 (trained for 600k iterations) and ESC-XL/2 vs. MeanFlow-XL/2 (trained for 1.2M iterations), as shown in Figure~\ref{fig:imagenetmfescfidconv}. Empirically, in the smaller B/2 setting, both methods converge rapidly to similar FID values. However, in the larger XL/2 model, MeanFlow’s FID curve plateaus in the later training stages, while ESC continues to improve and reaches 2.85. This suggests that in large-scale models, variance dominates generalization behavior, and the variance reduction introduced by plug-in velocity significantly enhances final performance.
\end{itemize}
\begin{figure*}
    \centering
        \subfigure[MeanFlow vs. ESC with SiT-B/2]{
            \includegraphics[width=0.485\linewidth, trim=10 00 30 00, clip]{rebuttal/imgnet_esc_meanflow_b2.pdf}
            \label{fig:}
            }\hspace{-0.5em}
        \subfigure[MeanFlow vs. ESC with SiT-XL/2]{
            \includegraphics[width=0.485\linewidth, trim=10 00 30 00, clip]{rebuttal/imgnet_esc_meanflow_xl2.pdf}
            \label{fig:}
            }\hspace{-0.5em}
        \caption{Convergence of FID with different model architectures.}
        \label{fig:imagenetmfescfidconv}
\end{figure*}



\section{Limitations and Future Works}\label{app:limitation}
\begin{itemize}
    \item \textbf{Slow convergence in few-step generation.} An interesting phenomenon we observed is that, under the proposed improvements of ESC, employing two-step generation, i.e., 
$\bm{x}_0 = X^\theta_{0.5,0} \circ X^\theta_{1,0.5}(\bm{x}_1)$, led to slower FID convergence compared to one-step generation. This effect is particularly evident under the SiT-XL/2 architecture, whereas for B/2, the two-step scheme still achieves better performance, as shown in Fig.~\ref{fig:2nfe}. Although MeanFlow also exhibits relatively slow convergence with two-step generation, it still outperforms one-step (2.93 vs.\ 3.43). One possible explanation is that, in variational adaptive weighting, predictions from $0$ to $1$ are inherently more difficult. With the stronger expressivity of the XL/2 architecture, the training naturally allocates higher weights to $\bm u_{0,1}^\theta$, while the simpler sub-task $\bm u_{0.5,0}^\theta$ receives less weight. In contrast, for the more capacity-limited B/2 architecture, fitting the easier task like $\bm u_{0.5,0}^\theta$ proves beneficial for the overall convergence. We leave a deeper investigation of this phenomenon as future work.
    \item \textbf{Inflexibility in training with CFG.} We observe that introducing CFG leads to a relative improvement of $(33.05-6.09)/33.05 =  81.5\%$, indicating that training with CFG is essential. However, the current approach follows Eq.~\ref{eq:uwithcfg}, which inevitably introduces two additional hyperparameters, $\omega$ and $\kappa$. As shown in Table~\ref{app:tabparamablation} and in Table~4 of the original work, the optimal values of these parameters, as well as the triggered intervals, vary significantly across architectures. This greatly complicates hyperparameter tuning, and for large models such as XL/2, results in substantial computational overhead. Therefore, we argue that alternative approaches, such as \emph{representation alignment}~\citep{repa}, \emph{representation entanglement}~\citep{reen}, or RL-guided generation~\citep{zheng2025diffusionnftonlinediffusionreinforcement}, may offer promising replacements by injecting class-related semantic information into training or enabling CFG-free diffusion generation. We leave the exploration of these directions for future work. 
    \item \textbf{Approximation for fast JVP.} Since computing JVP is required, techniques such as FlashAttention cannot be directly applied in architectures like SiT. Although this does not incur a significant time overhead, it leads to substantial memory consumption. Moreover, the computation of JVP itself is relatively expensive and introduces additional memory usage. In future work, we plan to explore numerical approximations of JVP to reduce reliance on explicit differential operators. 
    \item \textbf{Generalization to downstream tasks and more models.} Our current work focuses purely on generative modeling. An important future direction is to extend the proposed framework to downstream tasks where generation is conditioned on additional modalities, such as text-to-image synthesis, image editing, or molecule design. Incorporating cross-modal alignment mechanisms and scalable conditioning strategies would allow the model to generalize beyond unconditional settings, making it applicable to a wider range of real-world scenarios. In particular, extending the framework to text-to-image generation represents a natural and promising step, enabling richer semantic control and practical applications.  Furthermore,  the proposed techniques like plug-in velocity, should be regarded as a general training technique. 
        Since our paper includes extensive modular decomposition and performance comparisons across a wide range of methods, it is difficult to perform with/without plug-in velocity evaluations for all models under limited computational resources. We will consider extending the proposed techniques for evaluation to a broader set of models as part of our future work.
\end{itemize}
\begin{figure*}[]
    \centering
        \subfigure[ 4000 epochs, FID50k=3.39 ]{
            \includegraphics[width=0.35\linewidth]{esc-cifar/3999_0_ema0.99995.png}
            \label{fig:}
            }
        \subfigure[8000 epochs, FID50k=2.98]{
            \includegraphics[width=0.35\linewidth]{esc-cifar/7999_0_ema0.99995.png}
            \label{fig:}
            }

    \centering
        \subfigure[12000 epochs, FID50k=3.93]{
            \includegraphics[width=0.35\linewidth]{esc-cifar/11999_0_ema0.99995.png}
            \label{fig:}
            }
        \subfigure[16000 epochs, FID50k=2.95]{
            \includegraphics[width=0.35\linewidth]{esc-cifar/15999_0_ema0.99995.png}
            \label{fig:}
            }
        \caption{Images generated by ESC trained with CIFAR-10  of different epochs}
        \label{fig:imagescifar2}
\end{figure*}

\end{document}
